{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUknhuHKyc-E"
   },
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "    <img alt=\"arize logo\" src=\"https://storage.googleapis.com/arize-assets/arize-logo-white.jpg\" width=\"300\"/>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/arize/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/client_python\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://arize-ai.slack.com/join/shared_invite/zt-11t1vbu4x-xkBIHmOREQnYnYDH1GDfCg\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "\n",
    "<center><h1>Using Arize with RAG</h1></center>\n",
    "\n",
    "This guide shows you how to create a retrieval augmented generation chatbot and evaluate performance with Arize. RAG is typically to respond to queries using a specified set of documents instead of using the LLM's own training data, reducing hallucination and incorrect generations.\n",
    "\n",
    "We'll go through the following steps:\n",
    "\n",
    "* Create a RAG chatbot using LlamaIndex\n",
    "\n",
    "* Trace the retrieval and llm calls using Arize\n",
    "\n",
    "* Create a dataset to benchmark performance\n",
    "\n",
    "* Evaluate performance using LLM as a judge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FfImo32BJYkr"
   },
   "source": [
    "# Create a RAG chatbot using LlamaIndex\n",
    "\n",
    "Let's start with all of our boilerplate setup:\n",
    "\n",
    "1. Install packages for tracing and retrieval\n",
    "2. Setup our API keys\n",
    "3. Setup Phoenix for tracing\n",
    "4. Create our LlamaIndex query engine\n",
    "5. See your results in Phoenix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DcHymV1dh_SS"
   },
   "source": [
    "### Install packages for tracing and retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install llama-index, openai for setting up our RAG chatbot\n",
    "!pip install -qq llama-index==0.12.5 openai==1.57.1 llama-index-core\n",
    "\n",
    "# Install arize packages for tracing and evaluation\n",
    "!pip install -qq \"arize-phoenix-evals>=0.17.5\" \"arize-otel>=0.7.0\" \"openinference-instrumentation-llama-index>=3.0.4\" \"arize[Datasets]>7.29.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQnyEnJisyn3"
   },
   "source": [
    "### Setup our API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "SPACE_ID = globals().get(\"SPACE_ID\") or getpass(\n",
    "    \"ðŸ”‘ Enter your Arize Space ID: \"\n",
    ")\n",
    "API_KEY = globals().get(\"API_KEY\") or getpass(\"ðŸ”‘ Enter your Arize API Key: \")\n",
    "DEVELOPER_KEY = globals().get(\"DEVELOPER_KEY\") or getpass(\n",
    "    \"ðŸ”‘ Enter your Arize Developer Key: \"\n",
    ")\n",
    "OPENAI_API_KEY = globals().get(\"OPENAI_API_KEY\") or getpass(\n",
    "    \"ðŸ”‘ Enter your OpenAI API key: \"\n",
    ")\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kfid5cE99yN5"
   },
   "source": [
    "### Setup Arize for Tracing\n",
    "\n",
    "To follow with this tutorial, you'll need to sign up for Arize and get your API key. You can see the [guide here](https://docs.arize.com/arize/llm-tracing/quickstart-llm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import open-telemetry dependencies\n",
    "from arize.otel import register\n",
    "from openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n",
    "\n",
    "# Setup OTEL via our convenience function\n",
    "tracer_provider = register(\n",
    "    space_id=SPACE_ID,\n",
    "    api_key=API_KEY,\n",
    "    project_name=\"rag-cookbook\",  # name this to whatever you would like\n",
    ")\n",
    "LlamaIndexInstrumentor().instrument(tracer_provider=tracer_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Ewpx7Dgebym"
   },
   "source": [
    "### Create our LlamaIndex query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "!wget \"https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\" -O data/paul_graham_essay.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from pprint import pprint\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine(llm=OpenAI(model=\"gpt-4o-mini\"))\n",
    "response = query_engine.query(\"What did Paul Graham work on?\")\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in response.source_nodes:\n",
    "    text_fmt = node.node.get_content().strip().replace(\"\\n\", \" \")[:200] + \"...\"\n",
    "    print(text_fmt)\n",
    "    print(node.score)\n",
    "    print(\"--------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUyvcly1iNrv"
   },
   "source": [
    "### See your results in the Arize UI\n",
    "Once you've run a single query, you can see the trace in the Arize UI with each step taken by the retriever, the embedding, and the llm query.\n",
    "\n",
    "Click through the queries to better understand how the query engine is performing. Arize can be used to understand and troubleshoot your RAG pipeline by surfacing:\n",
    " - Application latency\n",
    " - Token usage\n",
    " - Runtime exceptions\n",
    " - Retrieved documents\n",
    " - Embeddings\n",
    " - LLM parameters\n",
    " - Prompt templates\n",
    " - Tool descriptions\n",
    " - LLM function calls\n",
    " - And more!\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/arize-assets/tutorials/images/llamaindex-arize-starter.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0Qvn8tAs9vL"
   },
   "source": [
    "# Create synthetic dataset of questions\n",
    "\n",
    "Using the template below, we're going to generate a dataframe of 25 questions we can use to test our customer support agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEN_TEMPLATE = \"\"\"\n",
    "You are an assistant that generates Q&A questions about Paul Graham's essay below.\n",
    "\n",
    "The questions should involve the essay contents, specific facts and figures,\n",
    "names, and elements of the story. Do not ask any questions where the answer is\n",
    "not in the essay contents.\n",
    "\n",
    "Respond with one question per line. Do not include any numbering at the beginning of each line. Do not include any category headings.\n",
    "Generate 10 questions. Be sure there are no duplicate questions.\n",
    "\n",
    "[START ESSAY]\n",
    "{essay}\n",
    "[END ESSAY]\n",
    "\"\"\"\n",
    "\n",
    "with open(\"data/paul_graham_essay.txt\", \"r\") as file:\n",
    "    file_content = file.read()\n",
    "\n",
    "GEN_TEMPLATE = GEN_TEMPLATE.format(essay=file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "\n",
    "nest_asyncio.apply()\n",
    "from phoenix.evals import OpenAIModel\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 500)\n",
    "\n",
    "model = OpenAIModel(model=\"gpt-4o\", max_tokens=1300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = model(GEN_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_response = resp.strip().split(\"\\n\\n\")\n",
    "\n",
    "questions_df = pd.DataFrame(split_response, columns=[\"input\"])\n",
    "print(questions_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGIbV49kHp4H"
   },
   "source": [
    "Now let's run it and manually inspect the traces! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rag(query_engine, questions_df):\n",
    "    response_df = questions_df.copy(deep=True)\n",
    "    for index, row in response_df.iterrows():\n",
    "        response = query_engine.query(row[\"input\"])\n",
    "        reference_text = \"\"\n",
    "        for node in response.source_nodes:\n",
    "            reference_text += node.text\n",
    "            reference_text += \"\\n\"\n",
    "        response_df.loc[index, \"output\"] = response\n",
    "        response_df.loc[index, \"reference\"] = reference_text\n",
    "    text_columns = [\"input\", \"output\", \"reference\"]\n",
    "    response_df[text_columns] = response_df[text_columns].apply(\n",
    "        lambda x: x.astype(str)\n",
    "    )\n",
    "    return response_df\n",
    "\n",
    "\n",
    "response_df = run_rag(query_engine, questions_df)\n",
    "response_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "beUkwcCgLaEa"
   },
   "source": [
    "# Evaluating your RAG app\n",
    "\n",
    "Now that we have a set of test cases, we can create evaluators to measure performance. This way, we don't have to manually inspect every single trace to see if the LLM is doing the right thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RELEVANCE_EVAL_TEMPLATE = \"\"\"You are comparing a reference text to a question and trying to determine if the reference text\n",
    "contains information relevant to answering the question. Here is the data:\n",
    "    [BEGIN DATA]\n",
    "    ************\n",
    "    [Question]: {input}\n",
    "    ************\n",
    "    [Reference text]: {reference}\n",
    "    [END DATA]\n",
    "\n",
    "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
    "contains information that can answer the Question. Please focus on whether the very specific\n",
    "question can be answered by the information in the Reference text.\n",
    "Your response must be single word, either \"relevant\" or \"unrelated\",\n",
    "and should not contain any text or characters aside from that word.\n",
    "\"unrelated\" means that the reference text does not contain an answer to the Question.\n",
    "\"relevant\" means the reference text contains an answer to the Question.\n",
    "\"\"\"\n",
    "\n",
    "CORRECTNESS_EVAL_TEMPLATE = \"\"\"You are given a question, an answer and reference text. You must determine whether the\n",
    "given answer correctly answers the question based on the reference text. Here is the data:\n",
    "    [BEGIN DATA]\n",
    "    ************\n",
    "    [Question]: {input}\n",
    "    ************\n",
    "    [Reference]: {reference}\n",
    "    ************\n",
    "    [Answer]: {output}\n",
    "    [END DATA]\n",
    "Your response must be a single word, either \"correct\" or \"incorrect\",\n",
    "and should not contain any text or characters aside from that word.\n",
    "\"correct\" means that the question is correctly and fully answered by the answer.\n",
    "\"incorrect\" means that the question is not correctly or only partially answered by the answer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1aivaxTCRQFl"
   },
   "source": [
    "We will be creating an LLM as a judge using the prompt templates above by taking the spans recorded by Phoenix, and then giving them labels using the `llm_classify` function. This function uses LLMs to evaluate your LLM calls and gives them labels and explanations. You can read more detail [here](https://docs.arize.com/phoenix/api/evals#phoenix.evals.llm_classify)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.evals import OpenAIModel, llm_classify\n",
    "\n",
    "RELEVANCE_RAILS = [\"relevant\", \"unrelated\"]\n",
    "CORRECTNESS_RAILS = [\"incorrect\", \"correct\"]\n",
    "\n",
    "relevance_eval_df = llm_classify(\n",
    "    dataframe=response_df,\n",
    "    template=RELEVANCE_EVAL_TEMPLATE,\n",
    "    model=OpenAIModel(model=\"gpt-4o\"),\n",
    "    rails=RELEVANCE_RAILS,\n",
    "    provide_explanation=True,\n",
    "    include_prompt=True,\n",
    "    concurrency=4,\n",
    ")\n",
    "\n",
    "correctness_eval_df = llm_classify(\n",
    "    dataframe=response_df,\n",
    "    template=CORRECTNESS_EVAL_TEMPLATE,\n",
    "    model=OpenAIModel(model=\"gpt-4o\"),\n",
    "    rails=CORRECTNESS_RAILS,\n",
    "    provide_explanation=True,\n",
    "    include_prompt=True,\n",
    "    concurrency=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vDV1KBdYQ_vh"
   },
   "source": [
    "Let's look at and inspect the results of our evaluatiion!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctness_eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with different k-values\n",
    "\n",
    "We can also experiment with different k-values for the retriever. This is the number of documents retrieved from the vector store. We can also experiment with different chunk sizes, chunk overlaps, and rerankers. We'll be using the ColbertReranker from LlamaIndex. You can read more about it [here](https://docs.llamaindex.ai/docs/postprocessors/colbert-reranker)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "\n",
    "def run_rag_with_settings(questions_df, k_value, chunk_size, chunk_overlap):\n",
    "    node_parser = SimpleNodeParser.from_defaults(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    nodes = node_parser.get_nodes_from_documents(documents)\n",
    "    vector_index = VectorStoreIndex(nodes)\n",
    "    query_engine = vector_index.as_query_engine(\n",
    "        similarity_top_k=k_value,  # Default is 2\n",
    "        response_mode=\"compact\",  # or use \"tree-summarize\"\n",
    "        llm=OpenAI(model=\"gpt-4o-mini\"),\n",
    "    )\n",
    "    response_df = run_rag(query_engine, questions_df)\n",
    "    return response_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's setup our evaluators to see how the performance changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluators(rag_df):\n",
    "    relevance_eval_df = llm_classify(\n",
    "        dataframe=rag_df,\n",
    "        template=RELEVANCE_EVAL_TEMPLATE,\n",
    "        model=OpenAIModel(model=\"gpt-4o\"),\n",
    "        rails=RELEVANCE_RAILS,\n",
    "        provide_explanation=True,\n",
    "        concurrency=4,\n",
    "    )\n",
    "    rag_df[\"relevance\"] = relevance_eval_df[\"label\"]\n",
    "    rag_df[\"relevance_explanation\"] = relevance_eval_df[\"explanation\"]\n",
    "\n",
    "    correctness_eval_df = llm_classify(\n",
    "        dataframe=rag_df,\n",
    "        template=CORRECTNESS_EVAL_TEMPLATE,\n",
    "        model=OpenAIModel(model=\"gpt-4o\"),\n",
    "        rails=CORRECTNESS_RAILS,\n",
    "        provide_explanation=True,\n",
    "        concurrency=4,\n",
    "    )\n",
    "    rag_df[\"correctness\"] = correctness_eval_df[\"label\"]\n",
    "    rag_df[\"correctness_explanation\"] = correctness_eval_df[\"explanation\"]\n",
    "    return rag_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's log these results to Arize and see how they compare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll create a dataset to store our questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arize.experimental.datasets import ArizeDatasetsClient\n",
    "from uuid import uuid1\n",
    "from arize.experimental.datasets.experiments.types import (\n",
    "    ExperimentTaskResultColumnNames,\n",
    "    EvaluationResultColumnNames,\n",
    ")\n",
    "from arize.experimental.datasets.utils.constants import GENERATIVE\n",
    "import pandas as pd\n",
    "\n",
    "# Set up the arize client\n",
    "arize_client = ArizeDatasetsClient(developer_key=DEVELOPER_KEY, api_key=API_KEY)\n",
    "dataset = None\n",
    "dataset_name = \"rag-experiments-\" + str(uuid1())[:3]\n",
    "\n",
    "dataset_id = arize_client.create_dataset(\n",
    "    space_id=SPACE_ID,\n",
    "    dataset_name=dataset_name,\n",
    "    dataset_type=GENERATIVE,\n",
    "    data=questions_df,\n",
    ")\n",
    "dataset = arize_client.get_dataset(space_id=SPACE_ID, dataset_id=dataset_id)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll define which columns of our dataframe will be mapped to outputs and which will be mapped to evaluation labels and explanations.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column mappings for task\n",
    "task_cols = ExperimentTaskResultColumnNames(\n",
    "    example_id=\"example_id\", result=\"output\"\n",
    ")\n",
    "# Define column mappings for evaluator\n",
    "relevance_evaluator_cols = EvaluationResultColumnNames(\n",
    "    label=\"relevance\",\n",
    "    explanation=\"relevance_explanation\",\n",
    ")\n",
    "correctness_evaluator_cols = EvaluationResultColumnNames(\n",
    "    label=\"correctness\",\n",
    "    explanation=\"correctness_explanation\",\n",
    ")\n",
    "\n",
    "\n",
    "def log_experiment_to_arize(experiment_df, experiment_name):\n",
    "    experiment_df[\"example_id\"] = dataset[\"id\"]\n",
    "    return arize_client.log_experiment(\n",
    "        space_id=SPACE_ID,\n",
    "        experiment_name=experiment_name + \"-\" + str(uuid1())[:2],\n",
    "        experiment_df=experiment_df,\n",
    "        task_columns=task_cols,\n",
    "        evaluator_columns={\n",
    "            \"correctness\": correctness_evaluator_cols,\n",
    "            \"relevance\": relevance_evaluator_cols,\n",
    "        },\n",
    "        dataset_name=dataset_name,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run it for each of our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Experiments for k-size\n",
    "k_2_chunk_100_overlap_10 = run_rag_with_settings(\n",
    "    questions_df, k_value=2, chunk_size=100, chunk_overlap=10\n",
    ")\n",
    "k_4_chunk_100_overlap_10 = run_rag_with_settings(\n",
    "    questions_df, k_value=4, chunk_size=100, chunk_overlap=10\n",
    ")\n",
    "k_10_chunk_100_overlap_10 = run_rag_with_settings(\n",
    "    questions_df, k_value=10, chunk_size=100, chunk_overlap=10\n",
    ")\n",
    "k_2_chunk_100_overlap_10 = run_evaluators(k_2_chunk_100_overlap_10)\n",
    "k_4_chunk_100_overlap_10 = run_evaluators(k_4_chunk_100_overlap_10)\n",
    "k_10_chunk_100_overlap_10 = run_evaluators(k_10_chunk_100_overlap_10)\n",
    "\n",
    "log_experiment_to_arize(k_2_chunk_100_overlap_10, \"k_2_chunk_100_overlap_10\")\n",
    "log_experiment_to_arize(k_4_chunk_100_overlap_10, \"k_4_chunk_100_overlap_10\")\n",
    "log_experiment_to_arize(k_10_chunk_100_overlap_10, \"k_10_chunk_100_overlap_10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments for chunk size\n",
    "k_2_chunk_200_overlap_10 = run_rag_with_settings(\n",
    "    questions_df, k_value=2, chunk_size=200, chunk_overlap=10\n",
    ")\n",
    "k_2_chunk_500_overlap_20 = run_rag_with_settings(\n",
    "    questions_df, k_value=2, chunk_size=500, chunk_overlap=20\n",
    ")\n",
    "k_2_chunk_1000_overlap_50 = run_rag_with_settings(\n",
    "    questions_df, k_value=2, chunk_size=1000, chunk_overlap=50\n",
    ")\n",
    "\n",
    "k_2_chunk_200_overlap_10 = run_evaluators(k_2_chunk_200_overlap_10)\n",
    "k_2_chunk_500_overlap_20 = run_evaluators(k_2_chunk_500_overlap_20)\n",
    "k_2_chunk_1000_overlap_50 = run_evaluators(k_2_chunk_1000_overlap_50)\n",
    "\n",
    "log_experiment_to_arize(k_2_chunk_200_overlap_10, \"k_2_chunk_200_overlap_10\")\n",
    "log_experiment_to_arize(k_2_chunk_500_overlap_20, \"k_2_chunk_500_overlap_20\")\n",
    "log_experiment_to_arize(k_2_chunk_1000_overlap_50, \"k_2_chunk_1000_overlap_50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment with HyDE\n",
    "\n",
    "We can also experiment with HyDE, a retrieval augmentation technique that uses LLMs to generate synthetic queries to retrieve more relevant documents. You can read more about it [here](https://docs.llamaindex.ai/docs/retrieval/hyde)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.query.query_transform import HyDEQueryTransform\n",
    "from llama_index.core.query_engine import TransformQueryEngine\n",
    "\n",
    "# Setup HyDe\n",
    "hyde = HyDEQueryTransform(\n",
    "    include_original=True, llm=OpenAI(model=\"gpt-4o-mini\")\n",
    ")\n",
    "query_engine = (\n",
    "    index.as_query_engine()\n",
    ")  # default k=2, chunk_size=1024, chunk_overlap=20\n",
    "hyde_query_engine = TransformQueryEngine(query_engine, hyde)\n",
    "\n",
    "# Run RAG with HyDE\n",
    "hyde_response_df = run_rag(hyde_query_engine, questions_df)\n",
    "\n",
    "# Evaluate RAG with HyDE\n",
    "hyde_response_df = run_evaluators(hyde_response_df)\n",
    "\n",
    "# Log to Arize\n",
    "log_experiment_to_arize(hyde_response_df, \"hyde\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
