{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUknhuHKyc-E"
   },
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "    <img alt=\"arize logo\" src=\"https://storage.googleapis.com/arize-assets/arize-logo-white.jpg\" width=\"300\"/>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/arize/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/client_python\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://join.slack.com/t/arize-ai/shared_invite/zt-1px8dcmlf-fmThhDFD_V_48oU7ALan4Q\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "\n",
    "<center><h1>Using Arize with RAG</h1></center>\n",
    "\n",
    "This guide shows you how to create a retrieval augmented generation chatbot and evaluate performance with Arize. RAG is typically to respond to queries using a specified set of documents instead of using the LLM's own training data, reducing hallucination and incorrect generations.\n",
    "\n",
    "We'll go through the following steps:\n",
    "\n",
    "* Create a RAG chatbot using LlamaIndex\n",
    "\n",
    "* Trace the retrieval and llm calls using Arize\n",
    "\n",
    "* Create a dataset to benchmark performance\n",
    "\n",
    "* Evaluate performance using LLM as a judge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FfImo32BJYkr"
   },
   "source": [
    "# Create a RAG chatbot using LlamaIndex\n",
    "\n",
    "Let's start with all of our boilerplate setup:\n",
    "\n",
    "1. Install packages for tracing and retrieval\n",
    "2. Setup our API keys\n",
    "3. Setup Phoenix for tracing\n",
    "4. Create our LlamaIndex query engine\n",
    "5. See your results in Phoenix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DcHymV1dh_SS"
   },
   "source": [
    "### Install packages for tracing and retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index openai arize-phoenix-evals arize-otel openinference-instrumentation-llama-index 'httpx<0.28'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQnyEnJisyn3"
   },
   "source": [
    "### Setup our API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kfid5cE99yN5"
   },
   "source": [
    "### Setup Arize for Tracing\n",
    "\n",
    "To follow with this tutorial, you'll need to sign up for Arize and get your API key. You can see the [guide here](https://docs.arize.com/arize/llm-tracing/quickstart-llm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import open-telemetry dependencies\n",
    "from arize.otel import register\n",
    "from openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n",
    "\n",
    "# Setup OTEL via our convenience function\n",
    "tracer_provider = register(\n",
    "    space_id=getpass(\"ðŸ”‘ Enter your Arize Space ID: \"),\n",
    "    api_key=getpass(\"ðŸ”‘ Enter your Arize API key: \"),\n",
    "    model_id=\"rag-cookbook\",  # name this to whatever you would like\n",
    ")\n",
    "LlamaIndexInstrumentor().instrument(tracer_provider=tracer_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Ewpx7Dgebym"
   },
   "source": [
    "### Create our LlamaIndex query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "!wget \"https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\" -O data/paul_graham_essay.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from pprint import pprint\n",
    "\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did Paul Graham work on?\")\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in response.source_nodes:\n",
    "    text_fmt = node.node.get_content().strip().replace(\"\\n\", \" \")[:200] + \"...\"\n",
    "    print(text_fmt)\n",
    "    print(node.score)\n",
    "    print(\"--------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUyvcly1iNrv"
   },
   "source": [
    "### See your results in the Arize UI\n",
    "Once you've run a single query, you can see the trace in the Arize UI with each step taken by the retriever, the embedding, and the llm query.\n",
    "\n",
    "Click through the queries to better understand how the query engine is performing.\n",
    "\n",
    "Phoenix can be used to understand and troubleshoot your by surfacing:\n",
    " - **Application latency** - highlighting slow invocations of LLMs, Retrievers, etc.\n",
    " - **Token Usage** - Displays the breakdown of token usage with LLMs to surface up your most expensive LLM calls\n",
    " - **Runtime Exceptions** - Critical runtime exceptions such as rate-limiting are captured as exception events.\n",
    " - **Retrieved Documents** - view all the documents retrieved during a retriever call and the score and order in which they were returned\n",
    " - **Embeddings** - view the embedding text used for retrieval and the underlying embedding model\n",
    "LLM Parameters - view the parameters used when calling out to an LLM to debug things like temperature and the system prompts\n",
    " - **Prompt Templates** - Figure out what prompt template is used during the prompting step and what variables were used.\n",
    " - **Tool Descriptions** - view the description and function signature of the tools your LLM has been given access to\n",
    " - **LLM Function Calls** - if using OpenAI or other a model with function calls, you can view the function selection and function messages in the input messages to the LLM.\n",
    "\n",
    "<img src=\"https://storage.cloud.google.com/arize-assets/tutorials/images/llamaindex-arize-starter.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0Qvn8tAs9vL"
   },
   "source": [
    "# Create synthetic dataset of questions\n",
    "\n",
    "Using the template below, we're going to generate a dataframe of 25 questions we can use to test our customer support agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEN_TEMPLATE = \"\"\"\n",
    "You are an assistant that generates Q&A questions about Paul Graham's essay below.\n",
    "\n",
    "The questions should involve the essay contents, specific facts and figures,\n",
    "names, and elements of the story. Do not ask any questions where the answer is\n",
    "not in the essay contents.\n",
    "\n",
    "Respond with one question per line. Do not include any numbering at the beginning of each line. Do not include any category headings.\n",
    "Generate 25 questions. Be sure there are no duplicate questions.\n",
    "\n",
    "[START ESSAY]\n",
    "{essay}\n",
    "[END ESSAY]\n",
    "\"\"\"\n",
    "\n",
    "with open(\"data/paul_graham_essay.txt\", \"r\") as file:\n",
    "    file_content = file.read()\n",
    "\n",
    "GEN_TEMPLATE = GEN_TEMPLATE.format(essay=file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "\n",
    "nest_asyncio.apply()\n",
    "from phoenix.evals import OpenAIModel\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 500)\n",
    "\n",
    "model = OpenAIModel(model=\"gpt-4o\", max_tokens=1300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = model(GEN_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_response = resp.strip().split(\"\\n\\n\")\n",
    "\n",
    "questions_df = pd.DataFrame(split_response, columns=[\"input\"])\n",
    "print(questions_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGIbV49kHp4H"
   },
   "source": [
    "Now let's run it and manually inspect the traces! Change the value in `.head(2)` from any number between 1 and 25 to run it on that many data points from the questions we generated earlier.\n",
    "\n",
    "Then manually inspect the outputs in Arize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: apply query_engine.query to every item in questions_df using column 'input'\n",
    "\n",
    "for index, row in questions_df.iterrows():\n",
    "    response = query_engine.query(row[\"input\"])\n",
    "    reference_text = \"\"\n",
    "    for node in response.source_nodes:\n",
    "        reference_text += node.text\n",
    "        reference_text += \"\\n\"\n",
    "    questions_df.loc[index, \"output\"] = response\n",
    "    questions_df.loc[index, \"reference\"] = reference_text\n",
    "questions_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "beUkwcCgLaEa"
   },
   "source": [
    "# Evaluating your RAG app\n",
    "\n",
    "Now that we have a set of test cases, we can create evaluators to measure performance. This way, we don't have to manually inspect every single trace to see if the LLM is doing the right thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RELEVANCE_EVAL_TEMPLATE = \"\"\"You are comparing a reference text to a question and trying to determine if the reference text\n",
    "contains information relevant to answering the question. Here is the data:\n",
    "    [BEGIN DATA]\n",
    "    ************\n",
    "    [Question]: {input}\n",
    "    ************\n",
    "    [Reference text]: {reference}\n",
    "    [END DATA]\n",
    "\n",
    "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
    "contains information that can answer the Question. Please focus on whether the very specific\n",
    "question can be answered by the information in the Reference text.\n",
    "Your response must be single word, either \"relevant\" or \"unrelated\",\n",
    "and should not contain any text or characters aside from that word.\n",
    "\"unrelated\" means that the reference text does not contain an answer to the Question.\n",
    "\"relevant\" means the reference text contains an answer to the Question.\n",
    "\"\"\"\n",
    "\n",
    "CORRECTNESS_EVAL_TEMPLATE = \"\"\"You are given a question, an answer and reference text. You must determine whether the\n",
    "given answer correctly answers the question based on the reference text. Here is the data:\n",
    "    [BEGIN DATA]\n",
    "    ************\n",
    "    [Question]: {input}\n",
    "    ************\n",
    "    [Reference]: {reference}\n",
    "    ************\n",
    "    [Answer]: {output}\n",
    "    [END DATA]\n",
    "Your response must be a single word, either \"correct\" or \"incorrect\",\n",
    "and should not contain any text or characters aside from that word.\n",
    "\"correct\" means that the question is correctly and fully answered by the answer.\n",
    "\"incorrect\" means that the question is not correctly or only partially answered by the\n",
    "answer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1aivaxTCRQFl"
   },
   "source": [
    "We will be creating an LLM as a judge using the prompt templates above by taking the spans recorded by Phoenix, and then giving them labels using the `llm_classify` function. This function uses LLMs to evaluate your LLM calls and gives them labels and explanations. You can read more detail [here](https://docs.arize.com/phoenix/api/evals#phoenix.evals.llm_classify).\n",
    "\n",
    "To get the spans in the right format, we'll be using our helper function `get_qa_with_reference`. You can see how this function works [here](https://docs.arize.com/phoenix/tracing/how-to-tracing/extract-data-from-spans#pre-defined-queries) and the github reference [here](https://github.com/Arize-ai/phoenix/blob/main/src/phoenix/trace/dsl/helpers.py#L71)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.evals import OpenAIModel, llm_classify\n",
    "\n",
    "RELEVANCE_RAILS = [\"relevant\", \"unrelated\"]\n",
    "CORRECTNESS_RAILS = [\"incorrect\", \"correct\"]\n",
    "\n",
    "relevance_eval_df = llm_classify(\n",
    "    dataframe=questions_df,\n",
    "    template=RELEVANCE_EVAL_TEMPLATE,\n",
    "    model=OpenAIModel(model=\"gpt-4o\"),\n",
    "    rails=RELEVANCE_RAILS,\n",
    "    provide_explanation=True,\n",
    "    include_prompt=True,\n",
    "    concurrency=4,\n",
    ")\n",
    "\n",
    "correctness_eval_df = llm_classify(\n",
    "    dataframe=questions_df,\n",
    "    template=CORRECTNESS_EVAL_TEMPLATE,\n",
    "    model=OpenAIModel(model=\"gpt-4o\"),\n",
    "    rails=CORRECTNESS_RAILS,\n",
    "    provide_explanation=True,\n",
    "    include_prompt=True,\n",
    "    concurrency=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vDV1KBdYQ_vh"
   },
   "source": [
    "Let's look at and inspect the results of our evaluatiion!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctness_eval_df"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
