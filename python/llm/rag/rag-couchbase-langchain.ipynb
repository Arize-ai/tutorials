{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUknhuHKyc-E"
   },
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "    <img alt=\"arize logo\" src=\"https://storage.googleapis.com/arize-assets/arize-logo-white.jpg\" width=\"300\"/>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/arize/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/client_python\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://arize-ai.slack.com/join/shared_invite/zt-11t1vbu4x-xkBIHmOREQnYnYDH1GDfCg\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "\n",
    "<center><h1>Using Arize with Couchbase</h1></center>\n",
    "\n",
    "This guide shows you how to create a retrieval augmented generation chatbot and evaluate performance with Arize and Couchbase. RAG is typically to respond to queries using a specified set of documents instead of using the LLM's own training data, reducing hallucination and incorrect generations.\n",
    "\n",
    "We'll go through the following steps:\n",
    "\n",
    "* Create a RAG chatbot with Langchain and Couchbase\n",
    "\n",
    "* Trace the retrieval and llm calls using Arize\n",
    "\n",
    "* Create a dataset to benchmark performance\n",
    "\n",
    "* Evaluate performance using LLM as a judge\n",
    "\n",
    "Much of the code in this tutorial is adapted from the [Langchain Couchbase Tutorial](https://python.langchain.com/docs/integrations/vectorstores/couchbase/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FfImo32BJYkr"
   },
   "source": [
    "# Create a RAG chatbot using Langchain and Couchbase\n",
    "\n",
    "Let's start with all of our boilerplate setup:\n",
    "\n",
    "1. Install packages for tracing and retrieval\n",
    "2. Setup our API keys\n",
    "3. Setup Arize for tracing\n",
    "4. Setup Couchbase\n",
    "5. Create our Langchain RAG query engine\n",
    "6. See your results in Arize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DcHymV1dh_SS"
   },
   "source": [
    "### Install packages for tracing and retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq openai langchain langchain_community langchain-openai langchain-couchbase\n",
    "\n",
    "!pip install -q arize-phoenix-evals arize-otel openinference-instrumentation-langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQnyEnJisyn3"
   },
   "source": [
    "### Setup our API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "SPACE_ID = globals().get(\"SPACE_ID\") or getpass(\n",
    "    \"ðŸ”‘ Enter your Arize Space ID: \"\n",
    ")\n",
    "API_KEY = globals().get(\"API_KEY\") or getpass(\"ðŸ”‘ Enter your Arize API Key: \")\n",
    "DEVELOPER_KEY = globals().get(\"DEVELOPER_KEY\") or getpass(\n",
    "    \"ðŸ”‘ Enter your Arize Developer Key: \"\n",
    ")\n",
    "OPENAI_API_KEY = globals().get(\"OPENAI_API_KEY\") or getpass(\n",
    "    \"ðŸ”‘ Enter your OpenAI API key: \"\n",
    ")\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kfid5cE99yN5"
   },
   "source": [
    "### Setup Arize for Tracing\n",
    "\n",
    "To follow with this tutorial, you'll need to sign up for Arize and get your API key. You can see the [guide here](https://docs.arize.com/arize/llm-tracing/quickstart-llm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import open-telemetry dependencies\n",
    "from arize.otel import register\n",
    "\n",
    "# Setup OTEL via our convenience function\n",
    "tracer_provider = register(\n",
    "    space_id=SPACE_ID,\n",
    "    api_key=API_KEY,\n",
    "    project_name=\"couchbase-rag\",\n",
    "    log_to_console=False,\n",
    "    batch=False,\n",
    ")\n",
    "\n",
    "# Import the automatic instrumentor from OpenInference\n",
    "from openinference.instrumentation.langchain import LangChainInstrumentor\n",
    "\n",
    "LangChainInstrumentor().uninstrument()\n",
    "LangChainInstrumentor().instrument(tracer_provider=tracer_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Couchbase\n",
    "\n",
    "You'll need to setup your Couchbase cluster by doing the following:\n",
    "1. Create an account at [Couchbase Cloud](https://cloud.couchbase.com/)\n",
    "2. Create a free cluster\n",
    "3. Create cluster access credentials\n",
    "4. Allow access to the cluster from your local machine\n",
    "5. Create a bucket to store your documents\n",
    "\n",
    "Screenshots below:\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/arize-assets/tutorials/images/couchbase-free-cluster.png\" width=\"800\"/>\n",
    "\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/arize-assets/tutorials/images/couchbase-cluster-access.png\" width=\"800\"/>\n",
    "\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/arize-assets/tutorials/images/couchbase-allowed-ips.png\" width=\"800\"/>\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/arize-assets/tutorials/images/couchbase-create-bucket.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Ewpx7Dgebym"
   },
   "source": [
    "### Create our Langchain RAG query engine\n",
    "\n",
    "Once you've setup your cluster, you can connect to it using langchain's couchbase package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUCHBASE_CONNECTION_STRING = getpass(\n",
    "    \"Enter the connection string for the Couchbase cluster: \"\n",
    ")\n",
    "DB_USERNAME = getpass(\"Enter the username for the Couchbase cluster: \")\n",
    "DB_PASSWORD = getpass(\"Enter the password for the Couchbase cluster: \")\n",
    "\n",
    "BUCKET_NAME = \"langchain_bucket\"\n",
    "SCOPE_NAME = \"_default\"\n",
    "COLLECTION_NAME = \"_default\"\n",
    "SEARCH_INDEX_NAME = \"langchain-index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "from couchbase.auth import PasswordAuthenticator\n",
    "from couchbase.cluster import Cluster\n",
    "from couchbase.options import ClusterOptions\n",
    "\n",
    "auth = PasswordAuthenticator(DB_USERNAME, DB_PASSWORD)\n",
    "options = ClusterOptions(auth)\n",
    "options.apply_profile(\"wan_development\")\n",
    "cluster = Cluster(COUCHBASE_CONNECTION_STRING, options)\n",
    "\n",
    "# Wait until the cluster is ready for use.\n",
    "cluster.wait_until_ready(timedelta(seconds=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before this step, you must also create a search index. You can do this by going to the Couchbase UI and clicking on the \"Search\" tab. Make sure the names match up with the ones we've defined above.\n",
    "\n",
    "Link below:\n",
    "https://docs.couchbase.com/cloud/vector-search/create-vector-search-index-ui.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_couchbase.vectorstores import CouchbaseVectorStore\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "vector_store = CouchbaseVectorStore(\n",
    "    cluster=cluster,\n",
    "    bucket_name=BUCKET_NAME,\n",
    "    scope_name=SCOPE_NAME,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    embedding=embeddings,\n",
    "    index_name=SEARCH_INDEX_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "!wget \"https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\" -O data/paul_graham_essay.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "\n",
    "def reset_vector_store(vector_store, chunk_size=1024, chunk_overlap=20):\n",
    "    results = vector_store.similarity_search(\n",
    "        k=1000,\n",
    "        query=\"\",  # Use an empty query or a specific one if needed\n",
    "        search_options={\n",
    "            \"query\": {\"field\": \"metadata.source\", \"match\": \"paul_graham_essay\"}\n",
    "        },\n",
    "    )\n",
    "    if results:\n",
    "        deleted_ids = []\n",
    "        for result in results:\n",
    "            deleted_ids.append(result.id)\n",
    "        vector_store.delete(ids=deleted_ids)\n",
    "    loader = TextLoader(\"./data/paul_graham_essay.txt\")\n",
    "    documents = loader.load()\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "\n",
    "    # Adding metadata to documents\n",
    "    for i, doc in enumerate(docs):\n",
    "        doc.metadata[\"source\"] = \"paul_graham_essay\"\n",
    "\n",
    "    vector_store.add_documents(docs)\n",
    "    return vector_store\n",
    "\n",
    "\n",
    "reset_vector_store(vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test the vector search directly with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What did Paul Graham say about the future of AI?\"\n",
    "vector_store.similarity_search(query, k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load different documents into the vector store to test with like below, with the metadata.source field used to filter the documents separately from vector queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "document_1 = Document(\n",
    "    page_content=\"I had chocalate chip pancakes and scrambled eggs for breakfast this morning.\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    ")\n",
    "\n",
    "document_2 = Document(\n",
    "    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    ")\n",
    "\n",
    "document_3 = Document(\n",
    "    page_content=\"Building an exciting new project with LangChain - come check it out!\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    ")\n",
    "\n",
    "document_4 = Document(\n",
    "    page_content=\"Robbers broke into the city bank and stole $1 million in cash.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    ")\n",
    "\n",
    "document_5 = Document(\n",
    "    page_content=\"Wow! That was an amazing movie. I can't wait to see it again.\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    ")\n",
    "\n",
    "document_6 = Document(\n",
    "    page_content=\"Is the new iPhone worth the price? Read this review to find out.\",\n",
    "    metadata={\"source\": \"website\"},\n",
    ")\n",
    "\n",
    "document_7 = Document(\n",
    "    page_content=\"The top 10 soccer players in the world right now.\",\n",
    "    metadata={\"source\": \"website\"},\n",
    ")\n",
    "\n",
    "document_8 = Document(\n",
    "    page_content=\"LangGraph is the best framework for building stateful, agentic applications!\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    ")\n",
    "\n",
    "document_9 = Document(\n",
    "    page_content=\"The stock market is down 500 points today due to fears of a recession.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    ")\n",
    "\n",
    "document_10 = Document(\n",
    "    page_content=\"I have a bad feeling I am going to get deleted :(\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    ")\n",
    "\n",
    "documents = [\n",
    "    document_1,\n",
    "    document_2,\n",
    "    document_3,\n",
    "    document_4,\n",
    "    document_5,\n",
    "    document_6,\n",
    "    document_7,\n",
    "    document_8,\n",
    "    document_9,\n",
    "    document_10,\n",
    "]\n",
    "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
    "\n",
    "vector_store.add_documents(documents=documents, ids=uuids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to tag the embedding field as a vector field in the search index settings. See image below:\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/arize-assets/tutorials/images/couchbase-search-index-settings.png\" width=\"800\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the vector search using the Langchain retriever interface across our new documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 1})\n",
    "docs = retriever.invoke(\"Is the stock market down?\", filter={\"source\": \"news\"})\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run an entire RAG query with the Langchain RAG query engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "question = \"What did Paul Graham say about AI?\"\n",
    "context = \"\"\n",
    "\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 1})\n",
    "docs = retriever.invoke(question, filter={\"source\": \"paul_graham_essay\"})\n",
    "for doc in docs:\n",
    "    context += doc.page_content\n",
    "\n",
    "messages = prompt.invoke(\n",
    "    {\"context\": context, \"question\": question}\n",
    ").to_messages()\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(\"Context: \", context)\n",
    "print(\"Response: \", response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUyvcly1iNrv"
   },
   "source": [
    "### See your results in the Arize UI\n",
    "Once you've run a single query, you can see the trace in the Arize UI with each step taken by the retriever, the embedding, and the llm query.\n",
    "\n",
    "Click through the queries to better understand how the query engine is performing. Arize can be used to understand and troubleshoot your RAG app by surfacing:\n",
    " - Application latency\n",
    " - Token usage\n",
    " - Runtime exceptions\n",
    " - Retrieved documents\n",
    " - Embeddings\n",
    " - LLM parameters\n",
    " - Prompt templates\n",
    " - Tool descriptions\n",
    " - LLM function calls\n",
    " - And more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0Qvn8tAs9vL"
   },
   "source": [
    "# Create synthetic dataset of questions\n",
    "\n",
    "Using the template below, we're going to generate a dataframe of 25 questions we can use to test our customer support agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEN_TEMPLATE = \"\"\"\n",
    "You are an assistant that generates Q&A questions about Paul Graham's essay below.\n",
    "\n",
    "The questions should involve the essay contents, specific facts and figures,\n",
    "names, and elements of the story. Do not ask any questions where the answer is\n",
    "not in the essay contents.\n",
    "\n",
    "Respond with one question per line. Do not include any numbering at the beginning of each line. Do not include any category headings.\n",
    "Generate 10 questions. Be sure there are no duplicate questions.\n",
    "\n",
    "[START ESSAY]\n",
    "{essay}\n",
    "[END ESSAY]\n",
    "\"\"\"\n",
    "\n",
    "with open(\"data/paul_graham_essay.txt\", \"r\") as file:\n",
    "    file_content = file.read()\n",
    "\n",
    "GEN_TEMPLATE = GEN_TEMPLATE.format(essay=file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "\n",
    "nest_asyncio.apply()\n",
    "from phoenix.evals import OpenAIModel\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 500)\n",
    "\n",
    "model = OpenAIModel(model=\"gpt-4o\", max_tokens=1300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = model(GEN_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_response = resp.strip().split(\"\\n\\n\")\n",
    "\n",
    "questions_df = pd.DataFrame(split_response, columns=[\"input\"])\n",
    "print(questions_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGIbV49kHp4H"
   },
   "source": [
    "Now let's run it and manually inspect the traces! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rag(vector_store, questions_df, k_value=1):\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": k_value})\n",
    "    response_df = questions_df.copy(deep=True)\n",
    "    for index, row in response_df.iterrows():\n",
    "        docs = retriever.invoke(row[\"input\"])\n",
    "        context = \"\"\n",
    "        for doc in docs:\n",
    "            context += doc.page_content\n",
    "        messages = prompt.invoke(\n",
    "            {\"context\": context, \"question\": row[\"input\"]}\n",
    "        ).to_messages()\n",
    "        response = llm.invoke(messages)\n",
    "        response_df.loc[index, \"output\"] = response.content\n",
    "        response_df.loc[index, \"reference\"] = context\n",
    "    text_columns = [\"input\", \"output\", \"reference\"]\n",
    "    response_df[text_columns] = response_df[text_columns].apply(\n",
    "        lambda x: x.astype(str)\n",
    "    )\n",
    "    return response_df\n",
    "\n",
    "\n",
    "response_df = run_rag(vector_store, questions_df, k_value=1)\n",
    "response_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "beUkwcCgLaEa"
   },
   "source": [
    "# Evaluating your RAG app\n",
    "\n",
    "Now that we have a set of test cases, we can create evaluators to measure performance. This way, we don't have to manually inspect every single trace to see if the LLM is doing the right thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RELEVANCE_EVAL_TEMPLATE = \"\"\"You are comparing a reference text to a question and trying to determine if the reference text\n",
    "contains information relevant to answering the question. Here is the data:\n",
    "    [BEGIN DATA]\n",
    "    ************\n",
    "    [Question]: {input}\n",
    "    ************\n",
    "    [Reference text]: {reference}\n",
    "    [END DATA]\n",
    "\n",
    "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
    "contains information that can answer the Question. Please focus on whether the very specific\n",
    "question can be answered by the information in the Reference text.\n",
    "Your response must be single word, either \"relevant\" or \"unrelated\",\n",
    "and should not contain any text or characters aside from that word.\n",
    "\"unrelated\" means that the reference text does not contain an answer to the Question.\n",
    "\"relevant\" means the reference text contains an answer to the Question.\n",
    "\"\"\"\n",
    "\n",
    "CORRECTNESS_EVAL_TEMPLATE = \"\"\"You are given a question, an answer and reference text. You must determine whether the\n",
    "given answer correctly answers the question based on the reference text. Here is the data:\n",
    "    [BEGIN DATA]\n",
    "    ************\n",
    "    [Question]: {input}\n",
    "    ************\n",
    "    [Reference]: {reference}\n",
    "    ************\n",
    "    [Answer]: {output}\n",
    "    [END DATA]\n",
    "Your response must be a single word, either \"correct\" or \"incorrect\",\n",
    "and should not contain any text or characters aside from that word.\n",
    "\"correct\" means that the question is correctly and fully answered by the answer.\n",
    "\"incorrect\" means that the question is not correctly or only partially answered by the answer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1aivaxTCRQFl"
   },
   "source": [
    "We will be creating an LLM as a judge using the prompt templates above by taking the spans recorded by Phoenix, and then giving them labels using the `llm_classify` function. This function uses LLMs to evaluate your LLM calls and gives them labels and explanations. You can read more detail [here](https://docs.arize.com/phoenix/api/evals#phoenix.evals.llm_classify)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.evals import OpenAIModel, llm_classify\n",
    "\n",
    "RELEVANCE_RAILS = [\"relevant\", \"unrelated\"]\n",
    "CORRECTNESS_RAILS = [\"incorrect\", \"correct\"]\n",
    "\n",
    "relevance_eval_df = llm_classify(\n",
    "    dataframe=response_df,\n",
    "    template=RELEVANCE_EVAL_TEMPLATE,\n",
    "    model=OpenAIModel(model=\"gpt-4o\"),\n",
    "    rails=RELEVANCE_RAILS,\n",
    "    provide_explanation=True,\n",
    "    include_prompt=True,\n",
    "    concurrency=4,\n",
    ")\n",
    "\n",
    "correctness_eval_df = llm_classify(\n",
    "    dataframe=response_df,\n",
    "    template=CORRECTNESS_EVAL_TEMPLATE,\n",
    "    model=OpenAIModel(model=\"gpt-4o\"),\n",
    "    rails=CORRECTNESS_RAILS,\n",
    "    provide_explanation=True,\n",
    "    include_prompt=True,\n",
    "    concurrency=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vDV1KBdYQ_vh"
   },
   "source": [
    "Let's look at and inspect the results of our evaluatiion!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctness_eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with different k-values, chunk sizes, and chunk overlaps\n",
    "\n",
    "Let's change the number of documents retrieved from the vector store, the size of the chunks loaded into the vector store, and the chunk overlaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_vector_store(vector_store, chunk_size=100, chunk_overlap=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_df = run_rag(vector_store, questions_df, k_value=2)\n",
    "print(rag_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rag_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's setup our evaluators to see how the performance changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluators(rag_df):\n",
    "    relevance_eval_df = llm_classify(\n",
    "        dataframe=rag_df,\n",
    "        template=RELEVANCE_EVAL_TEMPLATE,\n",
    "        model=OpenAIModel(model=\"gpt-4o\"),\n",
    "        rails=RELEVANCE_RAILS,\n",
    "        provide_explanation=True,\n",
    "        concurrency=4,\n",
    "    )\n",
    "    rag_df[\"relevance\"] = relevance_eval_df[\"label\"]\n",
    "    rag_df[\"relevance_explanation\"] = relevance_eval_df[\"explanation\"]\n",
    "\n",
    "    correctness_eval_df = llm_classify(\n",
    "        dataframe=rag_df,\n",
    "        template=CORRECTNESS_EVAL_TEMPLATE,\n",
    "        model=OpenAIModel(model=\"gpt-4o\"),\n",
    "        rails=CORRECTNESS_RAILS,\n",
    "        provide_explanation=True,\n",
    "        concurrency=4,\n",
    "    )\n",
    "    rag_df[\"correctness\"] = correctness_eval_df[\"label\"]\n",
    "    rag_df[\"correctness_explanation\"] = correctness_eval_df[\"explanation\"]\n",
    "    return rag_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's log these results to Arize and see how they compare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll create a dataset to store our questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arize.experimental.datasets import ArizeDatasetsClient\n",
    "from uuid import uuid1\n",
    "from arize.experimental.datasets.experiments.types import (\n",
    "    ExperimentTaskResultColumnNames,\n",
    "    EvaluationResultColumnNames,\n",
    ")\n",
    "from arize.experimental.datasets.utils.constants import GENERATIVE\n",
    "import pandas as pd\n",
    "\n",
    "# Set up the arize client\n",
    "arize_client = ArizeDatasetsClient(developer_key=DEVELOPER_KEY, api_key=API_KEY)\n",
    "dataset = None\n",
    "dataset_name = \"rag-experiments-\" + str(uuid1())[:3]\n",
    "\n",
    "dataset_id = arize_client.create_dataset(\n",
    "    space_id=SPACE_ID,\n",
    "    dataset_name=dataset_name,\n",
    "    dataset_type=GENERATIVE,\n",
    "    data=questions_df,\n",
    ")\n",
    "dataset = arize_client.get_dataset(space_id=SPACE_ID, dataset_id=dataset_id)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll define which columns of our dataframe will be mapped to outputs and which will be mapped to evaluation labels and explanations.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column mappings for task\n",
    "task_cols = ExperimentTaskResultColumnNames(\n",
    "    example_id=\"example_id\", result=\"output\"\n",
    ")\n",
    "# Define column mappings for evaluator\n",
    "relevance_evaluator_cols = EvaluationResultColumnNames(\n",
    "    label=\"relevance\",\n",
    "    explanation=\"relevance_explanation\",\n",
    ")\n",
    "correctness_evaluator_cols = EvaluationResultColumnNames(\n",
    "    label=\"correctness\",\n",
    "    explanation=\"correctness_explanation\",\n",
    ")\n",
    "\n",
    "\n",
    "def log_experiment_to_arize(experiment_df, experiment_name):\n",
    "    experiment_df[\"example_id\"] = dataset[\"id\"]\n",
    "    return arize_client.log_experiment(\n",
    "        space_id=SPACE_ID,\n",
    "        experiment_name=experiment_name + \"-\" + str(uuid1())[:2],\n",
    "        experiment_df=experiment_df,\n",
    "        task_columns=task_cols,\n",
    "        evaluator_columns={\n",
    "            \"correctness\": correctness_evaluator_cols,\n",
    "            \"relevance\": relevance_evaluator_cols,\n",
    "        },\n",
    "        dataset_name=dataset_name,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run it for each of our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Experiments for k-size\n",
    "reset_vector_store(vector_store, chunk_size=1000, chunk_overlap=20)\n",
    "k_2_chunk_1000_overlap_20 = run_rag(vector_store, questions_df, k_value=2)\n",
    "k_4_chunk_1000_overlap_20 = run_rag(vector_store, questions_df, k_value=4)\n",
    "k_10_chunk_1000_overlap_20 = run_rag(vector_store, questions_df, k_value=10)\n",
    "k_2_chunk_1000_overlap_20 = run_evaluators(k_2_chunk_1000_overlap_20)\n",
    "k_4_chunk_1000_overlap_20 = run_evaluators(k_4_chunk_1000_overlap_20)\n",
    "k_10_chunk_1000_overlap_20 = run_evaluators(k_10_chunk_1000_overlap_20)\n",
    "\n",
    "log_experiment_to_arize(k_2_chunk_1000_overlap_20, \"k_2_chunk_1000_overlap_20\")\n",
    "log_experiment_to_arize(k_4_chunk_1000_overlap_20, \"k_4_chunk_1000_overlap_20\")\n",
    "log_experiment_to_arize(\n",
    "    k_10_chunk_1000_overlap_20, \"k_10_chunk_1000_overlap_20\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments for chunk size\n",
    "reset_vector_store(vector_store, chunk_size=200, chunk_overlap=10)\n",
    "k_2_chunk_200_overlap_10 = run_rag(vector_store, questions_df, k_value=2)\n",
    "reset_vector_store(vector_store, chunk_size=500, chunk_overlap=20)\n",
    "k_2_chunk_500_overlap_20 = run_rag(vector_store, questions_df, k_value=2)\n",
    "reset_vector_store(vector_store, chunk_size=1000, chunk_overlap=50)\n",
    "k_2_chunk_1000_overlap_50 = run_rag(vector_store, questions_df, k_value=2)\n",
    "\n",
    "k_2_chunk_200_overlap_10 = run_evaluators(k_2_chunk_200_overlap_10)\n",
    "k_2_chunk_500_overlap_20 = run_evaluators(k_2_chunk_500_overlap_20)\n",
    "k_2_chunk_1000_overlap_50 = run_evaluators(k_2_chunk_1000_overlap_50)\n",
    "\n",
    "log_experiment_to_arize(k_2_chunk_200_overlap_10, \"k_2_chunk_200_overlap_10\")\n",
    "log_experiment_to_arize(k_2_chunk_500_overlap_20, \"k_2_chunk_500_overlap_20\")\n",
    "log_experiment_to_arize(k_2_chunk_1000_overlap_50, \"k_2_chunk_1000_overlap_50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the experiment results in the Arize UI and see how each RAG method performs.\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/arize-assets/tutorials/images/couchbase-rag-experiment.png\" width=\"800\"/>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
