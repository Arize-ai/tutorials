{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "425fb020-e864-40ce-a31f-8da40c73d14b",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "    <img alt=\"arize logo\" src=\"https://storage.googleapis.com/arize-assets/arize-logo-white.jpg\" width=\"300\"/>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/arize/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/client_python\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://arize-ai.slack.com/join/shared_invite/zt-11t1vbu4x-xkBIHmOREQnYnYDH1GDfCg\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "\n",
    "<center><h1>Evaluating Agentic RAG using Arize + Couchbase</h1></center>\n",
    "\n",
    "\n",
    "This tutorial is adapted from the [Langgraph Agentic RAG notebook](https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_agentic_rag.ipynb).\n",
    "\n",
    "\n",
    "This guide shows you how to create a Retrieval Augmented Generation (RAG) Agent using Couchbase Vectorstore and evaluate performance with Arize. Agentic RAG combines RAG with the power of agents. [Retrieval Agents](https://python.langchain.com/v0.2/docs/tutorials/qa_chat_history/#agents) are useful when we want to make decisions about whether to retrieve from an index. To implement a Retrieval Agent, we simply need to give an LLM access to a retriever tool.\n",
    "\n",
    "We'll go through the following steps:\n",
    "\n",
    "* Create a Agentic RAG QA chatbot with OpenAI, Langgraph, Couchbase and Agent Catalog\n",
    "\n",
    "* Trace the agent's function calls including retrieval and LLM calls using Arize\n",
    "\n",
    "* Create a dataset to benchmark performance\n",
    "\n",
    "* Evaluate performance using LLM as a judge\n",
    "\n",
    "* Experiment with different chunk sizes, overlaps, and k number of documents retrieved to see how these affect the performance of the Agentic RAG\n",
    "\n",
    "* Compare these experiments in Arize\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Notebook Setup\n",
    "\n",
    "First, let's download the required packages and set our API keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969fb438",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain-openai langchain-community langchain langgraph langgraph.prebuilt openai langchain-couchbase agentc langchain-huggingface langchain_core\n",
    "\n",
    "%pip install -qq \"arize-phoenix[evals]\" arize-otel openinference-instrumentation-openai openinference-instrumentation-langchain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b21090",
   "metadata": {},
   "source": [
    "#### Set API Keys\n",
    "\n",
    "To follow along with this tutorial, you'll need to [sign up for Arize](https://app.arize.com/auth/join) and get your Space, API and Developer keys. You can see the [guide here](https://docs.arize.com/arize/llm-tracing/quickstart-llm#get-your-api-keys). You will also need an [OpenAI key](https://openai.com/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4958a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Enter here or set as environment variables\n",
    "SPACE_ID = globals().get(\"SPACE_ID\") or getpass(\n",
    "    \"ðŸ”‘ Enter your Arize Space ID: \"\n",
    ")\n",
    "API_KEY = globals().get(\"API_KEY\") or getpass(\"ðŸ”‘ Enter your Arize API Key: \")\n",
    "OPENAI_API_KEY = globals().get(\"OPENAI_API_KEY\") or getpass(\n",
    "    \"ðŸ”‘ Enter your OpenAI API key: \"\n",
    ")\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d3365a",
   "metadata": {},
   "source": [
    "### Set up Arize Tracing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85841043",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arize.otel import register\n",
    "\n",
    "# Setup tracer provider via our convenience function\n",
    "tracer_provider = register(\n",
    "    space_id = SPACE_ID, #\n",
    "    api_key = API_KEY,\n",
    "    project_name = \"langgraph-agentic-rag\", # name this to whatever you would like\n",
    ")\n",
    "\n",
    "# Import the automatic instrumentor from OpenInference\n",
    "from openinference.instrumentation.langchain import LangChainInstrumentor\n",
    "\n",
    "# Finish automatic instrumentation\n",
    "LangChainInstrumentor().instrument(tracer_provider=tracer_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afe9d21",
   "metadata": {},
   "source": [
    "# Setup Couchbase\n",
    "\n",
    "You'll need to setup your Couchbase cluster by doing the following:\n",
    "1. Create an account at [Couchbase Cloud](https://cloud.couchbase.com/)\n",
    "2. Create a free cluster with the Data, Index, and Search services enabled*\n",
    "3. Create cluster access credentials\n",
    "4. Allow access to the cluster from your local machine\n",
    "5. Create a bucket to store your documents\n",
    "\n",
    "*The Search Service will be used to perform Semantic Search later when we use Agent catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f7697c",
   "metadata": {},
   "source": [
    "### Initialize Couchbase cluster\n",
    "\n",
    "Once you've setup your cluster, you can connect to it using langchain's couchbase package.\n",
    "\n",
    "Collect the following information from your cluster:\n",
    "- Connection string\n",
    "- Username\n",
    "- Password\n",
    "- Bucket name\n",
    "- Scope name\n",
    "- Collection name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c1f7a5",
   "metadata": {},
   "source": [
    "Before this step, you must also create a search index. You can do this by going to the Couchbase UI and clicking on the \"Search\" tab. Make sure the names match up with the ones we've defined above.\n",
    "\n",
    "Link below:\n",
    "https://docs.couchbase.com/cloud/vector-search/create-vector-search-index-ui.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fd5ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "from couchbase.auth import PasswordAuthenticator\n",
    "from couchbase.cluster import Cluster\n",
    "from couchbase.options import ClusterOptions\n",
    "from langchain_couchbase.vectorstores import CouchbaseVectorStore\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "#Cluster settings\n",
    "CB_CONN_STRING = getpass(\"Enter the connection string for the Couchbase cluster: \")\n",
    "CB_USERNAME = getpass(\"Enter the username for the Couchbase cluster: \")\n",
    "CB_PASSWORD = getpass(\"Enter the password for the Couchbase cluster: \")\n",
    "\n",
    "BUCKET_NAME = \"\" #enter your bucket name\n",
    "SCOPE_NAME = \"\" #enter your scope name\n",
    "COLLECTION_NAME = \"\" #enter your collection name\n",
    "SEARCH_INDEX_NAME = \"\" #enter your search index name\n",
    "\n",
    "#connect to couchbase cluster\n",
    "auth = PasswordAuthenticator(CB_USERNAME, CB_PASSWORD)\n",
    "options = ClusterOptions(auth)\n",
    "options.apply_profile(\"wan_development\")\n",
    "cluster = Cluster(CB_CONN_STRING, options)\n",
    "cluster.wait_until_ready(timedelta(seconds=5))\n",
    "\n",
    "#Initialize vector store\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L12-v2\")\n",
    "vector_store = CouchbaseVectorStore(\n",
    "    cluster=cluster,\n",
    "    bucket_name=BUCKET_NAME,\n",
    "    scope_name=SCOPE_NAME,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    embedding=embeddings,\n",
    "    index_name=SEARCH_INDEX_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a16d21",
   "metadata": {},
   "source": [
    "Since we will test multiple runs, we create a convenience function that will reset the vector store with new different chunk sizes and overlaps.  Documents content will be sourced from 3 blog posts by Lilian Weng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc86a87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Define the reset_vector_store function so we can run experiments with different chunk sizes\n",
    "def reset_vector_store(vector_store, chunk_size=1024, chunk_overlap=20):\n",
    "    try:    \n",
    "        results = vector_store.similarity_search(\n",
    "            k=1000,\n",
    "            query=\"\",  # Use an empty query or a specific one if needed\n",
    "            search_options={\n",
    "                \"query\": {\"field\": \"metadata.source\", \"match\": \"lilian_weng_blog\"}\n",
    "            },\n",
    "        )\n",
    "        if results:\n",
    "            deleted_ids = []\n",
    "            for result in results:\n",
    "                deleted_ids.append(result.id)\n",
    "            vector_store.delete(ids=deleted_ids)\n",
    "        # Load documents from a URL or file\n",
    "            urls = [\n",
    "                \"https://lilianweng.github.io/posts/2024-07-07-hallucination/\",\n",
    "                \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "                \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "            ]\n",
    "            docs = [WebBaseLoader(url).load() for url in urls]\n",
    "            docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "            # Use RecursiveCharacterTextSplitter\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=chunk_overlap,\n",
    "                separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],  # Hierarchical separators\n",
    "            )\n",
    "            doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "            # Adding metadata to documents\n",
    "            for i, doc in enumerate(doc_splits):\n",
    "                doc.metadata[\"source\"] = \"lilian_weng_blog\"\n",
    "            try:\n",
    "                vector_store.add_documents(doc_splits)\n",
    "            except ValueError as e:\n",
    "                print(f\"Failed to insert documents: {e}\")\n",
    "            return vector_store\n",
    "    except ValueError as e:\n",
    "        print(f\"Search failed with error: {e}\")\n",
    "\n",
    "# Reset the vector store\n",
    "reset_vector_store(vector_store)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74e4532",
   "metadata": {},
   "source": [
    "## Retriever Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b05193",
   "metadata": {},
   "source": [
    "### Create tools and prompts with Agent Catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225d2277-45b2-4ae8-a7d6-62b07fb4a002",
   "metadata": {},
   "source": [
    "Fetch our retriever tool from the Agent Catalog using the agentc provider. In the future, when more tools (and/or prompts) are required and the application grows more complex, Agent Catalog SDK and CLI can be used to automatically fetch the tools based on the use case (semantic search) or by name.\n",
    "\n",
    "For instructions on how this tool was created and more capabilities of Agent catalog, please refer to the documentation [here](https://couchbaselabs.github.io/agent-catalog/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b97bdd8-d7e3-444d-ac96-5ef4725f9048",
   "metadata": {},
   "outputs": [],
   "source": [
    "import agentc.langchain\n",
    "import agentc\n",
    "from langchain_core.tools import tool\n",
    "import os\n",
    "\n",
    "# For retrieval from the local catalog\n",
    "provider = agentc.Provider(\n",
    "    decorator=lambda t: tool(t.func)\n",
    ")\n",
    "\n",
    "# In case the tools were published to the Couchbase cluster beforehand\n",
    "# provider = agentc.Provider(\n",
    "#     decorator=lambda t: tool(t.func),\n",
    "#     secrets={\"CB_USERNAME\": CB_USERNAME,\n",
    "#             \"CB_PASSWORD\": CB_PASSWORD,\n",
    "#             \"CB_CONN_STRING\": CB_CONN_STRING})\n",
    "\n",
    "# This is the tool that will be used to retrieve documents from the vector store\n",
    "retriever_tool = provider.get_item(name=\"retriever_tool\", item_type=\"tool\")\n",
    "\n",
    "tools = retriever_tool\n",
    "\n",
    "print (retriever_tool)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6e8f78-1ef7-42ad-b2bf-835ed5850553",
   "metadata": {},
   "source": [
    "# Create Agent \n",
    "\n",
    "## Agent State\n",
    " \n",
    "We will define a graph of agents to help all involved agents communicate with each other better.\n",
    "Agents communicate through a `state` object that is passed around to each node and modified with output from that node.\n",
    "\n",
    "Our state will be a list of `messages` and each node in our graph will append to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e378706-47d5-425a-8ba0-57b9acffbd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Sequence, TypedDict\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    # The add_messages function defines how an update should be processed\n",
    "    # Default is to replace. add_messages says \"append\"\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc949d42-8a34-4231-bff0-b8198975e2ce",
   "metadata": {},
   "source": [
    "## Define the Nodes and Edges\n",
    "\n",
    "We can lay out an agentic RAG graph like this:\n",
    "\n",
    "* The state is a set of messages\n",
    "* Each node will update (append to) state\n",
    "* Conditional edges decide which node to visit next\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278d1d83-dda6-4de4-bf8b-be9965c227fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Literal, Sequence, TypedDict\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "### Edges\n",
    "\n",
    "def grade_documents(state) -> Literal[\"generate\", \"rewrite\"]:\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        str: A decision for whether the documents are relevant or not\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK RELEVANCE---\")\n",
    "\n",
    "    # Data model\n",
    "    class grade(BaseModel):\n",
    "        \"\"\"Binary score for relevance check.\"\"\"\n",
    "\n",
    "        binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
    "\n",
    "    # LLM\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4o\", streaming=True)\n",
    "\n",
    "    # LLM with tool and validation\n",
    "    llm_with_tool = model.with_structured_output(grade)\n",
    "\n",
    "\n",
    "    #fetch a prompt called \"grade_documents\" from the Agent Catalog\n",
    "    grade_documents_prompt = PromptTemplate(\n",
    "       template=provider.get_item(name=\"grade_documents\", item_type=\"prompt\").prompt.render(),\n",
    "       input_variables=[\"context\", \"question\"],\n",
    "   )\n",
    "\n",
    "    print (grade_documents_prompt)\n",
    "\n",
    "    # Chain\n",
    "    chain = grade_documents_prompt | llm_with_tool\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    question = messages[0].content\n",
    "    docs = last_message.content\n",
    "\n",
    "    scored_result = chain.invoke({\"question\": question, \"context\": docs})\n",
    "\n",
    "    score = scored_result.binary_score\n",
    "\n",
    "    if score == \"yes\":\n",
    "        print(\"---DECISION: DOCS RELEVANT---\")\n",
    "        return \"generate\"\n",
    "\n",
    "    else:\n",
    "        print(\"---DECISION: DOCS NOT RELEVANT---\")\n",
    "        print(score)\n",
    "        return \"rewrite\"\n",
    "\n",
    "\n",
    "### Nodes\n",
    "\n",
    "def agent(state):\n",
    "    \"\"\"\n",
    "    Invokes the agent model to generate a response based on the current state. Given\n",
    "    the question, it will decide to retrieve using the retriever tool, or simply end.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with the agent response appended to messages\n",
    "    \"\"\"\n",
    "    print(\"---CALL AGENT---\")\n",
    "    messages = state[\"messages\"]\n",
    "    model = ChatOpenAI(temperature=0, streaming=True, model=\"gpt-4-turbo\")\n",
    "    model = model.bind_tools(tools)\n",
    "    response = model.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "def rewrite(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "\n",
    "    msg = [\n",
    "        HumanMessage(\n",
    "            content=f\"\"\" \\n \n",
    "    Look at the input and try to reason about the underlying semantic intent / meaning. \\n \n",
    "    Here is the initial question:\n",
    "    \\n ------- \\n\n",
    "    {question} \n",
    "    \\n ------- \\n\n",
    "    Formulate an improved question: \"\"\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Grader\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4-0125-preview\", streaming=True)\n",
    "    response = model.invoke(msg)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "         dict: The updated state with re-phrased question\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    docs = last_message.content\n",
    "\n",
    "    # Prompt\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "    # LLM\n",
    "    llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0, streaming=True)\n",
    "    # Post-processing\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # Chain\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Run\n",
    "    response = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "print(\"*\" * 20 + \"Prompt[rlm/rag-prompt]\" + \"*\" * 20)\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")  # Show what the prompt looks like\n",
    "prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955882ef-7467-48db-ae51-de441f2fc3a7",
   "metadata": {},
   "source": [
    "## Define Graph\n",
    "\n",
    "* Start with an agent, `call_model`\n",
    "* Agent makes a decision to call a function\n",
    "* If so, then `action` to call tool (retriever)\n",
    "* Then call agent with the tool output added to messages (`state`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8718a37f-83c2-4f16-9850-e61e0f49c3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Define the nodes we will cycle between\n",
    "workflow.add_node(\"agent\", agent)  # agent\n",
    "retrieve = ToolNode(retriever_tool)\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieval\n",
    "workflow.add_node(\"rewrite\", rewrite)  # Re-writing the question\n",
    "workflow.add_node(\n",
    "    \"generate\", generate\n",
    ")  # Generating a response after we know the documents are relevant\n",
    "# Call agent node to decide to retrieve or not\n",
    "workflow.add_edge(START, \"agent\")\n",
    "\n",
    "# Decide whether to retrieve\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    # Assess agent decision\n",
    "    tools_condition,\n",
    "    {\n",
    "        # Translate the condition outputs to nodes in our graph\n",
    "        \"tools\": \"retrieve\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Edges taken after the `action` node is called.\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    # Assess agent decision\n",
    "    grade_documents,\n",
    ")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "workflow.add_edge(\"rewrite\", \"agent\")\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e143dd",
   "metadata": {},
   "source": [
    "Let's visualize the graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5a1d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c4f7e4",
   "metadata": {},
   "source": [
    "## Let's run the graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7649f05a-cb67-490d-b24a-74d41895139a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        (\"user\", \"What does Lilian Weng say about the types of adversarial attacks on LLMs?\"),\n",
    "    ]\n",
    "}\n",
    "for output in graph.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint.pprint(f\"Output from node '{key}':\")\n",
    "        pprint.pprint(value, indent=2, width=80, depth=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f0f46d",
   "metadata": {},
   "source": [
    "### View the trace in the Arize UI\n",
    "Once you've run a single query, you can see the trace in the [Arize UI](https://app.arize.com/) with each step taken by the retriever, the embedding, and the LLM query.\n",
    "\n",
    "Click through the queries to better understand how the query engine is performing. Arize can be used to understand and troubleshoot your RAG app by surfacing:\n",
    " - Application latency\n",
    " - Token usage\n",
    " - Runtime exceptions\n",
    " - Retrieved documents\n",
    " - Embeddings\n",
    " - LLM parameters\n",
    " - Prompt templates\n",
    " - Tool descriptions\n",
    " - LLM function calls\n",
    " - And more!\n",
    "\n",
    " <img src=\"https://storage.googleapis.com/arize-assets/tutorials/images/couchbase_langgraph_agent_traces.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0d6642",
   "metadata": {},
   "source": [
    "### Generate a synthetic dataset of questions \n",
    "\n",
    "We will run our Agent against the dataset of questions we generate, and then evaluate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3b9fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Define a template for generating questions\n",
    "GEN_TEMPLATE = \"\"\"\n",
    "You are an assistant that generates Q&A questions about the content below.\n",
    "\n",
    "The questions should involve the content, specific facts and figures,\n",
    "names, and elements of the story. Do not ask any questions where the answer is\n",
    "not in the content.\n",
    "\n",
    "Respond with one question per line. Do not include any numbering at the beginning of each line. Do not include any category headings.\n",
    "Generate 10 questions. Be sure there are no duplicate questions.\n",
    "\n",
    "[START CONTENT]\n",
    "{content}\n",
    "[END CONTENT]\n",
    "\"\"\"\n",
    "\n",
    "# Load the content you want to generate questions about\n",
    "content = \"\"\"\n",
    "Lilian Weng discusses various aspects of adversarial attacks on LLMs and prompt engineering techniques. Make sure to use Lilian Weng's name in the questions.\n",
    "\"\"\"\n",
    "\n",
    "# Format the template with the content\n",
    "formatted_template = GEN_TEMPLATE.format(content=content)\n",
    "\n",
    "# Initialize the language model\n",
    "model = ChatOpenAI(model=\"gpt-4o\", max_tokens=1300)\n",
    "\n",
    "# Generate questions using the language model\n",
    "response = model.invoke(formatted_template)\n",
    "\n",
    "# Extract the content from the response object\n",
    "questions_content = response.content  # Directly access the content attribute\n",
    "\n",
    "# Split the response into individual questions\n",
    "questions = questions_content.strip().split(\"\\n\")\n",
    "\n",
    "# Create a dataframe to store the questions\n",
    "questions_df = pd.DataFrame(questions, columns=[\"input\"])\n",
    "\n",
    "# Display the first few questions\n",
    "questions_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88360580",
   "metadata": {},
   "source": [
    "### Run our Agent against the list of generated questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09a9da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_rag(questions_df, k_value=2):\n",
    "    response_df = questions_df.copy(deep=True)\n",
    "    for index, row in response_df.iterrows():\n",
    "        inputs = {\n",
    "            \"messages\": [\n",
    "                (\"user\", f\"{row['input']}\"),\n",
    "            ]\n",
    "        }\n",
    "        for output in graph.stream(inputs):\n",
    "            for key, value in output.items():\n",
    "                if key == \"generate\":\n",
    "                    response_df.loc[index, \"output\"] = value[\"messages\"][-1]\n",
    "                if key == \"retrieve\":\n",
    "                    response_df.loc[index, \"reference\"] = value[\"messages\"][-1].content\n",
    "    text_columns = [\"input\", \"output\", \"reference\"]\n",
    "    response_df[text_columns] = response_df[text_columns].apply(\n",
    "        lambda x: x.astype(str)\n",
    "    )\n",
    "    return response_df\n",
    "\n",
    "response_df = run_rag(questions_df, k_value=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a016b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's inspect the results\n",
    "response_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028f9adb",
   "metadata": {},
   "source": [
    "# Evaluating your Agentic RAG using LLM as a Judge\n",
    "\n",
    "Now that we have run a set of test cases, we can create evaluators to measure performance of our run. This way, we don't have to manually inspect every single trace to see if the LLM is doing the right thing.  First, we'll define the prompts for the evaluators.\n",
    "\n",
    "There are two evaluators we will use for this example. \n",
    "\n",
    "1. *Retrieval Relevance:* This evaluator checks if the reference text selected by the retriever is relevant to the question.\n",
    "2. *QA Correctness:* This evaluator checks if the answer correctly answers the question based on the reference text provided.\n",
    "\n",
    "\n",
    "(For more information on these and other prebuilt evaluators see [here](https://docs.arize.com/arize/llm-evaluation-and-annotations/arize-evaluators-llm-as-a-judge#choose-an-evaluator).)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3062d687",
   "metadata": {},
   "source": [
    "We will be creating an LLM as a judge using prebuilt prompt templates, taking the spans recorded by Phoenix, and then giving them labels using the `llm_classify` function. This function uses LLMs to evaluate your LLM calls and gives them labels and explanations. You can read more detail [here](https://docs.arize.com/phoenix/api/evals#phoenix.evals.llm_classify)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352b54bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.evals import (\n",
    "    RAG_RELEVANCY_PROMPT_RAILS_MAP,\n",
    "    RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
    "    QA_PROMPT_RAILS_MAP,\n",
    "    QA_PROMPT_TEMPLATE,\n",
    "    OpenAIModel,\n",
    "    llm_classify\n",
    ")\n",
    "\n",
    "# The rails is used to hold the output to specific values based on the template\n",
    "RELEVANCE_RAILS = list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values())\n",
    "QA_RAILS = list(QA_PROMPT_RAILS_MAP.values())\n",
    "\n",
    "relevance_eval_df = llm_classify(\n",
    "    dataframe=response_df,\n",
    "    template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
    "    model=OpenAIModel(model=\"gpt-4o\"),\n",
    "    rails=RELEVANCE_RAILS,\n",
    "    provide_explanation=True,\n",
    "    include_prompt=True,\n",
    "    concurrency=4,\n",
    ")\n",
    "\n",
    "correctness_eval_df = llm_classify(\n",
    "    dataframe=response_df,\n",
    "    template=QA_PROMPT_TEMPLATE,\n",
    "    model=OpenAIModel(model=\"gpt-4o\"),\n",
    "    rails=QA_RAILS,\n",
    "    provide_explanation=True,\n",
    "    include_prompt=True,\n",
    "    concurrency=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd71b5d6",
   "metadata": {},
   "source": [
    "Let's look at and inspect the results of our evaluatiion!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14968601",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba555dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "correctness_eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7438ac7",
   "metadata": {},
   "source": [
    "## Experiment with different k-values and chunk sizes\n",
    "\n",
    "Re-run experiments with different k-values and chunk sizes. Then log the results to Arize to see how the performance changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6987dc",
   "metadata": {},
   "source": [
    "Let's setup our evaluators to see how the performance changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f85ca69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluators(rag_df):\n",
    "    relevance_eval_df = llm_classify(\n",
    "        dataframe=rag_df,\n",
    "        template=RELEVANCE_EVAL_TEMPLATE,\n",
    "        model=OpenAIModel(model=\"gpt-4o\"),\n",
    "        rails=RELEVANCE_RAILS,\n",
    "        provide_explanation=True,\n",
    "        concurrency=4,\n",
    "    )\n",
    "    rag_df[\"relevance\"] = relevance_eval_df[\"label\"]\n",
    "    rag_df[\"relevance_explanation\"] = relevance_eval_df[\"explanation\"]\n",
    "\n",
    "    correctness_eval_df = llm_classify(\n",
    "        dataframe=rag_df,\n",
    "        template=CORRECTNESS_EVAL_TEMPLATE,\n",
    "        model=OpenAIModel(model=\"gpt-4o\"),\n",
    "        rails=CORRECTNESS_RAILS,\n",
    "        provide_explanation=True,\n",
    "        concurrency=4,\n",
    "    )\n",
    "    rag_df[\"correctness\"] = correctness_eval_df[\"label\"]\n",
    "    rag_df[\"correctness_explanation\"] = correctness_eval_df[\"explanation\"]\n",
    "    return rag_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f6b85e",
   "metadata": {},
   "source": [
    "Let's log these results to Arize and see how they compare. \n",
    "\n",
    "First we'll create a dataset to store our questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c978fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arize.experimental.datasets import ArizeDatasetsClient\n",
    "from uuid import uuid1\n",
    "from arize.experimental.datasets.experiments.types import (\n",
    "    ExperimentTaskResultColumnNames,\n",
    "    EvaluationResultColumnNames,\n",
    ")\n",
    "from arize.experimental.datasets.utils.constants import GENERATIVE\n",
    "import pandas as pd\n",
    "\n",
    "# Set up the arize client\n",
    "arize_client = ArizeDatasetsClient(api_key=API_KEY)\n",
    "dataset = None\n",
    "dataset_name = \"rag-experiments-\" + str(uuid1())[:3]\n",
    "\n",
    "dataset_id = arize_client.create_dataset(\n",
    "    space_id=SPACE_ID,\n",
    "    dataset_name=dataset_name,\n",
    "    dataset_type=GENERATIVE,\n",
    "    data=questions_df,\n",
    ")\n",
    "dataset = arize_client.get_dataset(space_id=SPACE_ID, dataset_id=dataset_id)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c796409",
   "metadata": {},
   "source": [
    "Next we'll define which columns of our dataframe will be mapped to outputs and which will be mapped to evaluation labels and explanations.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8f2de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column mappings for task\n",
    "task_cols = ExperimentTaskResultColumnNames(\n",
    "    example_id=\"example_id\", result=\"output\"\n",
    ")\n",
    "# Define column mappings for evaluator\n",
    "relevance_evaluator_cols = EvaluationResultColumnNames(\n",
    "    label=\"relevance\",\n",
    "    explanation=\"relevance_explanation\",\n",
    ")\n",
    "correctness_evaluator_cols = EvaluationResultColumnNames(\n",
    "    label=\"correctness\",\n",
    "    explanation=\"correctness_explanation\",\n",
    ")\n",
    "\n",
    "\n",
    "def log_experiment_to_arize(experiment_df, experiment_name):\n",
    "    experiment_df[\"example_id\"] = dataset[\"id\"]\n",
    "    return arize_client.log_experiment(\n",
    "        space_id=SPACE_ID,\n",
    "        experiment_name=experiment_name + \"-\" + str(uuid1())[:2],\n",
    "        experiment_df=experiment_df,\n",
    "        task_columns=task_cols,\n",
    "        evaluator_columns={\n",
    "            \"correctness\": correctness_evaluator_cols,\n",
    "            \"relevance\": relevance_evaluator_cols,\n",
    "        },\n",
    "        dataset_name=dataset_name,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cd79cd",
   "metadata": {},
   "source": [
    "Now let's run it for each of our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49e5b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Experiments for different k-values\n",
    "reset_vector_store(vector_store, chunk_size=1024, chunk_overlap=20)\n",
    "k_2_chunk_1024_overlap_20 = run_rag(questions_df, k_value=2)\n",
    "k_4_chunk_1024_overlap_20 = run_rag(questions_df, k_value=4)\n",
    "\n",
    "# Run evaluators\n",
    "k_2_chunk_1024_overlap_20 = run_evaluators(k_2_chunk_1024_overlap_20)\n",
    "k_4_chunk_1024_overlap_20 = run_evaluators(k_4_chunk_1024_overlap_20)\n",
    "\n",
    "# Log experiments to Arize\n",
    "log_experiment_to_arize(k_2_chunk_1024_overlap_20, \"k_2_chunk_1024_overlap_20\")\n",
    "log_experiment_to_arize(k_4_chunk_1024_overlap_20, \"k_4_chunk_1024_overlap_20\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbf26bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments for different chunk sizes\n",
    "reset_vector_store(vector_store, chunk_size=200, chunk_overlap=20)\n",
    "k_2_chunk_200_overlap_20 = run_rag(questions_df, k_value=2)\n",
    "reset_vector_store(vector_store, chunk_size=500, chunk_overlap=20)\n",
    "k_2_chunk_500_overlap_20 = run_rag(questions_df, k_value=2)\n",
    "\n",
    "# Run evaluators\n",
    "k_2_chunk_200_overlap_20 = run_evaluators(k_2_chunk_200_overlap_20)\n",
    "k_2_chunk_500_overlap_20 = run_evaluators(k_2_chunk_500_overlap_20)\n",
    "\n",
    "# Log experiments to Arize\n",
    "log_experiment_to_arize(k_2_chunk_200_overlap_20, \"k_2_chunk_200_overlap_20\")\n",
    "log_experiment_to_arize(k_2_chunk_500_overlap_20, \"k_2_chunk_500_overlap_20\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4454781",
   "metadata": {},
   "source": [
    "### You can compare the experiment results in the Arize UI and see how each RAG method performs.  How did these changes impact performance?\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/arize-assets/tutorials/images/couchbase-rag-experiment.png\" width=\"800\"/>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
