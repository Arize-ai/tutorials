{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lab 5: RAG + Agentic RAG\n",
        "\n",
        "This lab introduces Retrieval Augmented Generation (RAG) and Agentic RAG patterns. You'll:\n",
        "- Build a simple RAG pipeline over curated local travel guides\n",
        "- Extend it to Agentic RAG where an agent decides when to retrieve\n",
        "- Add observability with Arize AX (OpenInference/OTel)\n",
        "\n",
        "Format follows the style of previous labs in this `labs/` folder.\n",
        "\n",
        "---\n",
        "\n",
        "### Objectives\n",
        "- Master Retrieval Augmented Generation (RAG) techniques\n",
        "- Build agents that can effectively use external knowledge\n",
        "- Implement agentic RAG patterns\n",
        "\n",
        "### Prerequisites\n",
        "- Python 3.10+\n",
        "- `OPENAI_API_KEY` for LLM and embeddings\n",
        "- Optional: `ARIZE_SPACE_ID` and `ARIZE_API_KEY` for tracing to Arize AX\n",
        "\n",
        "### What you'll build\n",
        "- A basic RAG retriever using `local_guides.json`\n",
        "- A minimal Agentic RAG example where the model calls a tool to retrieve\n",
        "- Optional tracing so you can observe prompts, tool calls, and spans\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup: installs, env, and imports\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment  \n",
        "project_root = Path.cwd()\n",
        "if project_root.name != 'agent-mastery-course':\n",
        "    project_root = project_root.parent\n",
        "load_dotenv(project_root / 'backend' / '.env')\n",
        "\n",
        "# Paths\n",
        "LAB_ROOT = Path(__file__).resolve().parent if \"__file__\" in globals() else Path.cwd()\n",
        "BACKEND_DIR = LAB_ROOT.parent / \"backend\"\n",
        "LOCAL_GUIDES = BACKEND_DIR / \"data\" / \"local_guides.json\"\n",
        "assert LOCAL_GUIDES.exists(), f\"Expected data at {LOCAL_GUIDES}\"\n",
        "\n",
        "print(\"local_guides.json:\", LOCAL_GUIDES)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Arize AX tracing\n",
        "from arize.otel import register\n",
        "from openinference.instrumentation.langchain import LangChainInstrumentor\n",
        "from openinference.instrumentation.litellm import LiteLLMInstrumentor\n",
        "from openinference.instrumentation import using_prompt_template\n",
        "\n",
        "\n",
        "# configure the Phoenix tracer\n",
        "tracer_provider = register(\n",
        "    space_id=os.getenv(\"ARIZE_SPACE_ID\"),\n",
        "    api_key=os.getenv(\"ARIZE_API_KEY\"),\n",
        "    project_name=\"lab5-rag-and-agentic-rag\",\n",
        ")\n",
        "\n",
        "LangChainInstrumentor().instrument(tracer_provider=tracer_provider, include_chains=True, include_agents=True, include_tools=True)\n",
        "LiteLLMInstrumentor().instrument(tracer_provider=tracer_provider, skip_dep_check=True)\n",
        "print(\"Tracing initialized: spans will be exported to Arize AX\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LLM init\n",
        "from typing import Optional, List, Dict, Any\n",
        "\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import InMemoryVectorStore\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.messages import SystemMessage\n",
        "\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    raise ValueError(\"Please set OPENAI_API_KEY to run this notebook.\")\n",
        "\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.2)\n",
        "print(\"LLM ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build a tiny retriever over local_guides.json\n",
        "\n",
        "print(\"ðŸ“š Loading curated travel guides...\")\n",
        "# Load curated docs\n",
        "raw = json.loads(LOCAL_GUIDES.read_text())\n",
        "\n",
        "def _to_doc(row: Dict[str, Any]) -> Optional[Document]:\n",
        "    description = row.get(\"description\")\n",
        "    city = row.get(\"city\")\n",
        "    if not description or not city:\n",
        "        return None\n",
        "    interests = row.get(\"interests\", []) or []\n",
        "    metadata = {\"city\": city, \"interests\": interests, \"source\": row.get(\"source\")}\n",
        "    interest_text = \", \".join(interests) if interests else \"general travel\"\n",
        "    content = f\"City: {city}\\nInterests: {interest_text}\\nGuide: {description}\"\n",
        "    return Document(page_content=content, metadata=metadata)\n",
        "\n",
        "DOCS: List[Document] = []\n",
        "for row in raw:\n",
        "    d = _to_doc(row)\n",
        "    if d:\n",
        "        DOCS.append(d)\n",
        "\n",
        "print(f\"ðŸ“„ Processed {len(DOCS)} travel guide documents\")\n",
        "\n",
        "# In-memory vector store if embeddings are available, else keyword fallback\n",
        "embeddings = None\n",
        "vectorstore = None\n",
        "if OpenAIEmbeddings is not None and InMemoryVectorStore is not None and os.getenv(\"OPENAI_API_KEY\"):\n",
        "    try:\n",
        "        print(\"ðŸ” Initializing semantic search with OpenAI embeddings...\")\n",
        "        embed_model = os.getenv(\"OPENAI_EMBED_MODEL\", \"text-embedding-3-small\")\n",
        "        embeddings = OpenAIEmbeddings(model=embed_model)\n",
        "        vectorstore = InMemoryVectorStore(embedding=embeddings)\n",
        "        print(\"âš¡ Embedding documents into vector store...\")\n",
        "        vectorstore.add_documents(DOCS)\n",
        "        print(f\"âœ… Vector store built with {len(DOCS)} docs using {embed_model}\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Embedding init failed, falling back to keywords: {str(e)}\")\n",
        "\n",
        "\n",
        "def retrieve(destination: str, interests: Optional[str], k: int = 3) -> List[Dict[str, Any]]:\n",
        "    if not DOCS:\n",
        "        print(\"âš ï¸ No documents available for retrieval\")\n",
        "        return []\n",
        "    \n",
        "    query = destination if not interests else f\"{destination} with interests {interests}\"\n",
        "    print(f\"ðŸ”Ž Searching for: '{query}'\")\n",
        "    \n",
        "    if vectorstore is None:\n",
        "        print(\"ðŸ“ Using keyword-based fallback search...\")\n",
        "        return _keyword_fallback(destination, interests, k)\n",
        "    \n",
        "    print(\"ðŸš€ Using semantic vector search...\")\n",
        "    try:\n",
        "        retriever = vectorstore.as_retriever(search_kwargs={\"k\": max(k, 4)})\n",
        "        docs = retriever.invoke(query)\n",
        "        docs = docs[:k]\n",
        "        results = [\n",
        "            {\"content\": d.page_content, \"metadata\": d.metadata, \"score\": float(d.metadata.get(\"score\", 0.0))}\n",
        "            for d in docs\n",
        "        ]\n",
        "        print(f\"ðŸ“‹ Found {len(results)} relevant documents\")\n",
        "        return results\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Vector search failed, using keyword fallback: {str(e)}\")\n",
        "        return _keyword_fallback(destination, interests, k)\n",
        "\n",
        "\n",
        "def _keyword_fallback(destination: str, interests: Optional[str], k: int) -> List[Dict[str, Any]]:\n",
        "    dest_lower = destination.lower()\n",
        "    interest_terms = [t.strip().lower() for t in (interests or \"\").split(\",\") if t.strip()]\n",
        "    \n",
        "    def _score(doc: Document) -> int:\n",
        "        score = 0\n",
        "        if dest_lower and dest_lower.split(\",\")[0] in doc.metadata.get(\"city\", \"\").lower():\n",
        "            score += 2\n",
        "        for term in interest_terms:\n",
        "            if term and term in \" \".join(doc.metadata.get(\"interests\") or []).lower():\n",
        "                score += 1\n",
        "            if term and term in doc.page_content.lower():\n",
        "                score += 1\n",
        "        return score\n",
        "    \n",
        "    ranked = sorted((( _score(d), d) for d in DOCS), key=lambda x: x[0], reverse=True)\n",
        "    out: List[Dict[str, Any]] = []\n",
        "    for sc, d in ranked[:k]:\n",
        "        if sc <= 0 and dest_lower not in d.metadata.get(\"city\", \"\").lower():\n",
        "            continue\n",
        "        out.append({\"content\": d.page_content, \"metadata\": d.metadata, \"score\": float(sc)})\n",
        "    \n",
        "    print(f\"ðŸ“‹ Found {len(out)} relevant documents using keyword matching\")\n",
        "    return out\n",
        "\n",
        "print(\"Docs loaded:\", len(DOCS))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic RAG: simple query then LLM synthesis\n",
        "\n",
        "def _compact(text: str, limit: int = 220) -> str:\n",
        "    cleaned = \" \".join(text.split())\n",
        "    if len(cleaned) <= limit:\n",
        "        return cleaned\n",
        "    truncated = cleaned[:limit].rstrip()\n",
        "    last_space = truncated.rfind(\" \")\n",
        "    if last_space > 0:\n",
        "        truncated = truncated[:last_space]\n",
        "    return truncated.rstrip(\",.;- \")\n",
        "\n",
        "\n",
        "def rag_answer(destination: str, interests: Optional[str] = None) -> str:\n",
        "    print(f\"ðŸŽ¯ Starting Basic RAG for {destination} with interests: {interests}\")\n",
        "    \n",
        "    # Step 1: Retrieve relevant documents\n",
        "    hits = retrieve(destination, interests, k=3)\n",
        "    \n",
        "    # Step 2: Display retrieved documents\n",
        "    if not hits:\n",
        "        print(\"âŒ No relevant documents found\")\n",
        "        context = \"No curated context available.\"\n",
        "    else:\n",
        "        print(f\"ðŸ“– Retrieved Documents:\")\n",
        "        for i, h in enumerate(hits, 1):\n",
        "            city = h['metadata'].get('city', 'Unknown')\n",
        "            score = h.get('score', 0)\n",
        "            content_preview = h['content'][:100] + \"...\" if len(h['content']) > 100 else h['content']\n",
        "            print(f\"  [{i}] {city} (score: {score:.2f})\")\n",
        "            print(f\"      {content_preview}\")\n",
        "        \n",
        "        context = \"\\n\".join([\n",
        "            f\"[{i+1}] {h['metadata'].get('city')}: {h['content']}\" for i, h in enumerate(hits)\n",
        "        ])\n",
        "    \n",
        "    # Step 3: Generate response using LLM\n",
        "    print(\"ðŸ¤– Generating response with LLM...\")\n",
        "    prompt_t = (\n",
        "        \"You are a helpful travel assistant.\\n\"\n",
        "        \"Use the retrieved notes to recommend authentic experiences in {destination} for interests: {interests}.\\n\"\n",
        "        \"Cite the numbered items when you rely on them.\\n\\n\"\n",
        "        \"Context:\\n{context}\"\n",
        "    )\n",
        "    vars_ = {\"destination\": destination, \"interests\": interests or \"local culture\", \"context\": context}\n",
        "    with using_prompt_template(template=prompt_t, variables=vars_, version=\"v1\"):\n",
        "        res = llm.invoke([SystemMessage(content=prompt_t.format(**vars_))])\n",
        "    \n",
        "    print(\"âœ… Basic RAG complete!\")\n",
        "    return res.content\n",
        "\n",
        "# Test Basic RAG\n",
        "print(\"=\" * 50)\n",
        "print(\"Testing Basic RAG\")\n",
        "print(\"=\" * 50)\n",
        "result = rag_answer(\"Lisbon\", \"food, music\")\n",
        "print(f\"\\nðŸ“ Final Answer:\\n{result[:300]}{'...' if len(result) > 300 else ''}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Agentic RAG: give the model a retriever tool\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def local_guides_retrieve(destination: str, interests: Optional[str] = None) -> str:\n",
        "    \"\"\"Retrieve curated local guide notes matching destination and optional interests.\"\"\"\n",
        "    print(f\"ðŸ”§ Tool called: local_guides_retrieve(destination='{destination}', interests='{interests}')\")\n",
        "    hits = retrieve(destination, interests, k=3)\n",
        "    if not hits:\n",
        "        print(\"  ðŸ“­ Tool result: No curated context available\")\n",
        "        return \"No curated context available.\"\n",
        "    \n",
        "    print(f\"  ðŸ“¦ Tool retrieved {len(hits)} documents:\")\n",
        "    for i, h in enumerate(hits, 1):\n",
        "        city = h['metadata'].get('city', 'Unknown')\n",
        "        score = h.get('score', 0)\n",
        "        print(f\"    [{i}] {city} (score: {score:.2f})\")\n",
        "    \n",
        "    lines = [f\"[{i+1}] {h['metadata'].get('city')}: {h['content']}\" for i, h in enumerate(hits)]\n",
        "    result = \"\\n\".join(lines)\n",
        "    print(f\"  âœ… Tool returning {len(result)} characters of context\")\n",
        "    return result\n",
        "\n",
        "\n",
        "def agentic_rag_answer(destination: str, interests: Optional[str] = None) -> str:\n",
        "    print(f\"ðŸ¤– Starting Agentic RAG for {destination} with interests: {interests}\")\n",
        "    \n",
        "    # Step 1: Create agent with retriever tool\n",
        "    tools = [local_guides_retrieve]\n",
        "    agent = llm.bind_tools(tools)\n",
        "    print(f\"ðŸ› ï¸ Agent equipped with {len(tools)} tool(s): {[t.name for t in tools]}\")\n",
        "    \n",
        "    # Step 2: Initial agent call\n",
        "    prompt_t = (\n",
        "        \"You are a travel assistant.\\n\"\n",
        "        \"First, call local_guides_retrieve to get context for {destination}.\\n\"\n",
        "        \"Then synthesize concise recommendations citing items like [1], [2].\"\n",
        "    )\n",
        "    vars_ = {\"destination\": destination}\n",
        "    print(\"ðŸ“¤ Sending initial prompt to agent...\")\n",
        "    \n",
        "    with using_prompt_template(template=prompt_t, variables=vars_, version=\"v1\"):\n",
        "        res = agent.invoke([SystemMessage(content=prompt_t.format(**vars_))])\n",
        "    \n",
        "    out = res.content\n",
        "    print(f\"ðŸ“¥ Agent response received (tool_calls: {len(getattr(res, 'tool_calls', []))})\")\n",
        "    \n",
        "    # Step 3: Handle tool calls if any\n",
        "    if getattr(res, \"tool_calls\", None):\n",
        "        print(f\"ðŸ”„ Processing {len(res.tool_calls)} tool call(s)...\")\n",
        "        try:\n",
        "            call = res.tool_calls[0]\n",
        "            print(f\"  ðŸŽ¯ Tool call: {call['name']} with args: {call.get('args', {})}\")\n",
        "            \n",
        "            if call[\"name\"] == \"local_guides_retrieve\":\n",
        "                args = call.get(\"args\", {}) or {}\n",
        "                args.setdefault(\"destination\", destination)\n",
        "                tool_result = local_guides_retrieve.invoke(args)  # type: ignore\n",
        "                \n",
        "                print(\"ðŸ¤– Sending context to LLM for synthesis...\")\n",
        "                followup = (\n",
        "                    f\"Using this context, write a concise recommendation for {destination}.\\n\"\n",
        "                    f\"Context:\\n{tool_result}\"\n",
        "                )\n",
        "                final = llm.invoke([SystemMessage(content=followup)])\n",
        "                out = final.content\n",
        "                print(\"âœ… Final synthesis complete!\")\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Tool execution failed: {str(e)}\")\n",
        "    else:\n",
        "        print(\"â„¹ï¸ No tool calls made by agent\")\n",
        "    \n",
        "    print(\"âœ… Agentic RAG complete!\")\n",
        "    return _compact(out)\n",
        "\n",
        "# Test Agentic RAG\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Testing Agentic RAG\")\n",
        "print(\"=\" * 50)\n",
        "result = agentic_rag_answer(\"Prague\", \"architecture, coffee\")\n",
        "print(f\"\\nðŸ“ Final Answer:\\n{result[:300]}{'...' if len(result) > 300 else ''}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Demo: Try it out\n",
        "\n",
        "Use the cells below to run both the Basic RAG and Agentic RAG flows.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo parameters\n",
        "DESTINATION = \"Barcelona\"\n",
        "INTERESTS = \"architecture, food\"\n",
        "\n",
        "print(\"Destination:\", DESTINATION)\n",
        "print(\"Interests:\", INTERESTS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run Basic RAG\n",
        "print(\"ðŸš€ Running Basic RAG Demo...\")\n",
        "print(f\"Query: {DESTINATION} with interests: {INTERESTS}\")\n",
        "print(\"-\" * 40)\n",
        "basic = rag_answer(DESTINATION, INTERESTS)\n",
        "print(f\"\\nðŸŽ‰ Basic RAG Result:\\n{basic}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run Agentic RAG\n",
        "print(\"\\nðŸ¤– Running Agentic RAG Demo...\")\n",
        "print(f\"Query: {DESTINATION} with interests: {INTERESTS}\")\n",
        "print(\"-\" * 40)\n",
        "agentic = agentic_rag_answer(DESTINATION, INTERESTS)\n",
        "print(f\"\\nðŸŽ‰ Agentic RAG Result:\\n{agentic}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tips and Next Steps\n",
        "- Explore how the backend example composes multiple agents and tools in `backend/main.py`.\n",
        "- Try swapping embeddings models or adding sources to `local_guides.json`.\n",
        "- With tracing enabled, inspect chains, tools, prompts, and latencies in Arize AX.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
