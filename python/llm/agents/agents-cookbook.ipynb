{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUknhuHKyc-E"
   },
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "    <img alt=\"arize logo\" src=\"https://storage.googleapis.com/arize-assets/arize-logo-white.jpg\" width=\"300\"/>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/arize/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/client_python\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://join.slack.com/t/arize-ai/shared_invite/zt-1px8dcmlf-fmThhDFD_V_48oU7ALan4Q\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "\n",
    "# <center>Using Arize with AI agents</center>\n",
    "\n",
    "This guide shows you how to create and evaluate agents with Arize to improve performance. We'll go through the following steps:\n",
    "\n",
    "* Create a customer support agent using a router template\n",
    "\n",
    "* Trace the agent activity, including function calling\n",
    "\n",
    "* Create a dataset to benchmark performance\n",
    "\n",
    "* Evaluate agent performance using code, human annotation, and LLM as a judge\n",
    "\n",
    "* Experiment with different prompts and models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "baTNFxbwX1e2"
   },
   "source": [
    "# Initial setup\n",
    "\n",
    "\n",
    "We'll setup our libraries, keys, and OpenAI tracing using Phoenix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n69HR7eJswNt"
   },
   "source": [
    "### Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq arize-otel openai openinference-instrumentation-openai opentelemetry-sdk opentelemetry-exporter-otlp gcsfs nest_asyncio arize-phoenix \"arize[Datasets]\" 'httpx<0.28'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQnyEnJisyn3"
   },
   "source": [
    "### Setup Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kfid5cE99yN5"
   },
   "source": [
    "### Setup Tracing\n",
    "\n",
    "To follow with this tutorial, you'll need to sign up for Arize and get your API key. You can see the [guide here](https://docs.arize.com/arize/llm-tracing/quickstart-llm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import open-telemetry dependencies\n",
    "from arize_otel import register_otel, Endpoints\n",
    "\n",
    "# Setup OTEL via our convenience function\n",
    "register_otel(\n",
    "    endpoints=Endpoints.ARIZE,\n",
    "    space_id=getpass(\"ðŸ”‘ Enter your Arize Space ID: \"),\n",
    "    api_key=getpass(\"ðŸ”‘ Enter your Arize API key: \"),\n",
    "    project_name=\"agents-cookbook\",  # name this to whatever you would like\n",
    ")\n",
    "# Import the automatic instrumentor from OpenInference\n",
    "from openinference.instrumentation.openai import OpenAIInstrumentor\n",
    "\n",
    "# Finish automatic instrumentation\n",
    "OpenAIInstrumentor().instrument()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLVAqLi5_KAi"
   },
   "source": [
    "# Create customer support agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_L2NP8TLpNB"
   },
   "source": [
    "We'll be creating a customer support agent using function calling following the architecture below:\n",
    "\n",
    "<img src=\"https://storage.cloud.google.com/arize-assets/tutorials/images/agent_architecture.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xSHfLpf2_kLr"
   },
   "source": [
    "### Setup functions and create customer support agent\n",
    "\n",
    "We have 6 functions that we define below.\n",
    "\n",
    "1. product_comparison\n",
    "2. product_search\n",
    "3. customer_support\n",
    "4. track_package\n",
    "5. product_details\n",
    "6. apply_discount_code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"product_comparison\",\n",
    "            \"description\": \"Compare features of two products.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"product_a_id\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The unique identifier of Product A.\",\n",
    "                    },\n",
    "                    \"product_b_id\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The unique identifier of Product B.\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"product_a_id\", \"product_b_id\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"product_search\",\n",
    "            \"description\": \"Search for products based on criteria.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The search query string.\",\n",
    "                    },\n",
    "                    \"category\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The category to filter the search.\",\n",
    "                    },\n",
    "                    \"min_price\": {\n",
    "                        \"type\": \"number\",\n",
    "                        \"description\": \"The minimum price of the products to search.\",\n",
    "                        \"default\": 0,\n",
    "                    },\n",
    "                    \"max_price\": {\n",
    "                        \"type\": \"number\",\n",
    "                        \"description\": \"The maximum price of the products to search.\",\n",
    "                    },\n",
    "                    \"page\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The page number for pagination.\",\n",
    "                        \"default\": 1,\n",
    "                    },\n",
    "                    \"page_size\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The number of results per page.\",\n",
    "                        \"default\": 20,\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"query\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"customer_support\",\n",
    "            \"description\": \"Get contact information for customer support regarding an issue.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"issue_type\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The type of issue (e.g., billing, technical support).\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"issue_type\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"track_package\",\n",
    "            \"description\": \"Track the status of a package based on the tracking number.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"tracking_number\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The tracking number of the package.\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"tracking_number\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"product_details\",\n",
    "            \"description\": \"Returns details for a given product id\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"product_id\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The id of a product to look up.\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"product_id\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"apply_discount_code\",\n",
    "            \"description\": \"Applies the discount code to a given order.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"order_id\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The id of the order to apply the discount code to.\",\n",
    "                    },\n",
    "                    \"discount_code\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The discount code to apply\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"order_id\", \"discount_code\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4DLzgs8aA-SL"
   },
   "source": [
    "We define a function below called run_prompt, which uses the chat completion call from OpenAI with functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import openai\n",
    "\n",
    "client = openai.Client()\n",
    "\n",
    "\n",
    "def run_prompt(input):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \" \",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": input,\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    print(response)\n",
    "\n",
    "    if (\n",
    "        hasattr(response.choices[0].message, \"tool_calls\")\n",
    "        and response.choices[0].message.tool_calls is not None\n",
    "        and len(response.choices[0].message.tool_calls) > 0\n",
    "    ):\n",
    "        tool_calls = response.choices[0].message.tool_calls\n",
    "    else:\n",
    "        tool_calls = []\n",
    "\n",
    "    if response.choices[0].message.content is None:\n",
    "        response.choices[0].message.content = \"\"\n",
    "    ret = {\n",
    "        \"question\": input,\n",
    "        \"tools\": tool_calls,\n",
    "        \"response\": response.choices[0].message.content,\n",
    "    }\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOM5C6-eC3i9"
   },
   "source": [
    "Let's test it and see if it returns the right function! Based on whether we set tool_choice to \"auto\" or \"required\", the router will have different behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_prompt(\"Hi, I'd like to apply to apply a discount code to my order.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sq4rcseCGKRc"
   },
   "source": [
    "Now we have a basic agent, let's generate a dataset of questions and run the prompt against this dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0Qvn8tAs9vL"
   },
   "source": [
    "# Create synthetic dataset of questions\n",
    "\n",
    "Using the template below, we're going to generate a dataframe of 25 questions we can use to test our customer support agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEN_TEMPLATE = \"\"\"\n",
    "You are an assistant that generates complex customer service questions.\n",
    "The questions should often involve:\n",
    "\n",
    "Multiple Categories: Questions that could logically fall into more than one category (e.g., combining product details with a discount code).\n",
    "Vague Details: Questions with limited or vague information that require clarification to categorize correctly.\n",
    "Mixed Intentions: Queries where the customerâ€™s goal or need is unclear or seems to conflict within the question itself.\n",
    "Indirect Language: Use of indirect or polite phrasing that obscures the direct need or request (e.g., using \"I was wondering if...\" or \"Perhaps you could help me with...\").\n",
    "For specific categories:\n",
    "\n",
    "Track Package: Include vague timing references (e.g., \"recently\" or \"a while ago\") instead of specific dates.\n",
    "Product Comparison and Product Search: Include generic descriptors without specific product names or IDs (e.g., \"high-end smartphones\" or \"energy-efficient appliances\").\n",
    "Apply Discount Code: Include questions about discounts that might apply to hypothetical or past situations, or without mentioning if they have made a purchase.\n",
    "Product Details: Ask for comparisons or details that involve multiple products or categories ambiguously (e.g., \"Tell me about your range of electronics that are good for home office setups\").\n",
    "\n",
    "Examples of More Challenging Questions\n",
    "\"There's an issue with one of the items I think I bought last monthâ€”what should I do?\"\n",
    "\"I need help with something I ordered, or maybe I'm just looking for something new. Can you help?\"\n",
    "\n",
    "Some questions should be straightforward uses of the provided functions\n",
    "\n",
    "Respond with a list, one question per line. Do not include any numbering at the beginning of each line. Do not include any category headings.\n",
    "Generate 25 questions. Be sure there are no duplicate questions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "\n",
    "nest_asyncio.apply()\n",
    "from phoenix.evals import OpenAIModel\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 500)\n",
    "\n",
    "model = OpenAIModel(model=\"gpt-4o\", max_tokens=1300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = model(GEN_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_response = resp.strip().split(\"\\n\")\n",
    "\n",
    "questions_df = pd.DataFrame(split_response, columns=[\"question\"])\n",
    "questions_df[\"tools\"] = \"\"\n",
    "questions_df[\"response\"] = \"\"\n",
    "print(questions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGIbV49kHp4H"
   },
   "source": [
    "Now let's run it and manually inspect the traces! Change the value in `.head(2)` from any number between 1 and 25 to run it on that many data points from the questions we generated earlier.\n",
    "\n",
    "Then manually inspect the outputs in Phoenix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_df = (\n",
    "    questions_df[\"question\"].head(2).apply(run_prompt).apply(pd.Series)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "beUkwcCgLaEa"
   },
   "source": [
    "# Evaluating your agent\n",
    "\n",
    "Now that we have a set of test cases, we can create evaluators to measure performance. This way, we don't have to manually inspect every single trace to see if the LLM is doing the right thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "929BTLH_Ql97"
   },
   "source": [
    "Here, we are defining our evaluation templates to judge whether the router selected a function correctly, whether it selected the right function, and whether it filled the arguments correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROUTER_EVAL_TEMPLATE = \"\"\" You are comparing a response to a question, and verifying whether that response should have made a function call instead of responding directly. Here is the data:\n",
    "    [BEGIN DATA]\n",
    "    ************\n",
    "    [Question]: {question}\n",
    "    ************\n",
    "    [Tool Response]: {tools}\n",
    "    ************\n",
    "    [LLM Response]: {response}\n",
    "    ************\n",
    "    [END DATA]\n",
    "\n",
    "Compare the Question above to the response. You must determine whether the reponse\n",
    "decided to call the correct function.\n",
    "Your response must be single word, either \"correct\" or \"incorrect\",\n",
    "and should not contain any text or characters aside from that word.\n",
    "\"incorrect\" means that the agent should have made function call instead of responding directly and did not, or the function call chosen was the incorrect one.\n",
    "\"correct\" means the selected function would correctly and fully answer the user's question.\n",
    "\n",
    "Here is more information on each function:\n",
    "product_comparison: Compare features of two products. Should include either the product id or name. If the name or id is present in the question and not present in the generated function, the response is incorrect.\n",
    "product_search: Search for products based on criteria.\n",
    "track_package: Track the status of a package based on the tracking number.\n",
    "customer_support: Get contact information for customer support regarding an issue. The response should always include an email or phone number.\n",
    "apply_discount_code: Applies a discount code to an order.\n",
    "product_details: Get detailed features on one product.\n",
    "\"\"\"\n",
    "\n",
    "FUNCTION_SELECTION_EVAL_TEMPLATE = \"\"\" You are comparing a function call response to a question and trying to determine if the generated call is correct. Here is the data:\n",
    "    [BEGIN DATA]\n",
    "    ************\n",
    "    [Question]: {question}\n",
    "    ************\n",
    "    [Tool Response]: {tools}\n",
    "    ************\n",
    "    [LLM Response]: {response}\n",
    "    ************\n",
    "    [END DATA]\n",
    "\n",
    "Compare the Question above to the function call. You must determine whether the function call\n",
    "will return the answer to the Question. Please focus on whether the very specific\n",
    "question can be answered by the function call.\n",
    "Your response must be single word, either \"correct\" or \"incorrect\",\n",
    "and should not contain any text or characters aside from that word.\n",
    "\"incorrect\" means that the function call will not provide an answer to the Question.\n",
    "\"correct\" means the function call will definitely provide an answer to the Question.\n",
    "\n",
    "Here is more information on each function:\n",
    "product_comparison: Compare features of two products. Should include either the product id or name. If the name or id is present in the question and not present in the generated function, the response is incorrect.\n",
    "product_search: Search for products based on criteria.\n",
    "track_package: Track the status of a package based on the tracking number.\n",
    "customer_support: Get contact information for customer support regarding an issue. The response should always include an email or phone number.\n",
    "apply_discount_code: Applies a discount code to an order.\n",
    "product_details: Get detailed features on one product.\n",
    "\"\"\"\n",
    "\n",
    "PARAMETER_EXTRACTION_EVAL_TEMPLATE = \"\"\" You are comparing a function call response to a question and trying to determine if the generated call has extracted the exact right parameters from the question. Here is the data:\n",
    "    [BEGIN DATA]\n",
    "    ************\n",
    "    [Question]: {question}\n",
    "    ************\n",
    "    [Tool Response]: {tools}\n",
    "    ************\n",
    "    [LLM Response]: {response}\n",
    "    ************\n",
    "    [END DATA]\n",
    "\n",
    "Compare the parameters in the generated function against the JSON provided below.\n",
    "The parameters extracted from the question must match the JSON below exactly.\n",
    "Your response must be single word, either \"correct\" or \"incorrect\",\n",
    "and should not contain any text or characters aside from that word.\n",
    "\"incorrect\" means that the parameters in the function do not match the JSON schema below exactly, or the generated function does not correctly answer the user's question.\n",
    "You should also respond with \"incorrect\" if the response makes up information that is not in the JSON schema.\n",
    "\"correct\" means the function call parameters match the JSON below and provides only relevant information.\n",
    "\n",
    "Here is more information on each function:\n",
    "product_comparison: Compare features of two products. Should include either the product id or name. If the name or id is present in the question and not present in the generated function, the response is incorrect.\n",
    "product_search: Search for products based on criteria.\n",
    "track_package: Track the status of a package based on the tracking number.\n",
    "customer_support: Get contact information for customer support regarding an issue. The response should always include an email or phone number.\n",
    "apply_discount_code: Applies a discount code to an order.\n",
    "product_details: Get detailed features on one product.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1aivaxTCRQFl"
   },
   "source": [
    "Let's run evaluations using Phoenix's llm_classify function for our responses dataframe we generated above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.evals import OpenAIModel, llm_classify\n",
    "\n",
    "rails = [\"incorrect\", \"correct\"]\n",
    "\n",
    "router_eval_df = llm_classify(\n",
    "    dataframe=response_df,\n",
    "    template=ROUTER_EVAL_TEMPLATE,\n",
    "    model=OpenAIModel(\"gpt-4o\"),\n",
    "    rails=rails,\n",
    "    provide_explanation=True,\n",
    "    include_prompt=True,\n",
    "    concurrency=4,\n",
    ")\n",
    "\n",
    "function_selection_eval_df = llm_classify(\n",
    "    dataframe=response_df,\n",
    "    template=FUNCTION_SELECTION_EVAL_TEMPLATE,\n",
    "    model=OpenAIModel(\"gpt-4o\"),\n",
    "    rails=rails,\n",
    "    provide_explanation=True,\n",
    "    include_prompt=True,\n",
    "    concurrency=4,\n",
    ")\n",
    "\n",
    "parameter_extraction_eval_df = llm_classify(\n",
    "    dataframe=response_df,\n",
    "    template=PARAMETER_EXTRACTION_EVAL_TEMPLATE,\n",
    "    model=OpenAIModel(\"gpt-4o\"),\n",
    "    rails=rails,\n",
    "    provide_explanation=True,\n",
    "    include_prompt=True,\n",
    "    concurrency=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vDV1KBdYQ_vh"
   },
   "source": [
    "Let's look at and inspect the results of our evaluatiion!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_selection_eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_extraction_eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHYgS5cpRE3b"
   },
   "source": [
    "# Create an experiment\n",
    "\n",
    "With our dataset of questions we generated above, we can use our experiments feature to track changes across models, prompts, parameters for our agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SgTEu7U4Rd5i"
   },
   "source": [
    "Let's create this dataset and upload it into the platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arize.experimental.datasets import ArizeDatasetsClient\n",
    "import os\n",
    "from uuid import uuid1\n",
    "from arize.experimental.datasets.experiments.evaluators.base import (\n",
    "    EvaluationResult,\n",
    "    Evaluator,\n",
    ")\n",
    "from arize.experimental.datasets.utils.constants import GENERATIVE\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Your developer key\n",
    "developer_key = \"eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpYXQiOjE3MTM0NjEwMjksInVzZXJJZCI6MTAwMDgsInV1aWQiOiI4NmIwOWUxMC0wMTk2LTRhMDUtODhiZS1iZGZjNGI5ZGRmNWQiLCJpc3MiOiJodHRwczovL2FwcC5hcml6ZS5jb20ifQ.ZX6NjfZ37pW1nwOH0IlhAWf1NyJuouTuMifs6TtKAdA\"\n",
    "api_key = \"416ad605925bf226fd9\"\n",
    "space_id = \"U3BhY2U6NjM3MjoyMXJG\"\n",
    "\n",
    "# Set up the arize client\n",
    "arize_client = ArizeDatasetsClient(developer_key=developer_key, api_key=api_key)\n",
    "\n",
    "\n",
    "dataset_id = arize_client.create_dataset(\n",
    "    space_id=space_id,\n",
    "    dataset_name=\"agents-cookbook-\" + str(uuid1()),\n",
    "    dataset_type=GENERATIVE,\n",
    "    data=questions_df.head(2),\n",
    ")\n",
    "dataset = arize_client.get_dataset(space_id=space_id, dataset_id=dataset_id)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from phoenix.evals import (\n",
    "    OpenAIModel,\n",
    "    llm_classify,\n",
    ")\n",
    "\n",
    "\n",
    "class RouterEvaluator(Evaluator):\n",
    "    def evaluate(self, output, dataset_row, **kwargs) -> EvaluationResult:\n",
    "        question = output.get(\"question\")\n",
    "        tools = output.get(\"tools\")\n",
    "        response = output.get(\"response\")\n",
    "\n",
    "        df_in = pd.DataFrame(\n",
    "            {\"question\": question, \"tools\": str(tools), \"response\": response},\n",
    "            index=[0],\n",
    "        )\n",
    "\n",
    "        rails = [\"correct\", \"incorrect\"]\n",
    "        expect_df = llm_classify(\n",
    "            dataframe=df_in,\n",
    "            template=ROUTER_EVAL_TEMPLATE,\n",
    "            model=OpenAIModel(model=\"gpt-4o\"),\n",
    "            rails=rails,\n",
    "            provide_explanation=True,\n",
    "            run_sync=True,\n",
    "        )\n",
    "\n",
    "        label = expect_df[\"label\"][0]\n",
    "        score = (\n",
    "            1 if rails and label == rails[0] else 0\n",
    "        )  # Choose the 0 item in rails as the correct \"1\" label\n",
    "        explanation = expect_df[\"explanation\"][0]\n",
    "        return EvaluationResult(\n",
    "            score=score, label=label, explanation=explanation\n",
    "        )\n",
    "\n",
    "\n",
    "class FunctionSelectionEvaluator(Evaluator):\n",
    "    def evaluate(self, output, dataset_row, **kwargs) -> EvaluationResult:\n",
    "        question = output.get(\"question\")\n",
    "        tools = output.get(\"tools\")\n",
    "        response = output.get(\"response\")\n",
    "\n",
    "        df_in = pd.DataFrame(\n",
    "            {\"question\": question, \"tools\": str(tools), \"response\": response},\n",
    "            index=[0],\n",
    "        )\n",
    "        rails = [\"correct\", \"incorrect\"]\n",
    "        expect_df = llm_classify(\n",
    "            dataframe=df_in,\n",
    "            template=FUNCTION_SELECTION_EVAL_TEMPLATE,\n",
    "            model=OpenAIModel(model=\"gpt-4o\"),\n",
    "            rails=rails,\n",
    "            provide_explanation=True,\n",
    "            run_sync=True,\n",
    "        )\n",
    "\n",
    "        label = expect_df[\"label\"][0]\n",
    "        score = (\n",
    "            1 if rails and label == rails[0] else 0\n",
    "        )  # Choose the 0 item in rails as the correct \"1\" label\n",
    "        explanation = expect_df[\"explanation\"][0]\n",
    "        return EvaluationResult(\n",
    "            score=score, label=label, explanation=explanation\n",
    "        )\n",
    "\n",
    "\n",
    "class ParameterExtractionEvaluator(Evaluator):\n",
    "    def evaluate(self, output, dataset_row, **kwargs) -> EvaluationResult:\n",
    "        question = output.get(\"question\")\n",
    "        tools = output.get(\"tools\")\n",
    "        response = output.get(\"response\")\n",
    "\n",
    "        df_in = pd.DataFrame(\n",
    "            {\"question\": question, \"tools\": str(tools), \"response\": response},\n",
    "            index=[0],\n",
    "        )\n",
    "        rails = [\"correct\", \"incorrect\"]\n",
    "        expect_df = llm_classify(\n",
    "            dataframe=df_in,\n",
    "            template=PARAMETER_EXTRACTION_EVAL_TEMPLATE,\n",
    "            model=OpenAIModel(model=\"gpt-4o\"),\n",
    "            rails=rails,\n",
    "            provide_explanation=True,\n",
    "            run_sync=True,\n",
    "        )\n",
    "\n",
    "        label = expect_df[\"label\"][0]\n",
    "        score = (\n",
    "            1 if rails and label == rails[0] else 0\n",
    "        )  # Choose the 0 item in rails as the correct \"1\" label\n",
    "        explanation = expect_df[\"explanation\"][0]\n",
    "        return EvaluationResult(\n",
    "            score=score, label=label, explanation=explanation\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_gen_task(example):\n",
    "    return run_prompt(example.dataset_row.get(\"question\"))\n",
    "\n",
    "\n",
    "## Run Experiment\n",
    "arize_client.run_experiment(\n",
    "    space_id=space_id,\n",
    "    dataset_id=dataset_id,\n",
    "    task=prompt_gen_task,\n",
    "    evaluators=[\n",
    "        RouterEvaluator(),\n",
    "        FunctionSelectionEvaluator(),\n",
    "        ParameterExtractionEvaluator(),\n",
    "    ],\n",
    "    experiment_name=\"agents-cookbook-exp-\" + str(uuid1())[:5],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
