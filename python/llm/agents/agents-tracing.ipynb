{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qk7N5Iz8_2kv"
   },
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "    <img alt=\"arize logo\" src=\"https://storage.googleapis.com/arize-assets/arize-logo-white.jpg\" width=\"300\"/>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/arize/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/client_python\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://join.slack.com/t/arize-ai/shared_invite/zt-1px8dcmlf-fmThhDFD_V_48oU7ALan4Q\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "\n",
    "<center><h1>Agent Tracing</h1></center>\n",
    "\n",
    "This notebook is a quickstart for tracing an agent which handles product search using a router template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUknhuHKyc-E"
   },
   "source": [
    "# Generate dataframe of user queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq \"arize-phoenix[evals]\" arize-otel openai openinference-instrumentation-openai opentelemetry-sdk opentelemetry-exporter-otlp 'httpx<0.28'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your OpenAI Key\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import open-telemetry dependencies\n",
    "from arize.otel import register\n",
    "\n",
    "# Setup OTEL via our convenience function\n",
    "tracer_provider = register(\n",
    "    space_id=getpass(\"ðŸ”‘ Enter your Arize Space ID: \"),\n",
    "    api_key=getpass(\"ðŸ”‘ Enter your Arize API key: \"),\n",
    "    project_name=\"agents-tracing-example\",  # name this to whatever you would like\n",
    ")\n",
    "# Import the automatic instrumentor from OpenInference\n",
    "from openinference.instrumentation.openai import OpenAIInstrumentor\n",
    "\n",
    "# Finish automatic instrumentation\n",
    "OpenAIInstrumentor().instrument(tracer_provider=tracer_provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "from phoenix.evals import OpenAIModel\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CL5LTGwFFXLF"
   },
   "source": [
    "# Generate Fake Questions for Agent\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEN_TEMPLATE = \"\"\"\n",
    "You are an assistant that generates complex customer service questions.\n",
    "The questions should often involve:\n",
    "\n",
    "Multiple Categories: Questions that could logically fall into more than one category (e.g., combining product details with a discount code).\n",
    "Vague Details: Questions with limited or vague information that require clarification to categorize correctly.\n",
    "Mixed Intentions: Queries where the customerâ€™s goal or need is unclear or seems to conflict within the question itself.\n",
    "Indirect Language: Use of indirect or polite phrasing that obscures the direct need or request (e.g., using \"I was wondering if...\" or \"Perhaps you could help me with...\").\n",
    "For specific categories:\n",
    "\n",
    "Track Package: Include vague timing references (e.g., \"recently\" or \"a while ago\") instead of specific dates.\n",
    "Product Comparison and Product Search: Include generic descriptors without specific product names or IDs (e.g., \"high-end smartphones\" or \"energy-efficient appliances\").\n",
    "Apply Discount Code: Include questions about discounts that might apply to hypothetical or past situations, or without mentioning if they have made a purchase.\n",
    "Product Details: Ask for comparisons or details that involve multiple products or categories ambiguously (e.g., \"Tell me about your range of electronics that are good for home office setups\").\n",
    "\n",
    "Examples of More Challenging Questions\n",
    "\"There's an issue with one of the items I think I bought last monthâ€”what should I do?\"\n",
    "\"I need help with something I ordered, or maybe I'm just looking for something new. Can you help?\"\n",
    "\n",
    "Some questions should be straightforward uses of the provided functions\n",
    "\n",
    "Respond with a list, one question per line. Do not include any numbering at the beginning of each line. Do not include any category headings.\n",
    "Generate 25 questions. Be sure there are no duplicate questions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OpenAIModel(model=\"gpt-4o\", max_tokens=1300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = model(GEN_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_response = resp.strip().split(\"\\n\")\n",
    "\n",
    "questions_df = pd.DataFrame(split_response, columns=[\"questions\"])\n",
    "questions_df[\"generated_function\"] = \"\"\n",
    "questions_df[\"response\"] = \"\"\n",
    "print(questions_df[\"questions\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ob9sxL00_KkG"
   },
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hs-y9xRpF1pL"
   },
   "source": [
    "#  Agent for Product Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hXJI_gA3F-qx"
   },
   "source": [
    "This section traces an example of an agent that has both a router function and a set of tools that also leverage an LLM in the tool function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"name\": \"product_comparison\",\n",
    "        \"description\": \"Compare features of two products.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"product_a_id\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The unique identifier of Product A.\",\n",
    "                },\n",
    "                \"product_b_id\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The unique identifier of Product B.\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"product_a_id\", \"product_b_id\"],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"name\": \"product_search\",\n",
    "        \"description\": \"Search for products based on criteria.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The search query string.\",\n",
    "                },\n",
    "                \"category\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The category to filter the search.\",\n",
    "                },\n",
    "                \"min_price\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"The minimum price of the products to search.\",\n",
    "                    \"default\": 0,\n",
    "                },\n",
    "                \"max_price\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"The maximum price of the products to search.\",\n",
    "                },\n",
    "                \"page\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"The page number for pagination.\",\n",
    "                    \"default\": 1,\n",
    "                },\n",
    "                \"page_size\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"The number of results per page.\",\n",
    "                    \"default\": 20,\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"query\"],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"name\": \"customer_support\",\n",
    "        \"description\": \"Get contact information for customer support regarding an issue.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"issue_type\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The type of issue (e.g., billing, technical support).\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"issue_type\"],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"name\": \"track_package\",\n",
    "        \"description\": \"Track the status of a package based on the tracking number.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"tracking_number\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"The tracking number of the package.\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"tracking_number\"],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"name\": \"product_details\",\n",
    "        \"description\": \"Returns details for a given product id\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"product_id\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The id of a product to look up.\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"product_id\"],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"name\": \"apply_discount_code\",\n",
    "        \"description\": \"Applies the discount code to a given order.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"order_id\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"The id of the order to apply the discount code to.\",\n",
    "                },\n",
    "                \"discount_code\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The discount code to apply\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"order_id\", \"discount_code\"],\n",
    "        },\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7wmOJpUd1OX"
   },
   "source": [
    "# Routing Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROUTER_TEMPLATE = \"\"\" You are comparing a response to a question, and verifying whether that response should have made a function call instead of responding directly. Here is the data:\n",
    "    [BEGIN DATA]\n",
    "    ************\n",
    "    [Question]: {question}\n",
    "    ************\n",
    "    [Response]: {generated_function}\n",
    "    [END DATA]\n",
    "\n",
    "Compare the Question above to the response. You must determine whether the reponse\n",
    "decided to call the correct function.\n",
    "Your response must be single word, either \"correct\" or \"incorrect\",\n",
    "and should not contain any text or characters aside from that word.\n",
    "\"incorrect\" means that the agent should have made function call instead of responding directly and did not, or the function call chosen was the incorrect one.\n",
    "\"correct\" means the selected function would correctly and fully answer the user's question.\n",
    "\n",
    "Here is more information on each function:\n",
    "{function_info}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opentelemetry import trace\n",
    "\n",
    "tracer = trace.get_tracer(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openinference.semconv.trace import (\n",
    "    SpanAttributes,\n",
    "    ToolCallAttributes,\n",
    "    OpenInferenceSpanKindValues,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import openai\n",
    "\n",
    "client = openai.Client()\n",
    "\n",
    "TASK_MODEL = \"gpt-3.5-turbo\"\n",
    "# TASK_MODEL = \"gpt-4o\"\n",
    "\n",
    "\n",
    "def agent_router(input):\n",
    "    # Obtain a tracer instance\n",
    "    tracer = trace.get_tracer(__name__)\n",
    "    with tracer.start_as_current_span(\n",
    "        \"AgentOperation\",\n",
    "        attributes={\n",
    "            SpanAttributes.OPENINFERENCE_SPAN_KIND: OpenInferenceSpanKindValues.AGENT.value\n",
    "        },\n",
    "    ) as span:\n",
    "        response = client.chat.completions.create(\n",
    "            model=TASK_MODEL,\n",
    "            temperature=0,\n",
    "            functions=functions,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \" \",\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": input[\"questions\"],\n",
    "                },\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        if hasattr(response.choices[0].message.function_call, \"name\"):\n",
    "            function_call_name = response.choices[0].message.function_call.name\n",
    "            arguments = response.choices[0].message.function_call.arguments\n",
    "            # Call handle_function_call if a function call is detected\n",
    "            generated_response = handle_function_call(\n",
    "                function_call_name, arguments\n",
    "            )\n",
    "        else:\n",
    "            function_call_name = \"no function called\"\n",
    "            arguments = \"no function called\"\n",
    "            generated_response = response.choices[0].message.content\n",
    "        span.set_attribute(SpanAttributes.INPUT_VALUE, input[\"questions\"])\n",
    "        span.set_attribute(SpanAttributes.OUTPUT_VALUE, generated_response)\n",
    "        ret = {\n",
    "            \"question\": input,\n",
    "            \"function_call_name\": function_call_name,\n",
    "            \"arguments\": arguments,\n",
    "            \"output\": generated_response,\n",
    "        }\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_function_call(function_call_name, arguments):\n",
    "    tracer = trace.get_tracer(__name__)\n",
    "\n",
    "    # Start a new span for the tool function handling\n",
    "    with tracer.start_as_current_span(\n",
    "        \"HandleFunctionCall\",\n",
    "        attributes={\n",
    "            SpanAttributes.OPENINFERENCE_SPAN_KIND: OpenInferenceSpanKindValues.TOOL.value,\n",
    "            ToolCallAttributes.TOOL_CALL_FUNCTION_NAME: function_call_name,\n",
    "            ToolCallAttributes.TOOL_CALL_FUNCTION_ARGUMENTS_JSON: str(\n",
    "                arguments\n",
    "            ),\n",
    "            SpanAttributes.INPUT_VALUE: function_call_name,\n",
    "        },\n",
    "    ):\n",
    "        # Here, we simulate the LLM call to generate a response based on function_call_name and arguments\n",
    "        prompt = f\"Function '{function_call_name}' was called with the following arguments: {arguments}. Generate a simulated looking response for this function call. Don't mention it's simulated in your response.\"\n",
    "\n",
    "        # Simulate calling the LLM with the constructed prompt\n",
    "        response = client.chat.completions.create(\n",
    "            model=TASK_MODEL,\n",
    "            temperature=0.7,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a helpful assistant.\",\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                },\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # Extract the generated response from the LLM\n",
    "        generated_response = response.choices[0].message.content\n",
    "\n",
    "        return generated_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def process_questions(df):\n",
    "    results = []\n",
    "    for _, row in df.iterrows():\n",
    "        # Apply the run_prompt function to each question in the dataframe\n",
    "        result = agent_router({\"questions\": row[\"questions\"]})\n",
    "        results.append(result)\n",
    "\n",
    "    # Convert the results into a DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_questions(questions_df)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
