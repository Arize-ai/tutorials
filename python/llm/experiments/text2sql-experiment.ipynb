{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-umpmOVRtw89"
   },
   "source": [
    "<center> <img src=\"https://storage.googleapis.com/arize-assets/arize-logo-white.jpg\" width=\"300\"/> </center>\n",
    "\n",
    "</center>\n",
    "<h1 align=\"center\">Experiments: Text2SQL</h1>\n",
    "\n",
    "---\n",
    "Let's work through a Text2SQL use case where we are starting from scratch without a nice and clean dataset of questions, SQL queries, or expected responses.\n",
    "\n",
    "‚ÑπÔ∏èThis notebook requires:\n",
    "- An OpenAI API key\n",
    "- An Arize Space ID & Developer Key (explained below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q 'arize[Datasets]' openai datasets pyarrow pydantic nest_asyncio 'arize-phoenix[evals]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGhQ5pS8y1SK"
   },
   "source": [
    "# Setup Config\n",
    "\n",
    "Copy the Arize developer API Key and Space ID from the Datasets page (shown below) to the variables in the cell below.\n",
    "\n",
    "<center><img src=\"https://storage.googleapis.com/arize-assets/fixtures/dataset_api_key.png\" width=\"700\"></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid1\n",
    "\n",
    "dataset_name = \"docs-qa-new-\" + str(uuid1())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from arize.experimental.datasets import ArizeDatasetsClient\n",
    "from arize.experimental.datasets.utils.constants import GENERATIVE\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import asyncio\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxp0xiW8vm9O"
   },
   "source": [
    "Let's make sure we can run async code in the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3E4mzKxrv08-"
   },
   "source": [
    "Lastly, let's make sure we have our openai API key set up.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "space_id = \"U3BhY2U6NjM3MjoyMXJG\"\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"üîë Enter your OpenAI API key: \")\n",
    "\n",
    "if not os.environ.get(\"ARIZE_API_KEY\"):\n",
    "    os.environ[\"ARIZE_API_KEY\"] = getpass(\"üîë Enter your ARIZE_API_KEY: \")\n",
    "\n",
    "if not os.environ.get(\"ARIZE_DEVELOPER_KEY\"):\n",
    "    os.environ[\"ARIZE_DEVELOPER_KEY\"] = getpass(\"üîë Enter your developer key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6wcc2MdwNLE"
   },
   "source": [
    "# Download Data\n",
    "\n",
    "We are going to use the NBA dataset that information from 2014 - 2018. We will use DuckDB as our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"suzyanil/nba-data\")[\"train\"]\n",
    "\n",
    "conn = duckdb.connect(database=\":memory:\", read_only=False)\n",
    "conn.register(\"nba\", data.to_pandas())\n",
    "\n",
    "conn.query(\"SELECT * FROM nba LIMIT 5\").to_df().to_dict(orient=\"records\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvxcjSwqwjHT"
   },
   "source": [
    "## Implement Text2SQL\n",
    "\n",
    "Let's start by implementing a simple text2sql logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import openai\n",
    "\n",
    "client = openai.AsyncClient()\n",
    "\n",
    "columns = conn.query(\"DESCRIBE nba\").to_df().to_dict(orient=\"records\")\n",
    "\n",
    "# We will use GPT4o to start\n",
    "TASK_MODEL = \"gpt-4o\"\n",
    "CONFIG = {\"model\": TASK_MODEL}\n",
    "\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are a SQL expert, and you are given a single table named nba with the following columns:\\n\"\n",
    "    f'{\",\".join(column[\"column_name\"] + \": \" + column[\"column_type\"] for column in columns)}\\n'\n",
    "    \"Write a SQL query corresponding to the user's request. Return just the query text, \"\n",
    "    \"with no formatting (backticks, markdown, etc.).\"\n",
    ")\n",
    "\n",
    "\n",
    "async def generate_query(input):\n",
    "    response = await client.chat.completions.create(\n",
    "        model=TASK_MODEL,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": input,\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = await generate_query(\"Who won the most games?\")\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3dU37wqawyag"
   },
   "source": [
    "Awesome, looks like the LLM is producing SQL! let's try running the query and see if we get the expected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_query(query):\n",
    "    return conn.query(query).fetchdf().to_dict(orient=\"records\")\n",
    "\n",
    "\n",
    "execute_query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1abv2kxw9mX"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Evaluation consists of three parts ‚Äî data, task, and scores. We'll start with data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"Which team won the most games?\",\n",
    "    \"Which team won the most games in 2015?\",\n",
    "    \"Who led the league in 3 point shots?\",\n",
    "    \"Which team had the biggest difference in records across two consecutive years?\",\n",
    "    \"What is the average number of free throws per year?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "em4kMEpPxC21"
   },
   "source": [
    "Let's store the data above as a versioned dataset in Arize.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arize_client = ArizeDatasetsClient(\n",
    "    developer_key=os.environ.get(\"ARIZE_DEVELOPER_KEY\"),\n",
    "    api_key=os.environ.get(\"ARIZE_API_KEY\"),\n",
    ")\n",
    "# Create a dataset from a DataFrame add your own data here\n",
    "test_df = pd.DataFrame([{\"question\": question} for question in questions])\n",
    "dataset_id = arize_client.create_dataset(\n",
    "    space_id=space_id,\n",
    "    dataset_name=dataset_name,\n",
    "    dataset_type=GENERATIVE,\n",
    "    data=test_df,\n",
    ")\n",
    "dataset_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KrYDvmS0z439"
   },
   "source": [
    "Let's now pull down the dataset from Arize in this environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = arize_client.get_dataset(space_id=space_id, dataset_id=dataset_id)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_gxVtgG2Hgx"
   },
   "source": [
    "Next, we'll define the task. The task is to generate SQL queries from natural language questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def text2sql(question):\n",
    "    query = await generate_query(question)\n",
    "    results = None\n",
    "    error = None\n",
    "    try:\n",
    "        results = execute_query(query)\n",
    "    except duckdb.Error as e:\n",
    "        error = str(e)\n",
    "\n",
    "    r = {\n",
    "        \"query\": query,\n",
    "        \"results\": results,\n",
    "        \"error\": error,\n",
    "    }\n",
    "    return json.dumps(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1pQcIsjI2_SH"
   },
   "source": [
    "Finally, we'll define the scores. We'll use the following simple scoring functions to see if the generated SQL queries are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if there are no sql execution errors\n",
    "def no_error(output):\n",
    "    output = json.loads(output)\n",
    "    return 1.0 if output.get(\"error\") is None else 0.0\n",
    "\n",
    "\n",
    "# Test if the query has results\n",
    "def has_results(output):\n",
    "    output = json.loads(output)\n",
    "    results = output.get(\"results\")\n",
    "    has_results = results is not None and len(results) > 0\n",
    "    return 1.0 if has_results else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZQMceUq3DtP"
   },
   "source": [
    "# Run Experiment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hV75E_sW3JC0"
   },
   "source": [
    "Run experiment and log results to Arize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the task to run text2sql on the input question\n",
    "def task(dataset_row):\n",
    "    input = dataset_row\n",
    "    return asyncio.run(text2sql(input[\"question\"]))\n",
    "\n",
    "\n",
    "experiment = arize_client.run_experiment(\n",
    "    space_id=space_id,\n",
    "    dataset_id=dataset_id,\n",
    "    task=task,\n",
    "    evaluators=[no_error, has_results],\n",
    "    experiment_name=\"text2sql_test-2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHPuB8jZXAVD"
   },
   "source": [
    "Ok! It looks like 3/5 of our queries are valid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "959GeSEUXOd0"
   },
   "source": [
    "#Interpreting the results\n",
    "\n",
    "Now that we ran the initial evaluation, it looks like two of the results are valid, two produce SQL errors, and one is incorrect.\n",
    "\n",
    "- The incorrect query didn't seem to get the date format correct. That would probably be improved by showing a sample of the data to the model (e.g. few shot example).\n",
    "\n",
    "- There are is a binder error, which may also have to do with not understanding the data format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZiODgG06Xa0e"
   },
   "source": [
    "Let's try to improve the prompt with few-shot examples and see if we can get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = (\n",
    "    conn.query(\"SELECT * FROM nba LIMIT 1\").to_df().to_dict(orient=\"records\")[0]\n",
    ")\n",
    "sample_rows = \"\\n\".join(\n",
    "    f\"{column['column_name']} | {column['column_type']} | {samples[column['column_name']]}\"\n",
    "    for column in columns\n",
    ")\n",
    "system_prompt = (\n",
    "    \"You are a SQL expert, and you are given a single table named nba with the following columns:\\n\\n\"\n",
    "    \"Column | Type | Example\\n\"\n",
    "    \"-------|------|--------\\n\"\n",
    "    f\"{sample_rows}\\n\"\n",
    "    \"\\n\"\n",
    "    \"Write a DuckDB SQL query corresponding to the user's request. \"\n",
    "    \"Return just the query text, with no formatting (backticks, markdown, etc.).\"\n",
    ")\n",
    "\n",
    "\n",
    "async def generate_query(input):\n",
    "    response = await client.chat.completions.create(\n",
    "        model=TASK_MODEL,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": input,\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "print(await generate_query(\"Which team won the most games in 2015?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0SHT7mwjYRko"
   },
   "source": [
    "Looking much better! Finally, let's add a scoring function that compares the results, if they exist, with the expected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.evals.models import OpenAIModel\n",
    "from phoenix.evals.classify import llm_classify\n",
    "from arize.experimental.datasets.experiments.types import EvaluationResult\n",
    "\n",
    "\n",
    "IS_SQL_EVAL_TEMPLATE = \"\"\"You are a SQL expert, is the following a valid SQL query that executes without errors? Return the single workd \"valid\" if is valid, and \"invalid\" if it is not.\n",
    "\n",
    "[BEGIN SQL QUERY]\n",
    "{query}\n",
    "[END SQL QUERY]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def check_is_sql(output):\n",
    "    output = json.loads(output)\n",
    "    query = output.get(\"query\")\n",
    "    df_in = pd.DataFrame({\"query\": query}, index=[0]) if query else None\n",
    "    eval_df = llm_classify(\n",
    "        dataframe=df_in,\n",
    "        template=IS_SQL_EVAL_TEMPLATE,\n",
    "        model=OpenAIModel(model=\"gpt-4o\"),\n",
    "        rails=[\"valid\", \"invalid\"],\n",
    "        provide_explanation=True,\n",
    "    )\n",
    "    # return score, label, explanation\n",
    "    return EvaluationResult(\n",
    "        score=1,\n",
    "        label=eval_df[\"label\"][0],\n",
    "        explanation=eval_df[\"explanation\"][0],\n",
    "    )\n",
    "\n",
    "\n",
    "experiment = arize_client.run_experiment(\n",
    "    space_id=space_id,\n",
    "    dataset_id=dataset_id,\n",
    "    task=task,\n",
    "    evaluators=[no_error, has_results, check_is_sql],\n",
    "    experiment_name=\"text2sql_test_new_prompt_and_eval-6\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dTnH30FiY0nM"
   },
   "source": [
    "Amazing. It looks like we removed one of the errors, and got a result for the incorrect query. Let's try out using LLM as a judge to see how well it can assess the results."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
