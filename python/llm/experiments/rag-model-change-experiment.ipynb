{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwQu4sg1HG4g"
   },
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "    <img alt=\"arize logo\" src=\"https://storage.googleapis.com/arize-assets/arize-logo-white.jpg\" width=\"300\"/>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/arize/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/client_python\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://arize-ai.slack.com/join/shared_invite/zt-11t1vbu4x-xkBIHmOREQnYnYDH1GDfCg\">Slack Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "\n",
    "# <center>Arize Experiments - Run Evals With Different Models</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_Y5Ma7nCVxt"
   },
   "source": [
    "This guide demonstrates how to use Arize for logging and analyzing different model experiments with your LLM. We're going to build RAG application, along with an experimentation pipeline that evaluates for hallucinations using different models. The original queries, documents, and outputs will be logged to an Arize dataset. Arize makes it easy to track and compare results from experiments, allowing you to identify which variations affect performance. You can read more about experiment tracking with Arize [here](https://docs.arize.com/arize/llm-experiments-and-testing/quickstart).\n",
    "In this tutorial, you will:\n",
    "\n",
    "*   Create a RAG application using LlamaIndex.\n",
    "\n",
    "*   Instrument the application to log the queries, documents, and outputs to Arize.\n",
    "\n",
    "*   Export traces from Arize, and subset into a dataset.\n",
    "\n",
    "*   Implement a script that runs the hallucination eval with different models, generates outputs using an LLM, and logs all output to the given experiment.\n",
    "\n",
    "*   Analyze the logged data in Arize to compare results across different models.\n",
    "\n",
    "By leveraging Arize for experiment tracking, you'll be able to systematically test different models at scale and use the logged data to inform your development process. Let's get started!\n",
    "\n",
    "ℹ️ This notebook requires:\n",
    "- An OpenAI API key\n",
    "- An Arize Space Key, API Key, and Developer Key (explained below)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QrDX_J7SJMOw"
   },
   "source": [
    "# Step 1: Setup Config\n",
    "\n",
    "* Navigate to Space Settings, and copy your Space ID and API Key.\n",
    "\n",
    "* Next, Make sure a Developer Key is active prior to running code below. To retrieve your GraphQL API Key, navigate to the [GraphQL Explorer](https://docs.arize.com/arize/api-reference/graphql-api/getting-started-with-programmatic-access#accessing-the-api-explorer), and select \"Get your developer key\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ko6NuWKlI39R"
   },
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q \"arize>7.29.0\" \"arize-phoenix-evals>=0.17.5\" openai==1.57.1 arize-otel==0.7.1 gcsfs==2024.10.0 llama-index-llms-openai==0.3.3 llama-index-core==0.12.5 openinference-instrumentation-llama-index==3.0.4 opentelemetry-instrumentation-httpx==0.49b2 opentelemetry-sdk==1.28.2 opentelemetry-exporter-otlp==1.28.2 llama-index-embeddings-openai==0.3.1 llama-index==0.12.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from uuid import uuid1\n",
    "\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "from arize.experimental.datasets import ArizeDatasetsClient\n",
    "from arize.experimental.datasets.experiments.evaluators.base import (\n",
    "    EvaluationResult,\n",
    "    Evaluator,\n",
    ")\n",
    "from arize.experimental.datasets.utils.constants import GENERATIVE\n",
    "from phoenix.evals import (\n",
    "    OpenAIModel,\n",
    "    llm_classify,\n",
    ")\n",
    "from typing import Dict, Any\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.environ.get(\"SPACE_ID\"):\n",
    "    os.environ[\"SPACE_ID\"] = getpass(\"🔑 Enter your space id: \")\n",
    "\n",
    "if not os.environ.get(\"ARIZE_API_KEY\"):\n",
    "    os.environ[\"ARIZE_API_KEY\"] = getpass(\"🔑 Enter your ARIZE_API_KEY: \")\n",
    "\n",
    "if not os.environ.get(\"ARIZE_DEVELOPER_KEY\"):\n",
    "    os.environ[\"ARIZE_DEVELOPER_KEY\"] = getpass(\"🔑 Enter your developer key: \")\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"🔑 Enter your OpenAI API key: \")\n",
    "\n",
    "SPACE_ID = os.environ.get(\"SPACE_ID\")\n",
    "API_KEY = os.environ.get(\"ARIZE_API_KEY\")\n",
    "DEVELOPER_KEY = os.environ.get(\"ARIZE_DEVELOPER_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Configure a Tracer\n",
    "\n",
    "We recommend using the `register` helper method below to configure a tracer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import open-telemetry dependencies\n",
    "from arize.otel import register\n",
    "from openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n",
    "\n",
    "project_name = \"model-change-experiment\"\n",
    "\n",
    "# Setup OTEL via our convenience function\n",
    "tracer_provider = register(\n",
    "    space_id=SPACE_ID,\n",
    "    api_key=API_KEY,\n",
    "    project_name=project_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we're using a Tracing Integration, this will take care of automatically creating the spans for your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LlamaIndexInstrumentor().instrument(tracer_provider=tracer_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. Build Your LlamaIndex RAG Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcsfs import GCSFileSystem\n",
    "from llama_index.core import (\n",
    "    Settings,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9edvcUzMJMOy"
   },
   "source": [
    "Set your OpenAI API key if it is not already set as an environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not openai_api_key:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"🔑 Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example uses a `RetrieverQueryEngine` over a pre-built index of the Arize documentation, but you can use whatever LlamaIndex application you like. Download the pre-built index of the Arize docs from cloud storage and instantiate your storage context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_system = GCSFileSystem(project=\"public-assets-275721\")\n",
    "index_path = \"arize-phoenix-assets/datasets/unstructured/llm/llama-index/arize-docs/index/\"\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    fs=file_system,\n",
    "    persist_dir=index_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to instantiate our query engine that will perform retrieval-augmented generation (RAG). Query engine is a generic interface in LlamaIndex that allows you to ask question over your data. A query engine takes in a natural language query, and returns a rich response. It is built on top of Retrievers. You can compose multiple query engines to achieve more advanced capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = OpenAI(model=\"gpt-4o\")\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")\n",
    "index = load_index_from_storage(\n",
    "    storage_context,\n",
    ")\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4. Use Our Instrumented Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "queries = [\n",
    "    \"How can I query for a monitor's status using GraphQL?\",\n",
    "    \"How do I delete a model?\",\n",
    "    \"How much does an enterprise license of Arize cost?\",\n",
    "    \"How do I log a prediction using the python SDK?\",\n",
    "]\n",
    "\n",
    "for query in tqdm(queries):\n",
    "    response = query_engine.query(query)\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Log into Arize and explore your application traces 🚀\n",
    "\n",
    "Log into your Arize account, and look for the model with the same `model_id`. You are likely to see the following page if you are sending a brand new model. Arize is processing your data and your model will be accessible for you to explore your traces in no time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ft0HMYRg-cq7"
   },
   "source": [
    "# Step 6. Export Traces\n",
    "\n",
    "It can be helpful to export the trace data from Arize for a variety of reasons. Common use cases include:\n",
    "\n",
    "* Testing out evaluations with a subset of the data\n",
    "\n",
    "* Create a dataset of few-shot examples programmatically \n",
    "\n",
    "* Augment trace data with metadata programmatically \n",
    "\n",
    "* Fine-tune a smaller model with production traces from Arize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "from arize.exporter import ArizeExportClient\n",
    "from arize.utils.types import Environments\n",
    "\n",
    "export_client = ArizeExportClient(api_key=DEVELOPER_KEY)\n",
    "\n",
    "start_time = datetime.now() - timedelta(days=14)  # 14 days ago\n",
    "end_time = datetime.now()  # Today\n",
    "\n",
    "print(\"#### Exporting your primary dataset into a dataframe.\")\n",
    "\n",
    "primary_df = export_client.export_model_to_df(\n",
    "    space_id=SPACE_ID,\n",
    "    model_id=project_name,\n",
    "    environment=Environments.TRACING,\n",
    "    start_time=start_time,\n",
    "    end_time=end_time,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7. Subset & Format Data For Dataset\n",
    "\n",
    "We'll subset the base spans and document spans, as they're what's necessary for the experiments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_spans = primary_df[\n",
    "    [\"context.trace_id\", \"attributes.input.value\", \"attributes.output.value\"]\n",
    "][primary_df[\"name\"] == \"BaseQueryEngine.query\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_docs = primary_df[[\"context.trace_id\", \"attributes.retrieval.documents\"]][\n",
    "    primary_df[\"name\"] == \"BaseRetriever.retrieve\"\n",
    "].reset_index(drop=True)\n",
    "\n",
    "df_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_documents(val: str):\n",
    "    return \"\".join([doc[\"document.content\"] for doc in val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_docs[\"documents_combined\"] = df_docs[\"attributes.retrieval.documents\"].apply(\n",
    "    format_documents\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CuOuYFSrvUBA"
   },
   "source": [
    "# Step 8. Create Your Dataset\n",
    "\n",
    "You can create many different kinds of datasets. We'll start with the simplest example below, and if you'd like to upload datasets with prompt variables, edit or delete your datasets, [follow this guide](https://docs.arize.com/arize/datasets/how-to-datasets/create-a-dataset-with-code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset = pd.merge(\n",
    "    df_base_spans, df_docs, on=\"context.trace_id\", how=\"inner\"\n",
    ")\n",
    "\n",
    "df_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the Arize Dataset Client to create or update a dataset.\n",
    "datasets_client = ArizeDatasetsClient(\n",
    "    developer_key=DEVELOPER_KEY, api_key=API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = datasets_client.create_dataset(\n",
    "    space_id=SPACE_ID,\n",
    "    dataset_name=f\"arize_experiment_{str(uuid1())[:5]}\",\n",
    "    dataset_type=GENERATIVE,\n",
    "    data=df_dataset,\n",
    ")\n",
    "\n",
    "print(dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WdTsHElvi4l"
   },
   "source": [
    "# Step 9. Setup LLM Attributes As A Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task(dataset_row: Dict[str, Any]) -> dict:\n",
    "    \"\"\"\n",
    "    Executes an LLM task based on an input.\n",
    "    Output must be JSON serialisable.\n",
    "\n",
    "    Args:\n",
    "        dataset_row: A dictionary representing a dataset row.\n",
    "\n",
    "    Returns:\n",
    "        LLM output as a dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    lst_cols = [\n",
    "        \"attributes.input.value\",\n",
    "        \"documents_combined\",\n",
    "        \"attributes.output.value\",\n",
    "    ]\n",
    "\n",
    "    dict_output_data = {\n",
    "        attribute: str(dataset_row[attribute]) for attribute in lst_cols\n",
    "    }\n",
    "\n",
    "    return dict_output_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P7TolvFySMTO"
   },
   "source": [
    "## Create an OpenAIModel Eval\n",
    "\n",
    "We'll create an eval model to check for hallucinations, using `gpt-4o`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_MODEL_NAME = \"gpt-4o\"\n",
    "eval_llm = OpenAIModel(model=LLM_MODEL_NAME, temperature=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUpRWqfWvnkr"
   },
   "source": [
    "# Step 10. Setup Your Evaluator\n",
    "\n",
    "\n",
    "Users have the option to run an experiment by creating an evaluator that inherits from the [Evaluator(ABC)](https://github.com/Arize-ai/client_python/blob/8ce56cf603f7e7887efe306fa81aaaa68b068ccd/arize/experimental/datasets/experiments/evaluators/base.py#L20) base class in the Arize Python SDK. The evaluator takes in a single dataset row as input and returns an [EvaluationResult](https://github.com/Arize-ai/client_python/blob/8ce56cf603f7e7887efe306fa81aaaa68b068ccd/arize/experimental/datasets/experiments/types.py#L103) dataclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.evals import HALLUCINATION_PROMPT_RAILS_MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HALLUCINATION_PROMPT_TEMPLATE = \"\"\"\n",
    "In this task, you will be presented with a query, a reference text and an answer. The answer is\n",
    "generated to the question based on the reference text. The answer may contain false information. You\n",
    "must use the reference text to determine if the answer to the question contains false information,\n",
    "if the answer is a hallucination of facts. Your objective is to determine whether the answer text\n",
    "contains factual information and is not a hallucination. A 'hallucination' refers to\n",
    "an answer that is not based on the reference text or assumes information that is not available in\n",
    "the reference text. Your response should be a single word: either \"factual\" or \"hallucinated\", and\n",
    "it should not include any other text or characters. \"hallucinated\" indicates that the answer\n",
    "provides factually inaccurate information to the query based on the reference text. \"factual\"\n",
    "indicates that the answer to the question is correct relative to the reference text, and does not\n",
    "contain made up information. Please read the query and reference text carefully before determining\n",
    "your response.\n",
    "\n",
    "    # Query: {attributes.input.value}\n",
    "    # Reference text: {documents_combined}\n",
    "    # Answer: {attributes.output.value}\n",
    "    Is the answer above factual or hallucinated based on the query and reference text?\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class HallucinationEval(Evaluator):\n",
    "    \"\"\"\n",
    "    Demonstrates using an LLM to judge correctness.\n",
    "    \"\"\"\n",
    "\n",
    "    def evaluate(\n",
    "        self, *, output: str, dataset_row: Dict[str, Any], **_: Any\n",
    "    ) -> EvaluationResult:\n",
    "        \"\"\"\n",
    "        Evaluate the output with the HALLUCINATION_PROMPT_TEMPLATE template and determine if output is hallucinating.\n",
    "\n",
    "        Args:\n",
    "            output: The output to be evaluated.\n",
    "            **_: Additional keyword arguments.\n",
    "\n",
    "        Returns:\n",
    "            EvaluationResult: The LLM evaluation result containing the explanation, score, and label.\n",
    "        \"\"\"\n",
    "\n",
    "        # df_input = pd.DataFrame(dataset_row, index=[0])\n",
    "        df_input = pd.DataFrame(output, index=[0])\n",
    "\n",
    "        # Map the boolean values to the expected labels\n",
    "        rails = list(HALLUCINATION_PROMPT_RAILS_MAP.values())\n",
    "\n",
    "        # Apply the LLM as a judge template to the input\n",
    "        eval_df = llm_classify(\n",
    "            dataframe=df_input,\n",
    "            template=HALLUCINATION_PROMPT_TEMPLATE,\n",
    "            model=eval_llm,\n",
    "            rails=rails,\n",
    "            provide_explanation=True,\n",
    "        )\n",
    "\n",
    "        # Create the evaluation df\n",
    "        eval_label = eval_df[\"label\"][0]\n",
    "        eval_result = EvaluationResult(\n",
    "            # Provide label, explanation, and score\n",
    "            label=eval_label,\n",
    "            score=1 if eval_label == \"hallucinated\" else 0,\n",
    "            explanation=eval_df[\"explanation\"][0],\n",
    "        )\n",
    "\n",
    "        return eval_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2JrkUFhvqIi"
   },
   "source": [
    "# Step 11A. Run Experiment (gpt-4o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run Experiment\n",
    "datasets_client.run_experiment(\n",
    "    space_id=SPACE_ID,\n",
    "    dataset_id=dataset_id,\n",
    "    task=task,\n",
    "    evaluators=[HallucinationEval()],\n",
    "    experiment_name=f\"Hallucination-Experiment-{LLM_MODEL_NAME}-{str(uuid1())[:5]}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2s3eFwvdSMTP"
   },
   "source": [
    "## Step 11B. Run the experiment (gpt-4o-mini)\n",
    "\n",
    "We'll now run the same experiment, but use `gpt-4o-mini` as the evaluation model. This will provide insight into if a different model yields different eval output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_MODEL_NAME = \"gpt-4o-mini\"\n",
    "eval_llm = OpenAIModel(model=LLM_MODEL_NAME, temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run Experiment\n",
    "datasets_client.run_experiment(\n",
    "    space_id=SPACE_ID,\n",
    "    dataset_id=dataset_id,\n",
    "    task=task,\n",
    "    evaluators=[HallucinationEval()],\n",
    "    experiment_name=f\"Hallucination-Experiment-{LLM_MODEL_NAME}-{str(uuid1())[:5]}\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
