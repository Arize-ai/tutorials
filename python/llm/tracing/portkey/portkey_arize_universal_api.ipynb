{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DwzhaFcy-dTE"
   },
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "    <img alt=\"arize logo\" src=\"https://storage.googleapis.com/arize-assets/arize-logo-white.jpg\" width=\"300\"/>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/arize/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/client_python\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://arize-ai.slack.com/join/shared_invite/zt-11t1vbu4x-xkBIHmOREQnYnYDH1GDfCg\">Slack Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "    <img alt=\"arize logo\" src=\"https://camo.githubusercontent.com/19d13d2afb3500141b9a802689d89745886f5b3f56d758e4725b70df14c8697a/68747470733a2f2f616e616c7974696373696e6469616d61672e636f6d2f77702d636f6e74656e742f75706c6f6164732f323032332f30382f4c6f676f2d6f6e2d77686974652d6261636b67726f756e642e706e67\" width=\"300\"/>\n",
    "    </p>\n",
    "</center>\n",
    "\n",
    "<center><h1> Portkey Universal API + Arize Tracing/Evals </h1></center>\n",
    "\n",
    "This notebook will walk through how you can use Portkey to seamlessly use different LLMs within the same application. It also adds Arize tracing/evals so you can observe and evaluate the different LLM calls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PT_LIq8JJdDt"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Portkey openinference-instrumentation-portkey portkey-ai arize-otel arize-phoenix \"arize[Tracing]>=7.1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env variables\n",
    "import os, getpass\n",
    "\n",
    "os.environ[\"PORTKEY_API_KEY\"] = getpass.getpass(\"Enter your PORTKEY_API_KEY: \")\n",
    "os.environ[\"ARIZE_API_KEY\"] = getpass.getpass(\"Enter your ARIZE_API_KEY: \")\n",
    "os.environ[\"ARIZE_SPACE_ID\"] = getpass.getpass(\"Enter your ARIZE_SPACE_ID: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_VIRTUAL_KEY\"] = getpass.getpass(\"Enter your OPENAI_VIRTUAL_KEY: \")\n",
    "os.environ[\"CLAUDE_VIRTUAL_KEY\"] = getpass.getpass(\"Enter your CLAUDE_VIRTUAL_KEY: \")\n",
    "os.environ[\"GEMINI_VIRTUAL_KEY\"] = getpass.getpass(\"Enter your GEMINI_VIRTUAL_KEY: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vbseOo_HunRe"
   },
   "source": [
    "# Setup Arize Tracing with PortkeyInstrumentor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import open-telemetry dependencies\n",
    "from arize.otel import register\n",
    "\n",
    "# Setup OTel via our convenience function\n",
    "tracer_provider = register(\n",
    "    space_id = os.getenv(\"ARIZE_SPACE_ID\"), # in app space settings page\n",
    "    api_key = os.getenv(\"ARIZE_API_KEY\"), # in app space settings page\n",
    "    project_name = \"portkey-debate\", # name this to whatever you would like\n",
    ")\n",
    "\n",
    "# Import openinference instrumentor to map Portkey traces to a standard format\n",
    "from openinference.instrumentation.portkey import PortkeyInstrumentor\n",
    "\n",
    "# Turn on the instrumentor\n",
    "PortkeyInstrumentor().instrument(tracer_provider=tracer_provider)\n",
    "\n",
    "tracer = tracer_provider.get_tracer(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNSnWIirus4u"
   },
   "source": [
    "# Setup application using Portkey Universal API for different LLM calls\n",
    "\n",
    "This application runs a structured debate between two LLMsâ€”one arguing â€œproâ€ and the other â€œconâ€ on a given topicâ€”while a third LLM acts as moderator to score each side and suggest prompt refinements. Over multiple iterations, the debate prompt is progressively improved to produce more balanced and persuasive arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from portkey_ai import Portkey\n",
    "import os\n",
    "\n",
    "# â”€â”€â”€ Initialize each LLM client â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "PORTKEY_API_KEY = os.getenv(\"PORTKEY_API_KEY\")\n",
    "\n",
    "# GPT-4o for â€œagainstâ€ arguments\n",
    "openai = Portkey(\n",
    "    api_key = PORTKEY_API_KEY,\n",
    "    virtual_key = os.getenv(\"OPENAI_VIRTUAL_KEY\")\n",
    ")\n",
    "\n",
    "# Claude for â€œproâ€ arguments\n",
    "claude = Portkey(\n",
    "    api_key      = PORTKEY_API_KEY,\n",
    "    virtual_key = os.getenv(\"CLAUDE_VIRTUAL_KEY\")\n",
    ")\n",
    "\n",
    "# Gemini for moderation & prompt refinement\n",
    "gemini = Portkey(\n",
    "    api_key      = PORTKEY_API_KEY,\n",
    "    virtual_key = os.getenv(\"GEMINI_VIRTUAL_KEY\")\n",
    ")\n",
    "\n",
    "\n",
    "# â”€â”€â”€ Single Debate Round Function â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def debate_round(topic: str, debate_prompt: str) -> dict:\n",
    "    \"\"\"\n",
    "    1ï¸âƒ£ Claude makes the PRO argument.\n",
    "    2ï¸âƒ£ GPT-4o makes the CON argument.\n",
    "    3ï¸âƒ£ Gemini scores both and suggests a refined prompt.\n",
    "    Returns dict with keys: pro, con, new_prompt.\n",
    "    \"\"\"\n",
    "\n",
    "    # PRO side (Claude)\n",
    "    pro_resp = claude.chat.completions.create(\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    f\"Argue in favor of the following topic:\\n\\n\"\n",
    "                    f\"**{topic}**\\n\\n\"\n",
    "                    f\"Use this debate prompt as context:\\n{debate_prompt}\"\n",
    "                )\n",
    "            }\n",
    "        ],\n",
    "        model    = \"claude-3-opus-20240229\",\n",
    "        max_tokens = 250\n",
    "\n",
    "    )\n",
    "    pro_text = pro_resp[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    # CON side (GPT-4o)\n",
    "    con_resp = openai.chat.completions.create(\n",
    "        model    = \"gpt-4\",\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    f\"Argue against the following topic:\\n\\n\"\n",
    "                    f\"**{topic}**\\n\\n\"\n",
    "                    f\"Use this debate prompt as context:\\n{debate_prompt}\"\n",
    "                )\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    con_text = con_resp[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    # Moderator (Gemini) â€” score & refine\n",
    "    mod_resp = gemini.chat.completions.create(\n",
    "        model    = \"gemini-1.5-pro\",\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    f\"You are a debate moderator. Here are the two sides on â€œ{topic}â€:\\n\\n\"\n",
    "                    f\"ðŸŸ¢ PRO ARGUMENT:\\n{pro_text}\\n\\n\"\n",
    "                    f\"ðŸ”´ CON ARGUMENT:\\n{con_text}\\n\\n\"\n",
    "                    \"1. Assign each argument a persuasiveness score (0â€“10).\\n\"\n",
    "                    \"2. Give 1â€“2 sentences on their main strengths/weaknesses.\\n\"\n",
    "                    \"3. Suggest an improved debate prompt that will yield more balanced arguments next round.\\n\"\n",
    "                    \"Return **only** the improved debate prompt.\"\n",
    "                )\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    new_prompt = mod_resp[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "\n",
    "    return {\"pro\": pro_text, \"con\": con_text, \"new_prompt\": new_prompt}\n",
    "\n",
    "\n",
    "# â”€â”€â”€ Run Multiple Rounds â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    topic          = \"Implementing a nationwide four-day workweek\"\n",
    "    initial_prompt = \"Debate the pros and cons of a four-day workweek.\"\n",
    "    rounds         = 3\n",
    "\n",
    "    prompt = initial_prompt\n",
    "    for i in range(1, rounds + 1):\n",
    "        result = debate_round(topic, prompt)\n",
    "        print(f\"\\nâ”€â”€ Round {i} â”€â”€\")\n",
    "        print(\"ðŸ”µ PRO:\\n\", result[\"pro\"])\n",
    "        print(\"\\nðŸ”´ CON:\\n\", result[\"con\"])\n",
    "        print(\"\\nðŸ› ï¸  Suggested New Prompt:\\n\", result[\"new_prompt\"])\n",
    "        prompt = result[\"new_prompt\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NxwKxn7uHLqE"
   },
   "source": [
    "#Evals\n",
    "\n",
    "Let's add some Arize Evals. Specifically we will add a toxicity eval to make sure the outputs from the debators aren't racist, sexist, chauvinistic, overly biased, or otherwise toxic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0ZnYSoovFsv"
   },
   "source": [
    "## Export Traces from Arize into Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from arize.exporter import ArizeExportClient\n",
    "from arize.utils.types import Environments\n",
    "\n",
    "client = ArizeExportClient()\n",
    "\n",
    "print('#### Exporting your primary dataset into a dataframe.')\n",
    "\n",
    "## TO RETRIEVE THE FIELDS FOR THIS AUTOMATICALLY, GO TO ARIZE -> PROJECTS -> YOUR PROJECT -> DOWNLOAD\n",
    "\n",
    "primary_df = client.export_model_to_df(\n",
    "    space_id=os.getenv(\"ARIZE_SPACE_ID\"),\n",
    "    model_id='portkey-debate',\n",
    "    environment=Environments.TRACING,\n",
    "    start_time=datetime.fromisoformat(''),\n",
    "    end_time=datetime.fromisoformat(''),\n",
    "    # Optionally specify columns to improve query performance\n",
    "    # columns=['context.span_id', 'attributes.llm.input']\n",
    ")\n",
    "primary_df[\"input\"] = primary_df[\"attributes.input.value\"]\n",
    "primary_df[\"output\"] = primary_df[\"attributes.output.value\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y43SIiPSvHoK"
   },
   "source": [
    "## Create Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.evals import (\n",
    "    TOXICITY_PROMPT_RAILS_MAP,\n",
    "    TOXICITY_PROMPT_TEMPLATE,\n",
    "    OpenAIModel,\n",
    "    download_benchmark_dataset,\n",
    "    llm_classify,\n",
    ")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OPENAI_API_KEY: \")\n",
    "\n",
    "model = OpenAIModel(\n",
    "    model_name=\"gpt-4\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "#The rails is used to hold the output to specific values based on the template\n",
    "#It will remove text such as \",,,\" or \"...\"\n",
    "#Will ensure the binary value expected from the template is returned\n",
    "rails = list(TOXICITY_PROMPT_RAILS_MAP.values())\n",
    "toxic_classifications = llm_classify(\n",
    "    dataframe=primary_df,\n",
    "    template=TOXICITY_PROMPT_TEMPLATE,\n",
    "    model=model,\n",
    "    rails=rails,\n",
    "    provide_explanation=True, #optional to generate explanations for the value produced by the eval LLM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_classifications[\"eval.tone_eval.label\"] = toxic_classifications[\"label\"]\n",
    "toxic_classifications[\"eval.tone_eval.explanation\"] = toxic_classifications[\"explanation\"]\n",
    "toxic_classifications = toxic_classifications.set_index(primary_df[\"context.span_id\"])\n",
    "toxic_classifications[\"context.span_id\"] = toxic_classifications.index\n",
    "toxic_classifications.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Um9kUZIPvQ3Z"
   },
   "source": [
    "## Export Evals dataset to Arize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arize.pandas.logger import Client\n",
    "\n",
    "arize_client = Client(\n",
    "    space_id=os.getenv(\"ARIZE_SPACE_ID\"),\n",
    "    api_key=os.getenv(\"ARIZE_API_KEY\")\n",
    ")\n",
    "\n",
    "arize_client.log_evaluations_sync(toxic_classifications, 'portkey-debate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
