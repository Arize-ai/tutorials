{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "43f8e85c-70c2-4de3-99b8-acdbb58d6c4a",
      "metadata": {
        "id": "43f8e85c-70c2-4de3-99b8-acdbb58d6c4a"
      },
      "source": [
        "\n",
        "\n",
        "<center> <img src=\"https://storage.googleapis.com/arize-assets/arize-logo-white.jpg\" width=\"300\"/> </center>\n",
        "\n",
        "# <center>Tracing via OTLP using Arize and Phoenix</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94f4db8b-dc93-4cef-ac58-205d1ec21b36",
      "metadata": {
        "id": "94f4db8b-dc93-4cef-ac58-205d1ec21b36"
      },
      "source": [
        "This guide demonstrates how to use Arize for monitoring and debugging your LLM using Traces and Spans. We're going to build a simple query engine using LlamaIndex and retrieval-augmented generation (RAG) to answer questions about the [Arize documentation](https://docs.arize.com/arize/). You can read more about LLM tracing [here](https://docs.arize.com/arize/llm-large-language-models/llm-traces). Arize & Phoenix make your LLM applications observable by visualizing the underlying structure of each call to your query engine and surfacing problematic `spans` of execution based on latency, token count, or other evaluation metrics.\n",
        "\n",
        "In this tutorial, you will:\n",
        "1. Use opentelemetry and [openinference](https://github.com/Arize-ai/openinference/tree/main) to instrument our application and sent traces via OTLP to Arize and Phoenix.\n",
        "2. Build a simple query engine using LlamaIndex that uses RAG to answer questions about the Arize documentation\n",
        "3. Inspect the traces and spans of your application to identify sources of latency and cost\n",
        "\n",
        "ℹ️ This notebook requires:\n",
        "- An OpenAI API key\n",
        "- An Arize Space & API Key (explained below)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "899f02b0-f638-4da8-a72d-371b07a5a28c",
      "metadata": {
        "id": "899f02b0-f638-4da8-a72d-371b07a5a28c"
      },
      "source": [
        "## Step 1: Install Dependencies 📚\n",
        "Let's get the notebook setup with dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2398520d-47d5-450e-a0c6-3969ede28626",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2398520d-47d5-450e-a0c6-3969ede28626",
        "outputId": "2dd42e85-6d1c-4734-81b0-0fbfc42c736b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opentelemetry-exporter-otlp\n",
            "  Downloading opentelemetry_exporter_otlp-1.28.2-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting openinference-instrumentation-llama-index>=1.3.0\n",
            "  Downloading openinference_instrumentation_llama_index-3.0.4-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc==1.28.2 (from opentelemetry-exporter-otlp)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.28.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-http==1.28.2 (from opentelemetry-exporter-otlp)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.28.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.28.2->opentelemetry-exporter-otlp) (1.2.15)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.28.2->opentelemetry-exporter-otlp) (1.66.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.63.2 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.28.2->opentelemetry-exporter-otlp) (1.68.0)\n",
            "Requirement already satisfied: opentelemetry-api~=1.15 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.28.2->opentelemetry-exporter-otlp) (1.28.2)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.28.2 (from opentelemetry-exporter-otlp-proto-grpc==1.28.2->opentelemetry-exporter-otlp)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.28.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.28.2 (from opentelemetry-exporter-otlp-proto-grpc==1.28.2->opentelemetry-exporter-otlp)\n",
            "  Downloading opentelemetry_proto-1.28.2-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk~=1.28.2 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.28.2->opentelemetry-exporter-otlp) (1.28.2)\n",
            "Requirement already satisfied: requests~=2.7 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-http==1.28.2->opentelemetry-exporter-otlp) (2.32.3)\n",
            "Collecting protobuf<6.0,>=5.0 (from opentelemetry-proto==1.28.2->opentelemetry-exporter-otlp-proto-grpc==1.28.2->opentelemetry-exporter-otlp)\n",
            "  Downloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Collecting openinference-instrumentation>=0.1.17 (from openinference-instrumentation-llama-index>=1.3.0)\n",
            "  Downloading openinference_instrumentation-0.1.18-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting openinference-semantic-conventions>=0.1.9 (from openinference-instrumentation-llama-index>=1.3.0)\n",
            "  Downloading openinference_semantic_conventions-0.1.12-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting opentelemetry-instrumentation (from openinference-instrumentation-llama-index>=1.3.0)\n",
            "  Downloading opentelemetry_instrumentation-0.49b2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions in /usr/local/lib/python3.10/dist-packages (from openinference-instrumentation-llama-index>=1.3.0) (0.49b2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from openinference-instrumentation-llama-index>=1.3.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from openinference-instrumentation-llama-index>=1.3.0) (1.16.0)\n",
            "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api~=1.15->opentelemetry-exporter-otlp-proto-grpc==1.28.2->opentelemetry-exporter-otlp) (8.5.0)\n",
            "Requirement already satisfied: packaging>=18.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation->openinference-instrumentation-llama-index>=1.3.0) (24.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api~=1.15->opentelemetry-exporter-otlp-proto-grpc==1.28.2->opentelemetry-exporter-otlp) (3.21.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http==1.28.2->opentelemetry-exporter-otlp) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http==1.28.2->opentelemetry-exporter-otlp) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http==1.28.2->opentelemetry-exporter-otlp) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http==1.28.2->opentelemetry-exporter-otlp) (2024.8.30)\n",
            "Downloading opentelemetry_exporter_otlp-1.28.2-py3-none-any.whl (7.0 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_grpc-1.28.2-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_http-1.28.2-py3-none-any.whl (17 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.28.2-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.28.2-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openinference_instrumentation_llama_index-3.0.4-py3-none-any.whl (25 kB)\n",
            "Downloading openinference_instrumentation-0.1.18-py3-none-any.whl (14 kB)\n",
            "Downloading openinference_semantic_conventions-0.1.12-py3-none-any.whl (9.1 kB)\n",
            "Downloading opentelemetry_instrumentation-0.49b2-py3-none-any.whl (30 kB)\n",
            "Downloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl (316 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.6/316.6 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf, openinference-semantic-conventions, opentelemetry-proto, opentelemetry-exporter-otlp-proto-common, opentelemetry-instrumentation, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, openinference-instrumentation, opentelemetry-exporter-otlp, openinference-instrumentation-llama-index\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.5\n",
            "    Uninstalling protobuf-4.25.5:\n",
            "      Successfully uninstalled protobuf-4.25.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.28.3 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.28.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openinference-instrumentation-0.1.18 openinference-instrumentation-llama-index-3.0.4 openinference-semantic-conventions-0.1.12 opentelemetry-exporter-otlp-1.28.2 opentelemetry-exporter-otlp-proto-common-1.28.2 opentelemetry-exporter-otlp-proto-grpc-1.28.2 opentelemetry-exporter-otlp-proto-http-1.28.2 opentelemetry-instrumentation-0.49b2 opentelemetry-proto-1.28.2 protobuf-5.28.3\n",
            "Collecting arize-phoenix\n",
            "  Downloading arize_phoenix-5.12.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting aioitertools (from arize-phoenix)\n",
            "  Downloading aioitertools-0.12.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting aiosqlite (from arize-phoenix)\n",
            "  Downloading aiosqlite-0.20.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting alembic<2,>=1.3.0 (from arize-phoenix)\n",
            "  Downloading alembic-1.14.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting arize-phoenix-evals>=0.13.1 (from arize-phoenix)\n",
            "  Downloading arize_phoenix_evals-0.17.5-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting arize-phoenix-otel>=0.5.1 (from arize-phoenix)\n",
            "  Downloading arize_phoenix_otel-0.6.1-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting authlib (from arize-phoenix)\n",
            "  Downloading Authlib-1.3.2-py2.py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from arize-phoenix) (5.5.0)\n",
            "Collecting fastapi (from arize-phoenix)\n",
            "  Downloading fastapi-0.115.5-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting grpc-interceptor (from arize-phoenix)\n",
            "  Downloading grpc_interceptor-0.15.4-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: grpcio in /usr/local/lib/python3.10/dist-packages (from arize-phoenix) (1.68.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from arize-phoenix) (0.27.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from arize-phoenix) (3.1.4)\n",
            "Requirement already satisfied: numpy!=2.0.0 in /usr/local/lib/python3.10/dist-packages (from arize-phoenix) (1.26.4)\n",
            "Requirement already satisfied: openinference-instrumentation>=0.1.12 in /usr/local/lib/python3.10/dist-packages (from arize-phoenix) (0.1.18)\n",
            "Requirement already satisfied: openinference-semantic-conventions>=0.1.12 in /usr/local/lib/python3.10/dist-packages (from arize-phoenix) (0.1.12)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp in /usr/local/lib/python3.10/dist-packages (from arize-phoenix) (1.28.2)\n",
            "Requirement already satisfied: opentelemetry-proto>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from arize-phoenix) (1.28.2)\n",
            "Requirement already satisfied: opentelemetry-sdk in /usr/local/lib/python3.10/dist-packages (from arize-phoenix) (1.28.2)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions in /usr/local/lib/python3.10/dist-packages (from arize-phoenix) (0.49b2)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.10/dist-packages (from arize-phoenix) (2.2.2)\n",
            "Requirement already satisfied: protobuf<6.0,>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from arize-phoenix) (5.28.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from arize-phoenix) (5.9.5)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from arize-phoenix) (17.0.0)\n",
            "Requirement already satisfied: pydantic!=2.0.*,<3,>=1.0 in /usr/local/lib/python3.10/dist-packages (from arize-phoenix) (2.9.2)\n",
            "Collecting python-multipart (from arize-phoenix)\n",
            "  Downloading python_multipart-0.0.17-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from arize-phoenix) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from arize-phoenix) (1.13.1)\n",
            "Requirement already satisfied: sqlalchemy<3,>=2.0.4 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy[asyncio]<3,>=2.0.4->arize-phoenix) (2.0.36)\n",
            "Collecting sqlean-py>=3.45.1 (from arize-phoenix)\n",
            "  Downloading sqlean.py-3.47.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Collecting starlette (from arize-phoenix)\n",
            "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting strawberry-graphql==0.243.1 (from arize-phoenix)\n",
            "  Downloading strawberry_graphql-0.243.1-py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from arize-phoenix) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.10/dist-packages (from arize-phoenix) (4.12.2)\n",
            "Collecting uvicorn (from arize-phoenix)\n",
            "  Downloading uvicorn-0.32.1-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting websockets (from arize-phoenix)\n",
            "  Downloading websockets-14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from arize-phoenix) (1.16.0)\n",
            "Collecting graphql-core<3.4.0,>=3.2.0 (from strawberry-graphql==0.243.1->arize-phoenix)\n",
            "  Downloading graphql_core-3.2.5-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from strawberry-graphql==0.243.1->arize-phoenix) (2.8.2)\n",
            "Collecting Mako (from alembic<2,>=1.3.0->arize-phoenix)\n",
            "  Downloading Mako-1.3.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: opentelemetry-api in /usr/local/lib/python3.10/dist-packages (from openinference-instrumentation>=0.1.12->arize-phoenix) (1.28.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->arize-phoenix) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->arize-phoenix) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.0.*,<3,>=1.0->arize-phoenix) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.0.*,<3,>=1.0->arize-phoenix) (2.23.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<3,>=2.0.4->sqlalchemy[asyncio]<3,>=2.0.4->arize-phoenix) (3.1.1)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.10/dist-packages (from authlib->arize-phoenix) (43.0.3)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette->arize-phoenix) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->arize-phoenix) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->arize-phoenix) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->arize-phoenix) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->arize-phoenix) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->arize-phoenix) (0.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->arize-phoenix) (3.0.2)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc==1.28.2 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp->arize-phoenix) (1.28.2)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http==1.28.2 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp->arize-phoenix) (1.28.2)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.28.2->opentelemetry-exporter-otlp->arize-phoenix) (1.2.15)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.28.2->opentelemetry-exporter-otlp->arize-phoenix) (1.66.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.28.2 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.28.2->opentelemetry-exporter-otlp->arize-phoenix) (1.28.2)\n",
            "Requirement already satisfied: requests~=2.7 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-http==1.28.2->opentelemetry-exporter-otlp->arize-phoenix) (2.32.3)\n",
            "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api->openinference-instrumentation>=0.1.12->arize-phoenix) (8.5.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->arize-phoenix) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->arize-phoenix) (3.5.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->arize-phoenix) (8.1.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette->arize-phoenix) (1.2.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.7.0->strawberry-graphql==0.243.1->arize-phoenix) (1.16.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography->authlib->arize-phoenix) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography->authlib->arize-phoenix) (2.22)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api->openinference-instrumentation>=0.1.12->arize-phoenix) (3.21.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http==1.28.2->opentelemetry-exporter-otlp->arize-phoenix) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http==1.28.2->opentelemetry-exporter-otlp->arize-phoenix) (2.2.3)\n",
            "Downloading arize_phoenix-5.12.0-py3-none-any.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading strawberry_graphql-0.243.1-py3-none-any.whl (306 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m306.0/306.0 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.14.0-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading arize_phoenix_evals-0.17.5-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading arize_phoenix_otel-0.6.1-py3-none-any.whl (10 kB)\n",
            "Downloading sqlean.py-3.47.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aioitertools-0.12.0-py3-none-any.whl (24 kB)\n",
            "Downloading aiosqlite-0.20.0-py3-none-any.whl (15 kB)\n",
            "Downloading Authlib-1.3.2-py2.py3-none-any.whl (225 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.1/225.1 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.5-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpc_interceptor-0.15.4-py3-none-any.whl (20 kB)\n",
            "Downloading python_multipart-0.0.17-py3-none-any.whl (24 kB)\n",
            "Downloading uvicorn-0.32.1-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.2/168.2 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_core-3.2.5-py3-none-any.whl (203 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.2/203.2 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Mako-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sqlean-py, websockets, uvicorn, python-multipart, Mako, grpc-interceptor, graphql-core, aiosqlite, aioitertools, strawberry-graphql, starlette, alembic, fastapi, authlib, arize-phoenix-evals, arize-phoenix-otel, arize-phoenix\n",
            "Successfully installed Mako-1.3.6 aioitertools-0.12.0 aiosqlite-0.20.0 alembic-1.14.0 arize-phoenix-5.12.0 arize-phoenix-evals-0.17.5 arize-phoenix-otel-0.6.1 authlib-1.3.2 fastapi-0.115.5 graphql-core-3.2.5 grpc-interceptor-0.15.4 python-multipart-0.0.17 sqlean-py-3.47.0 starlette-0.41.3 strawberry-graphql-0.243.1 uvicorn-0.32.1 websockets-14.1\n"
          ]
        }
      ],
      "source": [
        "# Dependencies needed to build the Llama Index RAG application\n",
        "!pip install gcsfs openai>=1 llama-index>=0.10.3\n",
        "\n",
        "# Dependencies needed to export spans and send them to our collectors: Arize and/or Phoenix\n",
        "!pip install opentelemetry-exporter-otlp 'openinference-instrumentation-llama-index>=1.3.0'\n",
        "\n",
        "# Install Phoenix if you want to send traces to Arize and Phoenix simultaneously.\n",
        "!pip install arize-phoenix"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bf0c55e-69f0-4d81-b65e-13388866b467",
      "metadata": {
        "id": "7bf0c55e-69f0-4d81-b65e-13388866b467"
      },
      "source": [
        "## Step 2: OTLP Instrumentation\n",
        "Let's import the dependencies we need"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "91c4eb78-f33b-4db9-8d60-ca22ef392f3f",
      "metadata": {
        "id": "91c4eb78-f33b-4db9-8d60-ca22ef392f3f"
      },
      "outputs": [],
      "source": [
        "from opentelemetry.sdk.trace.export import SimpleSpanProcessor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61d13b60-f4dc-4992-bcc3-af57acb59792",
      "metadata": {
        "id": "61d13b60-f4dc-4992-bcc3-af57acb59792"
      },
      "source": [
        "### Step 2.a: Define an exporter to Phoenix\n",
        "We need to start a `phoenix` session to act as a collector for the spans we export."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e21500b8-4252-498b-9a31-7a5ffe8a5549",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "e21500b8-4252-498b-9a31-7a5ffe8a5549",
        "outputId": "7bb95147-a6d8-4272-9e40-602ba35b8366"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌍 To view the Phoenix app in your browser, visit https://7abbvil1cr1-496ff2e9c6d22116-6006-colab.googleusercontent.com/\n",
            "📖 For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
          ]
        }
      ],
      "source": [
        "import phoenix as px\n",
        "session = px.launch_app()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b3c762f-0b0c-42f3-8c3f-557fd1eb1a9b",
      "metadata": {
        "id": "0b3c762f-0b0c-42f3-8c3f-557fd1eb1a9b"
      },
      "source": [
        "Next, we create an OTLP exporter with the Phoenix endpoint detailed above. Note that we use HTTP to export to Phoenix, which acts as a collector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d0ec33a1-8939-4637-be43-ab704836f576",
      "metadata": {
        "id": "d0ec33a1-8939-4637-be43-ab704836f576"
      },
      "outputs": [],
      "source": [
        "from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter as PhoenixOTLPSpanExporter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b1e5a190-1ccc-46de-b536-513d9855f6c6",
      "metadata": {
        "id": "b1e5a190-1ccc-46de-b536-513d9855f6c6"
      },
      "outputs": [],
      "source": [
        "phoenix_endpoint = \"http://127.0.0.1:6006/v1/traces\"\n",
        "span_phoenix_exporter = PhoenixOTLPSpanExporter(endpoint=phoenix_endpoint)\n",
        "span_phoenix_processor = SimpleSpanProcessor(span_exporter=span_phoenix_exporter)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "729ce7dc-55fa-44ce-b457-754c32b3d4fc",
      "metadata": {
        "id": "729ce7dc-55fa-44ce-b457-754c32b3d4fc"
      },
      "source": [
        "### Step 2.b: Define an exporter to Arize\n",
        "Creating an Arize exporter is very similar to what we did for Phoenix. We just need 2 more things:\n",
        "* Space and API keys, that will be send as headers\n",
        "* Model ID and version, sent as resource attributes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16bce764-0d42-4e9a-a86e-bee64a30a07c",
      "metadata": {
        "id": "16bce764-0d42-4e9a-a86e-bee64a30a07c"
      },
      "source": [
        "Copy the Arize API_KEY and SPACE_ID from your Space Settings page (shown below) to the variables in the cell below. We will also be setting up some metadata to use across all logging.\n",
        "\n",
        "<center><img src=\"https://storage.googleapis.com/arize-assets/fixtures/copy-id-and-key.png\" width=\"700\"></center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "83f3a52e-873c-4128-a183-a9db38f51305",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83f3a52e-873c-4128-a183-a9db38f51305",
        "outputId": "92a6237a-d111-48f0-bcb7-6587eac96023"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Import and Setup Arize Client Done! Now we can start using Arize!\n"
          ]
        }
      ],
      "source": [
        "SPACE_ID = \"U3BhY2U6NjM3MjoyMXJG\" # Change this line\n",
        "API_KEY = \"416ad605925bf226fd9\" # Change this line\n",
        "\n",
        "model_id = \"tutorial-otlp-tracing-llama-index-rag\"\n",
        "model_version = \"1.0\"\n",
        "\n",
        "if SPACE_ID == \"SPACE_ID\" or API_KEY == \"API_KEY\":\n",
        "    raise ValueError(\"❌ CHANGE SPACE_ID AND/OR API_KEY\")\n",
        "else:\n",
        "    print(\"✅ Import and Setup Arize Client Done! Now we can start using Arize!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28dbeb10-f0c3-4225-a225-ac7c18571e6e",
      "metadata": {
        "id": "28dbeb10-f0c3-4225-a225-ac7c18571e6e"
      },
      "source": [
        "Next, we create an OTLP exproter with the Arize endpoint detailed above. Note that we use GRPC to export to Arize, which acts as a collector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d1e103c2-5b87-4ba3-9d8d-c250a748ff31",
      "metadata": {
        "id": "d1e103c2-5b87-4ba3-9d8d-c250a748ff31"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from opentelemetry.sdk.resources import Resource\n",
        "from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter as ArizeOTLPSpanExporter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "d68e9e43-f046-4f50-8934-e874faff33d6",
      "metadata": {
        "id": "d68e9e43-f046-4f50-8934-e874faff33d6"
      },
      "outputs": [],
      "source": [
        "# Set the Space and API keys as headers\n",
        "os.environ['OTEL_EXPORTER_OTLP_TRACES_HEADERS']=f\"space_id={SPACE_ID},api_key={API_KEY}\"\n",
        "\n",
        "# Set the model id and version as resource attributes\n",
        "resource = Resource(\n",
        "    attributes={\n",
        "        \"model_id\":model_id,\n",
        "        \"model_version\":model_version,\n",
        "    }\n",
        ")\n",
        "\n",
        "arize_endpoint = \"https://otlp.arize.com/v1\"\n",
        "span_arize_exporter = ArizeOTLPSpanExporter(endpoint=arize_endpoint)\n",
        "span_arize_processor = SimpleSpanProcessor(span_exporter=span_arize_exporter)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8bd34ae-4b67-4a8a-b538-b5022635d0ca",
      "metadata": {
        "id": "a8bd34ae-4b67-4a8a-b538-b5022635d0ca"
      },
      "source": [
        "### Step 2.c: Define a trace provider and initiate the instrumentation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "808cafd0-4490-4db0-a7b5-b7903ff9f063",
      "metadata": {
        "id": "808cafd0-4490-4db0-a7b5-b7903ff9f063"
      },
      "outputs": [],
      "source": [
        "from opentelemetry.sdk import trace as trace_sdk\n",
        "from opentelemetry import trace as trace_api\n",
        "from openinference.instrumentation.llama_index import LlamaIndexInstrumentor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "de43f124-a8fd-41be-8a49-c82188f420c8",
      "metadata": {
        "id": "de43f124-a8fd-41be-8a49-c82188f420c8"
      },
      "outputs": [],
      "source": [
        "tracer_provider = trace_sdk.TracerProvider(resource=resource)\n",
        "tracer_provider.add_span_processor(span_processor=span_phoenix_processor)\n",
        "tracer_provider.add_span_processor(span_processor=span_arize_processor)\n",
        "trace_api.set_tracer_provider(tracer_provider=tracer_provider)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "cb13ac75-083b-402f-862f-12316c3f3cb5",
      "metadata": {
        "id": "cb13ac75-083b-402f-862f-12316c3f3cb5"
      },
      "outputs": [],
      "source": [
        "# If you are running the instrumentation from a Colab environment, set skip_dep_check to True\n",
        "# For more information check https://github.com/Arize-ai/openinference/issues/100\n",
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "LlamaIndexInstrumentor().instrument(skip_dep_check=IN_COLAB)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b4aa150-82f5-4268-b7fd-95b059b03d59",
      "metadata": {
        "id": "5b4aa150-82f5-4268-b7fd-95b059b03d59"
      },
      "source": [
        "## Step 3: Build Your Llama Index RAG Application 📁\n",
        "Let's import the dependencies we need"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "a0731faa-f263-4441-9cee-50460b5842a0",
      "metadata": {
        "id": "a0731faa-f263-4441-9cee-50460b5842a0"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from getpass import getpass\n",
        "\n",
        "import openai\n",
        "from gcsfs import GCSFileSystem\n",
        "from llama_index.core import (\n",
        "    Settings,\n",
        "    StorageContext,\n",
        "    load_index_from_storage,\n",
        ")\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8874e8d7-2a95-4547-8061-768e9acab805",
      "metadata": {
        "id": "8874e8d7-2a95-4547-8061-768e9acab805"
      },
      "source": [
        "Set your OpenAI API key if it is not already set as an environment variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "9f29abbe-5bab-49b3-a643-c15a5d4f6265",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f29abbe-5bab-49b3-a643-c15a5d4f6265",
        "outputId": "13d2b7d7-5e56-47d6-ba15-29c3ae3d00b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔑 Enter your OpenAI API key: ··········\n"
          ]
        }
      ],
      "source": [
        "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
        "    openai_api_key = getpass(\"🔑 Enter your OpenAI API key: \")\n",
        "openai.api_key = openai_api_key\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b2fbe94-f071-47f5-9ebb-3563560814ab",
      "metadata": {
        "id": "8b2fbe94-f071-47f5-9ebb-3563560814ab"
      },
      "source": [
        "This example uses a `RetrieverQueryEngine` over a pre-built index of the Arize documentation, but you can use whatever LlamaIndex application you like. Download our pre-built index of the Arize docs from cloud storage and instantiate your storage context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "de04f2a9-cb92-4c7f-945f-0a629bdcbe20",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de04f2a9-cb92-4c7f-945f-0a629bdcbe20",
        "outputId": "78352733-9af5-4a2a-eb94-e73140526082"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:llama_index.core.graph_stores.simple:No existing llama_index.core.graph_stores.simple found at arize-phoenix-assets/datasets/unstructured/llm/llama-index/arize-docs/index/graph_store.json. Initializing a new graph_store from scratch. \n"
          ]
        }
      ],
      "source": [
        "file_system = GCSFileSystem(project=\"public-assets-275721\")\n",
        "index_path = \"arize-phoenix-assets/datasets/unstructured/llm/llama-index/arize-docs/index/\"\n",
        "storage_context = StorageContext.from_defaults(\n",
        "    fs=file_system,\n",
        "    persist_dir=index_path,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "806ebe03-abbc-4545-95b7-4a7a5942cba2",
      "metadata": {
        "id": "806ebe03-abbc-4545-95b7-4a7a5942cba2"
      },
      "source": [
        "We are now ready to instantiate our query engine that will perform retrieval-augmented generation (RAG). Query engine is a generic interface in LlamaIndex that allows you to ask question over your data. A query engine takes in a natural language query, and returns a rich response. It is built on top of Retrievers. You can compose multiple query engines to achieve more advanced capability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "bcf35c57-8399-4d31-8e57-735e0de2ce57",
      "metadata": {
        "id": "bcf35c57-8399-4d31-8e57-735e0de2ce57"
      },
      "outputs": [],
      "source": [
        "Settings.llm = OpenAI(model=\"gpt-4-turbo-preview\")\n",
        "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")\n",
        "index = load_index_from_storage(\n",
        "    storage_context,\n",
        ")\n",
        "query_engine = index.as_query_engine()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e00f99cc-f3e7-4b74-a613-6c0b1df70ef1",
      "metadata": {
        "id": "e00f99cc-f3e7-4b74-a613-6c0b1df70ef1"
      },
      "source": [
        "Let's test asking a question:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "4e13d61d-3cab-4e07-a14b-357038646ad2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e13d61d-3cab-4e07-a14b-357038646ad2",
        "outputId": "ecd2c815-50a1-40aa-b6f6-f55e62ce5c20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arize is a machine learning observability platform that assists AI Engineers in monitoring, troubleshooting, and explaining their models. It enables you to monitor your model's real-time performance, even when there's a delay in receiving ground truth or feedback. The platform aids in identifying the root causes of model failures or performance degradation through tracing and explainability features. Additionally, it allows for the comparison of performance across multiple models and provides insights into drift, data quality, and model fairness/bias metrics. Arize is designed to integrate seamlessly with your existing machine learning infrastructure, offering flexibility in deployment as either a SaaS or an on-premise solution. This makes it a versatile tool for AI Engineers working in teams of any size, from individual contributors to large enterprises.\n"
          ]
        }
      ],
      "source": [
        "response = query_engine.query(\"What is Arize and how can it help me as an AI Engineer?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e20a4000-8267-44a1-a849-768167aa6624",
      "metadata": {
        "id": "e20a4000-8267-44a1-a849-768167aa6624"
      },
      "source": [
        "Great! Our application works. Let's move on to the Observability Instrumentation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fd4cadb-55b5-49b1-8bf3-8e4ef7a1a4f6",
      "metadata": {
        "id": "2fd4cadb-55b5-49b1-8bf3-8e4ef7a1a4f6"
      },
      "source": [
        "## Step 4: Use our instrumented query engine\n",
        "\n",
        "We will download a dataset of queries for our RAG application to answer and see the traces appear in Arize and Phoenix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "2096825c-ba77-4c44-9460-7b82a3de7ea7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2096825c-ba77-4c44-9460-7b82a3de7ea7",
        "outputId": "3fc8f9c4-1a4b-4cb3-e9c8-3e608b2e29bc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['How do I use the SDK to upload a ranking model?',\n",
              " 'What drift metrics are supported in Arize?',\n",
              " 'Does Arize support batch models?',\n",
              " 'Does Arize support training data?',\n",
              " 'How do I configure a threshold if my data has seasonality trends?']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "from urllib.request import urlopen\n",
        "\n",
        "queries_url = \"http://storage.googleapis.com/arize-phoenix-assets/datasets/unstructured/llm/context-retrieval/arize_docs_queries.jsonl\"\n",
        "queries = []\n",
        "with urlopen(queries_url) as response:\n",
        "    for line in response:\n",
        "        line = line.decode(\"utf-8\").strip()\n",
        "        data = json.loads(line)\n",
        "        queries.append(data[\"query\"])\n",
        "\n",
        "queries[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "59677acb-788e-402d-ac5d-1f96b911d83c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59677acb-788e-402d-ac5d-1f96b911d83c",
        "outputId": "074da824-b4f6-4cdd-d165-d9a240d4593e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:41<00:00,  8.33s/it]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "from openinference.instrumentation import using_attributes\n",
        "\n",
        "N1 = 5 # Number of traces for your first session\n",
        "SESSION_ID_1 = \"session-id-1\" # Identifer for your first session\n",
        "USER_ID_1 = \"john_smith\" # Identifer for your first session\n",
        "METADATA = {\n",
        "    \"key_bool\": True,\n",
        "    \"key_str\": \"value1\",\n",
        "    \"key_int\": 1\n",
        "}\n",
        "\n",
        "qa_pairs = []\n",
        "for query in tqdm(queries[:N1]):\n",
        "    with using_attributes(\n",
        "        session_id=SESSION_ID_1,\n",
        "        user_id=USER_ID_1,\n",
        "        metadata=METADATA,\n",
        "    ):\n",
        "        resp = query_engine.query(query)\n",
        "        qa_pairs.append((query,resp))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "908387b1-e514-455d-9e5b-5d574937dda9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "908387b1-e514-455d-9e5b-5d574937dda9",
        "outputId": "99f0d471-737e-4190-c133-c87ea5df795d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:13<00:00,  4.39s/it]\n"
          ]
        }
      ],
      "source": [
        "N2 = 3 # Number of traces for your second session\n",
        "SESSION_ID_2 = \"session-id-2\" # Identifer for your second session\n",
        "USER_ID_2 = \"jane_doe\" # Identifer for your second session\n",
        "\n",
        "for query in tqdm(queries[N1:N1+N2]):\n",
        "    with using_attributes(\n",
        "        session_id=SESSION_ID_2,\n",
        "        user_id=USER_ID_2,\n",
        "        metadata=METADATA\n",
        "    ):\n",
        "        resp = query_engine.query(query)\n",
        "        qa_pairs.append((query,resp))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "18e79697-36d1-4929-9413-05de9903d159",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18e79697-36d1-4929-9413-05de9903d159",
        "outputId": "f3a9a1a3-e7c7-4d57-f548-b5d7d2a351c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------\n",
            ">> QUESTION: How do I use the SDK to upload a ranking model?\n",
            ">> ANSWER: To upload a ranking model using the SDK, you would typically follow a series of steps that align with the SDK's documentation and functionalities. While the specific instructions for uploading a ranking model can vary depending on the SDK you are using, a general approach might include:\n",
            "\n",
            "1. **Initialization**: Start by initializing the SDK in your development environment. This usually involves importing the necessary libraries and setting up any required credentials or API keys for authentication.\n",
            "\n",
            "2. **Model Preparation**: Ensure your ranking model is ready for upload. This might involve training the model on your data, evaluating its performance using appropriate metrics (such as NDCG, GroupAUC, MAP, MRR, AUC, PR-AUC, Log Loss), and saving it in a format supported by the SDK.\n",
            "\n",
            "3. **Define Metadata**: Prepare any metadata that needs to accompany your model. This could include information about the model's use case (e.g., Search Ranking, Collaborative Filtering Recommender Systems, Content Filtering Recommender Systems, Classification-Based Ranking Models), the expected fields (such as rank, relevance score, relevance_labels), and the performance metrics used for evaluation.\n",
            "\n",
            "4. **Upload Command**: Use the SDK's specific command or function for uploading models. This typically requires specifying the path to your model file, any relevant metadata, and possibly the target environment or model name under which it will be stored.\n",
            "\n",
            "5. **Verification**: After uploading, verify that the model has been successfully uploaded and is accessible for its intended use. This might involve performing a test query or using the SDK's tools to list available models.\n",
            "\n",
            "6. **Integration and Testing**: Finally, integrate the uploaded model with your application or service and conduct thorough testing to ensure it performs as expected in a live environment, particularly focusing on how it ranks query results based on relevance.\n",
            "\n",
            "Remember, the exact commands and steps depend on the specifics of the SDK you are using, so it's important to consult the SDK's documentation for detailed instructions tailored to your scenario.\n",
            "\n",
            "-------------------------------------------------------\n",
            ">> QUESTION: What drift metrics are supported in Arize?\n",
            ">> ANSWER: Arize supports the Population Stability Index, KL Divergence, and Wasserstein Distance as drift metrics.\n",
            "\n",
            "---------------------------------------------\n",
            ">> QUESTION: Does Arize support batch models?\n",
            ">> ANSWER: Yes, Arize supports batch models as it integrates with platforms like UbiOps, which allows for the deployment and serving of models, indicating support for various model operational modes including batch processing.\n",
            "\n",
            "----------------------------------------------\n",
            ">> QUESTION: Does Arize support training data?\n",
            ">> ANSWER: Yes, Arize supports the ingestion of training data to enable monitoring and analysis of model performance, including comparisons with production data, identifying drift, and understanding data quality issues.\n",
            "\n",
            "------------------------------------------------------------------------------\n",
            ">> QUESTION: How do I configure a threshold if my data has seasonality trends?\n",
            ">> ANSWER: To configure a threshold for data with seasonality trends, you should consider adjusting the auto threshold sensitivity to better match the seasonal variations in your data. This can be done by editing the standard deviation number in the 'Monitor Settings' card. By increasing or decreasing the standard deviation, you can make the threshold more or less sensitive to changes in your data, which may help account for seasonal trends.\n",
            "\n",
            "--------------------------------------------------------------------------------------\n",
            ">> QUESTION: How are clusters in the UMAP calculated? When are the clusters refreshed?\n",
            ">> ANSWER: Clusters in the UMAP are calculated based on the similarity of data points, where groups of related points form clusters. The closer points or clusters are to each other, the more similar they are. This similarity is determined through the UMAP algorithm, which visualizes the global and local structure of the data by projecting it into a lower-dimensional space. The clusters are refreshed when a new point in time is selected on the drift visualization at the top of the page and a new UMAP is generated to visualize the selected point in time. This process allows for the investigation of how data clusters have drifted or changed over time.\n",
            "\n",
            "------------------------------------------\n",
            ">> QUESTION: How does Arize calculate AUC?\n",
            ">> ANSWER: The provided context information does not include details on how Arize calculates AUC (Area Under the Curve). It focuses on how Arize calculates drift metrics and the methodology for binning within the drift tab, but does not mention the calculation of AUC.\n",
            "\n",
            "---------------------------------------------------------\n",
            ">> QUESTION: Can I send truth labels to Arize separtely? \n",
            ">> ANSWER: Yes, you can send truth labels to Arize separately.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for q,a in qa_pairs:\n",
        "    q_msg = f\">> QUESTION: {q}\"\n",
        "    print(f\"{'-'*len(q_msg)}\")\n",
        "    print(q_msg)\n",
        "    print(f\">> ANSWER: {a}\\n\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}