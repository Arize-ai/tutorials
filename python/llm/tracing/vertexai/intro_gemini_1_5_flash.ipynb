{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yVV6txOmNMn"
      },
      "source": [
        "<center>\n",
        "    <p style=\"text-align:center\">\n",
        "    <img alt=\"arize logo\" src=\"https://storage.googleapis.com/arize-assets/arize-logo-white.jpg\" width=\"300\"/>\n",
        "        <br>\n",
        "        <a href=\"https://docs.arize.com/arize/\">Docs</a>\n",
        "        |\n",
        "        <a href=\"https://github.com/Arize-ai/client_python\">GitHub</a>\n",
        "        |\n",
        "        <a href=\"https://join.slack.com/t/arize-ai/shared_invite/zt-1px8dcmlf-fmThhDFD_V_48oU7ALan4Q\">Community</a>\n",
        "    </p>\n",
        "</center>\n",
        "\n",
        "# <center>VertexAI Tracing</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1DnOs6rkbOy"
      },
      "source": [
        "Learn how to instrument vertex ai with Arize. Learn more about [Gemini 1.5 Flash](https://deepmind.google/technologies/gemini/flash/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61RBz8LLbxCR"
      },
      "source": [
        "## Getting Started\n",
        "\n",
        "In order to activate VertexAI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "1. Create a new Google Cloud project\n",
        "2. Create a billing account linked to this project\n",
        "3. Enable the Vertex API with the above link\n",
        "4. Save the project id to copy into the cell below\n",
        "\n",
        "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No17Cw5hgx12"
      },
      "source": [
        "### Install dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFy3H3aPgx12",
        "outputId": "21926732-dd7b-455d-cc20-fce09de00b22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/118.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.7/118.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.3/64.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.2/159.2 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.1 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install Vertex AI SDK\n",
        "!pip install --upgrade --user --quiet google-cloud-aiplatform\n",
        "\n",
        "# External dependencies needed to instrument your VertexAI application using opentelemetry\n",
        "!pip install -qq opentelemetry-sdk==1.28.0 opentelemetry-exporter-otlp==1.28.0 opentelemetry-proto==1.28.0\n",
        "\n",
        "# Arize dependencies needed to instrument your VertexAI application using openinference\n",
        "!pip install -qq \"openinference-instrumentation-vertexai>=0.1.4\" \"arize-otel>=0.7.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5Xep4W9lq-Z"
      },
      "source": [
        "### Restart runtime\n",
        "\n",
        "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which restarts the current kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XRvKdaPDTznN"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbmM4z7FOBpM"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. ⚠️</b>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmWOrTJ3gx13"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you are running this notebook on Google Colab, run the cell below to authenticate your environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NyKGtVQjgx13"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrHzbVqnK0GF"
      },
      "source": [
        "### Setup Arize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLeGwTPTK5ku",
        "outputId": "f8f2efdb-dca6-4116-e054-b4b4c30a3b4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔑 Enter your Space ID: ··········\n",
            "🔑 Enter your API Key: ··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "if not (SPACE_ID := os.getenv(\"SPACE_ID\")):\n",
        "    SPACE_ID = getpass(\"🔑 Enter your Space ID: \")\n",
        "\n",
        "if not (API_KEY := os.getenv(\"API_KEY\")):\n",
        "    API_KEY = getpass(\"🔑 Enter your API Key: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae28EeqDK2nu",
        "outputId": "7e0463cb-b855-4f8d-ff20-bb37adf05858"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔭 OpenTelemetry Tracing Details 🔭\n",
            "|  Arize Project: vertexai-tracing-tutorial\n",
            "|  Span Processor: BatchSpanProcessor\n",
            "|  Collector Endpoint: otlp.arize.com\n",
            "|  Transport: gRPC\n",
            "|  Transport Headers: {'space_id': '****', 'api_key': '****', 'user-agent': '****'}\n",
            "|  \n",
            "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from openinference.instrumentation.vertexai import VertexAIInstrumentor\n",
        "\n",
        "from arize.otel import register\n",
        "\n",
        "# Setup OTel via our convenience function\n",
        "tracer_provider = register(\n",
        "    space_id = SPACE_ID,\n",
        "    api_key = API_KEY,\n",
        "    project_name = \"vertexai-tracing-tutorial\",\n",
        ")\n",
        "\n",
        "VertexAIInstrumentor().instrument(tracer_provider=tracer_provider)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF4l8DTdWgPY"
      },
      "source": [
        "### Set Google Cloud project information and initialize Vertex AI SDK\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nqwi-5ufWp_B",
        "outputId": "e037bda0-77de-4687-cf1c-f987a7bbae31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Project ID:··········\n"
          ]
        }
      ],
      "source": [
        "# Use the environment variable if the user doesn't provide Project ID.\n",
        "import os\n",
        "from getpass import getpass\n",
        "import vertexai\n",
        "\n",
        "PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT\", None)\n",
        "if not PROJECT_ID:\n",
        "    PROJECT_ID = getpass(\"Enter your Project ID:\")\n",
        "\n",
        "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXHfaVS66_01"
      },
      "source": [
        "### Import libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lslYAvw37JGQ"
      },
      "outputs": [],
      "source": [
        "import IPython.display\n",
        "from vertexai.generative_models import (\n",
        "    GenerationConfig,\n",
        "    GenerativeModel,\n",
        "    HarmBlockThreshold,\n",
        "    HarmCategory,\n",
        "    Part,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY1nfXrqRxVX"
      },
      "source": [
        "### Load the Gemini 1.5 Flash model\n",
        "\n",
        "To learn more about all [Gemini API models on Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "U7ExWmuLBdIA"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"gemini-1.5-flash-002\"  # @param {type:\"string\"}\n",
        "\n",
        "model = GenerativeModel(MODEL_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9OKM0-4SQf8"
      },
      "source": [
        "### Vertex AI SDK basic usage\n",
        "\n",
        "Below is a simple example that demonstrates how to prompt the Gemini 1.5 Flash model using the Vertex AI SDK. Learn more about the [Gemini API parameters](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini#gemini-pro)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhFxrtfdSwOP",
        "outputId": "1a536781-543d-40a0-ae5c-d193a7597e7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total_tokens: 32\n",
            "total_billable_characters: 108\n",
            "\n",
            "\n",
            "Answer:\n",
            "J'aime les bagels.\n",
            "\n",
            "\n",
            "Usage metadata:\n",
            "{'prompt_token_count': 32, 'candidates_token_count': 7, 'total_token_count': 39}\n",
            "\n",
            "Finish reason:\n",
            "1\n",
            "\n",
            "Safety settings:\n",
            "[category: HARM_CATEGORY_HATE_SPEECH\n",
            "probability: NEGLIGIBLE\n",
            "probability_score: 0.216011539\n",
            "severity: HARM_SEVERITY_LOW\n",
            "severity_score: 0.258325756\n",
            ", category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
            "probability: NEGLIGIBLE\n",
            "probability_score: 0.0609752387\n",
            "severity: HARM_SEVERITY_NEGLIGIBLE\n",
            "severity_score: 0.114368707\n",
            ", category: HARM_CATEGORY_HARASSMENT\n",
            "probability: NEGLIGIBLE\n",
            "probability_score: 0.181263119\n",
            "severity: HARM_SEVERITY_NEGLIGIBLE\n",
            "severity_score: 0.164516851\n",
            ", category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
            "probability: NEGLIGIBLE\n",
            "probability_score: 0.0900932476\n",
            "severity: HARM_SEVERITY_LOW\n",
            "severity_score: 0.218669638\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "# Load a example model with system instructions\n",
        "example_model = GenerativeModel(\n",
        "    MODEL_ID,\n",
        "    system_instruction=[\n",
        "        \"You are a helpful language translator.\",\n",
        "        \"Your mission is to translate text in English to French.\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Set model parameters\n",
        "generation_config = GenerationConfig(\n",
        "    temperature=0.9,\n",
        "    top_p=1.0,\n",
        "    top_k=32,\n",
        "    candidate_count=1,\n",
        "    max_output_tokens=8192,\n",
        ")\n",
        "\n",
        "# Set safety settings\n",
        "safety_settings = {\n",
        "    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "}\n",
        "\n",
        "prompt = \"\"\"\n",
        "  User input: I like bagels.\n",
        "  Answer:\n",
        "\"\"\"\n",
        "\n",
        "# Set contents to send to the model\n",
        "contents = [prompt]\n",
        "\n",
        "# Counts tokens\n",
        "print(example_model.count_tokens(contents))\n",
        "\n",
        "# Prompt the model to generate content\n",
        "response = example_model.generate_content(\n",
        "    contents,\n",
        "    generation_config=generation_config,\n",
        "    safety_settings=safety_settings,\n",
        ")\n",
        "\n",
        "# Print the model response\n",
        "print(f\"\\nAnswer:\\n{response.text}\")\n",
        "print(f'\\nUsage metadata:\\n{response.to_dict().get(\"usage_metadata\")}')\n",
        "print(f\"\\nFinish reason:\\n{response.candidates[0].finish_reason}\")\n",
        "print(f\"\\nSafety settings:\\n{response.candidates[0].safety_ratings}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "intro_gemini_1_5_flash.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
