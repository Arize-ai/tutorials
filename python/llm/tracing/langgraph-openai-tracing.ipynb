{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "    <p style=\"text-align:center\">\n",
        "    <img alt=\"arize logo\" src=\"https://storage.googleapis.com/arize-assets/arize-logo-white.jpg\" width=\"300\"/>\n",
        "        <br>\n",
        "        <a href=\"https://docs.arize.com/arize/\">Docs</a>\n",
        "        |\n",
        "        <a href=\"https://github.com/Arize-ai/client_python\">GitHub</a>\n",
        "        |\n",
        "        <a href=\"https://join.slack.com/t/arize-ai/shared_invite/zt-1px8dcmlf-fmThhDFD_V_48oU7ALan4Q\">Community</a>\n",
        "    </p>\n",
        "</center>\n",
        "\n",
        "<center><h1>Using Arize with LangGraph</h1></center>\n",
        "\n",
        "This tutorial sets up langgraph nodes to answer weather questions with tools, and instruments it with Arize and OpenAI."
      ],
      "metadata": {
        "id": "rIR7LDya93iy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M98jm6Gea_0j"
      },
      "outputs": [],
      "source": [
        "!pip install -qq -U --upgrade langgraph langchain_openai langchain_community langchain-anthropic openinference-instrumentation-langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -qq \"arize-phoenix[evals]\" arize-otel openai openinference-instrumentation-openai opentelemetry-sdk opentelemetry-exporter-otlp"
      ],
      "metadata": {
        "id": "4Q9j20rbk7_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Your Arize SPACE ID and Key\n",
        "\n",
        "# Import open-telemetry dependencies\n",
        "from arize_otel import register_otel, Endpoints\n",
        "\n",
        "# Setup OTEL via our convenience function\n",
        "register_otel(\n",
        "    endpoints = Endpoints.ARIZE,\n",
        "    space_id = \"ARIZE_SPACE_ID\", # in app space settings page\n",
        "    api_key = \"ARIZE_API_KEY\", # in app space settings page\n",
        "    model_id = \"my_test_agent_langgraph\", # name this to whatever you would like\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8CuW0i9lIhU",
        "outputId": "2b0d1cd4-d31f-4f9f-b7b2-fc1f488ecc34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:opentelemetry.trace:Overriding of current TracerProvider is not allowed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install --upgrade tenacity\n"
      ],
      "metadata": {
        "id": "-kgI2Cg7lu2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated, Literal, TypedDict\n",
        "\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import END, StateGraph, MessagesState\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "\n",
        "# Import the automatic instrumentor from OpenInference\n",
        "from openinference.instrumentation.langchain import LangChainInstrumentor\n",
        "\n",
        "# Finish automatic instrumentation\n",
        "LangChainInstrumentor().instrument()"
      ],
      "metadata": {
        "id": "5nO7AJtIb11X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7349c152-569b-4dc8-d766-245d84765279"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:opentelemetry.instrumentation.instrumentor:Attempting to instrument while already instrumented\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass(\"OpenAI API key\")"
      ],
      "metadata": {
        "id": "9jmp3IHKdGnQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7a807a2-f247-410f-eb9a-0e492ae66fd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API key··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the tools for the agent to use\n",
        "@tool\n",
        "def search(query: str):\n",
        "    \"\"\"\n",
        "    Search for a given query and return results.\n",
        "    \"\"\"\n",
        "    # This is a placeholder, but don't tell the LLM that...\n",
        "    if \"sf\" in query.lower() or \"san francisco\" in query.lower():\n",
        "        return \"It's 60 degrees and foggy.\"\n",
        "    return \"It's 90 degrees and sunny.\"\n",
        "\n",
        "\n",
        "tools = [search]\n",
        "tool_node = ToolNode(tools)\n",
        "\n",
        "model = ChatOpenAI(temperature=0).bind_tools(tools)\n",
        "\n",
        "# Define the function that determines whether to continue or not\n",
        "def should_continue(state: MessagesState) -> Literal[\"tools\", END]:\n",
        "    messages = state['messages']\n",
        "    last_message = messages[-1]\n",
        "    # If the LLM makes a tool call, then we route to the \"tools\" node\n",
        "    if last_message.tool_calls:\n",
        "        return \"tools\"\n",
        "    # Otherwise, we stop (reply to the user)\n",
        "    return END\n",
        "\n",
        "\n",
        "# Define the function that calls the model\n",
        "def call_model(state: MessagesState):\n",
        "    messages = state['messages']\n",
        "    response = model.invoke(messages)\n",
        "    # We return a list, because this will get added to the existing list\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "\n",
        "# Define a new graph\n",
        "workflow = StateGraph(MessagesState)\n",
        "\n",
        "# Define the two nodes we will cycle between\n",
        "workflow.add_node(\"agent\", call_model)\n",
        "workflow.add_node(\"tools\", tool_node)\n",
        "\n",
        "# Set the entrypoint as `agent`\n",
        "# This means that this node is the first one called\n",
        "workflow.set_entry_point(\"agent\")\n",
        "\n",
        "# We now add a conditional edge\n",
        "workflow.add_conditional_edges(\n",
        "    # First, we define the start node. We use `agent`.\n",
        "    # This means these are the edges taken after the `agent` node is called.\n",
        "    \"agent\",\n",
        "    # Next, we pass in the function that will determine which node is called next.\n",
        "    should_continue,\n",
        ")\n",
        "\n",
        "# We now add a normal edge from `tools` to `agent`.\n",
        "# This means that after `tools` is called, `agent` node is called next.\n",
        "workflow.add_edge(\"tools\", 'agent')\n",
        "\n",
        "# Initialize memory to persist state between graph runs\n",
        "checkpointer = MemorySaver()\n",
        "\n",
        "# Finally, we compile it!\n",
        "# This compiles it into a LangChain Runnable,\n",
        "# meaning you can use it as you would any other runnable.\n",
        "# Note that we're (optionally) passing the memory when compiling the graph\n",
        "app = workflow.compile(checkpointer=checkpointer)\n",
        "\n",
        "# Use the Runnable\n",
        "final_state = app.invoke(\n",
        "    {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]},\n",
        "    config={\"configurable\": {\"thread_id\": 42}}\n",
        ")\n",
        "final_state[\"messages\"][-1].content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0Y_n0878cSRE",
        "outputId": "17a378a6-d3d0-4955-816b-68485c49ed96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The weather in San Francisco is currently 60 degrees and foggy.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_state = app.invoke(\n",
        "    {\"messages\": [HumanMessage(content=\"what about ny\")]},\n",
        "    config={\"configurable\": {\"thread_id\": 42}}\n",
        ")\n",
        "final_state[\"messages\"][-1].content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HG2SStKTdfZU",
        "outputId": "f5045711-92d9-4154-cbec-c5ae0ee2b867"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The weather in New York is currently 90 degrees and sunny.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ]
}