{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQdiI3thB3LL"
      },
      "source": [
        "<center><img src=\"https://storage.googleapis.com/arize-assets/fixtures/Arize-Phoenix-header.jpg\" width=\"2000\"/></center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKfgxgxPB3LP"
      },
      "source": [
        "<center>\n",
        "    <h2>LLM Application Tracing & Evaluation Workflows</h2>\n",
        "    <h3>Exporting from Phoenix to Arize<br></h3>\n",
        "</center>\n",
        "\n",
        "\n",
        "This guide demonstrates how to use Arize for monitoring and debugging your LLM using Traces and Spans. We're going to use data from a chatbot built on top of Arize docs (https://docs.arize.com/arize/), with example query and retrieved text. Let's figure out how to understand how well our RAG system is working.\n",
        "\n",
        "In this tutorial we will:\n",
        "1. Build a RAG application using Llama-Index\n",
        "1. Set up [Phoenix](https://docs.arize.com/phoenix) as a [trace collector](https://docs.arize.com/phoenix/tracing/llm-traces) for the Llama-Index application\n",
        "2. Use Phoenix's [evals library](https://docs.arize.com/phoenix/evaluation/llm-evals) to compute LLM generated evaluations of our RAG app responses\n",
        "3. Use arize SDK to export the traces and evaluations to Arize\n",
        "\n",
        "You can read more about LLM tracing in Arize [here](https://docs.arize.com/arize/llm-large-language-models/llm-traces)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkQIvkUgB3LP"
      },
      "source": [
        "## Step 1: Install Dependencies 📚\n",
        "Let's get the notebook setup with dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjHekZqvB3LP",
        "outputId": "3758cfe0-1af2-4921-f948-ce93b1bcb118",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m1.3/1.6 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m306.0/306.0 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.6/316.6 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.1/225.1 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.2/168.2 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.2/203.2 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.28.3 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.28.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Dependencies needed to build the Llama Index RAG application\n",
        "!pip install -qq gcsfs llama-index-llms-openai llama-index-embeddings-openai\n",
        "\n",
        "# Dependencies needed to export spans and send them to our collector: Phoenix\n",
        "!pip install -qq \"llama-index-callbacks-arize-phoenix>=0.1.2\"\n",
        "\n",
        "# Install Phoenix to generate evaluations\n",
        "!pip install -qq \"arize-phoenix[evals]\"\n",
        "\n",
        "# Install Arize SDK with `Tracing` extra dependencies to export Phoenix data to Arize\n",
        "!pip install -qq \"arize[Tracing]>=7.12.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5GBfItTB3LP"
      },
      "source": [
        "## Step 2: Set up Phoenix as a Trace Collector in our LLM app\n",
        "\n",
        "To get started, launch the phoenix app. Make sure to open the app in your browser using the link below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKmwmWv_B3LP"
      },
      "outputs": [],
      "source": [
        "import phoenix as px\n",
        "session = px.launch_app()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSYMm88hB3LQ"
      },
      "source": [
        "Once you have started a Phoenix server, you can start your LlamaIndex application and configure it to send traces to Phoenix. To do this, you will have to add configure Phoenix as the global handler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2VJo9_tB3LQ"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import set_global_handler\n",
        "set_global_handler(\"arize_phoenix\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59_8aJQUB3LQ"
      },
      "source": [
        "That's it! The Llama-Index application we build next will send traces to Phoenix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwo92bqkB3LQ"
      },
      "source": [
        "## Step 3: Build Your Llama Index RAG Application 📁\n",
        "\n",
        "We start by setting your OpenAI API key if it is not already set as an environment variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4Vz17aGB3LQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
        "    openai_api_key = getpass(\"🔑 Enter your OpenAI API key: \")\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIc9ziZJB3LQ"
      },
      "source": [
        "This example uses a `RetrieverQueryEngine` over a pre-built index of the Arize documentation, but you can use whatever LlamaIndex application you like. Download the pre-built index of the Arize docs from cloud storage and instantiate your storage context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ep1n6XMWB3LQ"
      },
      "outputs": [],
      "source": [
        "from gcsfs import GCSFileSystem\n",
        "from llama_index.core import StorageContext\n",
        "\n",
        "file_system = GCSFileSystem(project=\"public-assets-275721\")\n",
        "index_path = \"arize-phoenix-assets/datasets/unstructured/llm/llama-index/arize-docs/index/\"\n",
        "storage_context = StorageContext.from_defaults(\n",
        "    fs=file_system,\n",
        "    persist_dir=index_path,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8Nx_rikB3LQ"
      },
      "source": [
        "We are now ready to instantiate our query engine that will perform retrieval-augmented generation (RAG). Query engine is a generic interface in LlamaIndex that allows you to ask question over your data. A query engine takes in a natural language query, and returns a rich response. It is built on top of Retrievers. You can compose multiple query engines to achieve more advanced capability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJHpbHqwB3LQ"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.core import (\n",
        "    Settings,\n",
        "    load_index_from_storage,\n",
        ")\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "\n",
        "Settings.llm = OpenAI(model=\"gpt-4-turbo-preview\")\n",
        "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")\n",
        "index = load_index_from_storage(\n",
        "    storage_context,\n",
        ")\n",
        "query_engine = index.as_query_engine()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vH_SDJ4B3LQ"
      },
      "source": [
        "Let's test our app by asking a question about the Arize documentation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Yjz9TV5B3LQ"
      },
      "outputs": [],
      "source": [
        "response = query_engine.query(\"What is Arize and how can it help me as an AI Engineer?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CohwvarYB3LQ"
      },
      "source": [
        "Great! Our application works!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1j2NjbqUB3LQ"
      },
      "source": [
        "## Step 4: Use the instrumented Query Engine\n",
        "\n",
        "We will download a dataset of questions for our RAG application to answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IixVa8RB3LQ"
      },
      "outputs": [],
      "source": [
        "from urllib.request import urlopen\n",
        "import json\n",
        "\n",
        "queries_url = \"http://storage.googleapis.com/arize-phoenix-assets/datasets/unstructured/llm/context-retrieval/arize_docs_queries.jsonl\"\n",
        "queries = []\n",
        "with urlopen(queries_url) as response:\n",
        "    for line in response:\n",
        "        line = line.decode(\"utf-8\").strip()\n",
        "        data = json.loads(line)\n",
        "        queries.append(data[\"query\"])\n",
        "\n",
        "queries[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oh6YIp9eB3LQ"
      },
      "source": [
        "We use the instrumented query engine and get responses from our RAG app."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_CCcJlNB3LQ"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "N = 10 # Sample size\n",
        "qa_pairs = []\n",
        "for query in tqdm(queries[:N]):\n",
        "    resp = query_engine.query(query)\n",
        "    qa_pairs.append((query,resp))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8q1BBsFB3LR"
      },
      "source": [
        "To see the questions and answers in phoenix, use the link described when we started the phoenix server"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5owQlp1B3LR"
      },
      "source": [
        "## Step 5: Run Evaluations on the data in Phoenix\n",
        "\n",
        "We will use the phoenix client to extract data in the correct format for specific evaluations and the custom evaluators, also from phoenix, to run evaluations on our RAG application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ln8KiABB3LR"
      },
      "outputs": [],
      "source": [
        "from phoenix.session.evaluation import get_qa_with_reference\n",
        "\n",
        "px_client = px.Client()  # Define phoenix client\n",
        "queries_df = get_qa_with_reference(px_client)   # Get question, answer and reference data from phoenix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plPJlB4hB3LR"
      },
      "source": [
        "Next, we enable concurrent evaluations for better performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGYIM9VRB3LR"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()  # needed for concurrent evals in notebook environments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoMvYN5DB3LR"
      },
      "source": [
        "Then, we define our evaluators and run the evaluations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Qsht5t3B3LR"
      },
      "outputs": [],
      "source": [
        "from phoenix.evals import (\n",
        "    HallucinationEvaluator,\n",
        "    OpenAIModel,\n",
        "    QAEvaluator,\n",
        "    run_evals,\n",
        ")\n",
        "\n",
        "eval_model = OpenAIModel(\n",
        "    model=\"gpt-4-turbo-preview\",\n",
        ")\n",
        "hallucination_evaluator = HallucinationEvaluator(eval_model)\n",
        "qa_correctness_evaluator = QAEvaluator(eval_model)\n",
        "\n",
        "hallucination_eval_df, qa_correctness_eval_df = run_evals(\n",
        "    dataframe=queries_df,\n",
        "    evaluators=[hallucination_evaluator, qa_correctness_evaluator],\n",
        "    provide_explanation=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1vyN0vRB3LR"
      },
      "source": [
        "Finally, we log the evaluations into Phoenix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhwK3wSWB3LR"
      },
      "outputs": [],
      "source": [
        "from phoenix.trace import SpanEvaluations\n",
        "\n",
        "px_client.log_evaluations(\n",
        "    SpanEvaluations(eval_name=\"Hallucination\", dataframe=hallucination_eval_df),\n",
        "    SpanEvaluations(eval_name=\"QA_Correctness\", dataframe=qa_correctness_eval_df),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-7Bzk-iB3LR"
      },
      "source": [
        "## Step 6: Export data to Arize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XyQrXQ-B3LR"
      },
      "source": [
        "### Step 6.a: Get data into dataframes\n",
        "\n",
        "We extract the spans and evals dataframes from the phoenix client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kT_YDr4CB3LR"
      },
      "outputs": [],
      "source": [
        "tds = px_client.get_trace_dataset()\n",
        "spans_df = tds.get_spans_dataframe(include_evaluations=False)\n",
        "spans_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efm9fIAXB3LR"
      },
      "outputs": [],
      "source": [
        "evals_df = tds.get_evals_dataframe()\n",
        "evals_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r97WxyjUB3LR"
      },
      "source": [
        "### Step 6.b: Initialize arize client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrxUK1AvB3LR"
      },
      "outputs": [],
      "source": [
        "from arize.pandas.logger import Client"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rnqa6OOsB3LR"
      },
      "source": [
        "Sign up/ log in to your Arize account [here](https://app.arize.com/auth/login). Find your [space ID and API key](https://docs.arize.com/arize/api-reference/arize.pandas/client). Copy/paste into the cell below.\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/arize-assets/fixtures/copy-id-and-key.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFHwpWIYB3LR"
      },
      "outputs": [],
      "source": [
        "SPACE_ID = \"SPACE_ID\" #Change this line\n",
        "API_KEY = \"API_KEY\" #Change this line\n",
        "\n",
        "if SPACE_ID == \"SPACE_ID\" or API_KEY == \"API_KEY\":\n",
        "    raise ValueError(\"❌ CHANGE SPACE_ID AND/OR API_KEY\")\n",
        "else:\n",
        "    print(\"✅ Import and Setup Arize Client Done! Now we can start using Arize!\")\n",
        "\n",
        "arize_client = Client(space_id=SPACE_ID, api_key=API_KEY,)\n",
        "model_id = \"tutorial-tracing-llama-index-rag-export-from-phoenix\"\n",
        "model_version = \"1.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DdZvVggB3LS"
      },
      "source": [
        "Lastly, we use `log_spans` from the arize client to log our spans data and, if we have evaluations, we can pass the optional `evals_dataframe`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrApnOpeB3LS"
      },
      "outputs": [],
      "source": [
        "response = arize_client.log_spans(\n",
        "    dataframe=spans_df,\n",
        "    evals_dataframe=evals_df,\n",
        "    model_id=model_id,\n",
        "    model_version=model_version,\n",
        ")\n",
        "\n",
        "# If successful, the server will return a status_code of 200\n",
        "if response.status_code != 200:\n",
        "    print(f\"❌ logging failed with response code {response.status_code}, {response.text}\")\n",
        "else:\n",
        "    print(f\"✅ You have successfully logged traces set to Arize\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}