{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee2b3238",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e541c8c8",
   "metadata": {},
   "source": [
    "<center>\n",
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "    <img alt=\"arize logo\" src=\"https://storage.googleapis.com/arize-assets/arize-logo-white.jpg\" width=\"300\"/>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/arize/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/client_python\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://arize-ai.slack.com/join/shared_invite/zt-11t1vbu4x-xkBIHmOREQnYnYDH1GDfCg\">Slack Community</a>\n",
    "    </p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2c8e6f",
   "metadata": {},
   "source": [
    "# Trace-Level Evals for a Movie Recommendation Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68d55ab",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to run trace-level evaluations for a movie recommendation agent. By analyzing individual traces, each representing a single user request, you can gain insights into how well the system is performing on a per-interaction basis. Trace-level evaluations are particularly valuable for identifying successes and failures for end-to-end performance.\n",
    "\n",
    "In this notebook, you will:\n",
    "- Build and capture interactions (traces) from your movie recommendation agent\n",
    "- Evaluate each trace across key dimensions such as Recommendation Relevance and Tool Usage\n",
    "- Format the evaluation outputs to match Arizeâ€™s schema and log them to the platform\n",
    "- Learn a robust pipeline for assessing trace-level performance\n",
    "\n",
    "âœ… You will need a free Arize AX account and an OpenAI API key to run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549464f8",
   "metadata": {},
   "source": [
    "# Set Up Keys & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b425a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qqqqq openinference-instrumentation-openai openai openinference-instrumentation-openai-agents arize-otel openai-agents arize-phoenix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15652859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"ARIZE_SPACE_ID\"] = globals().get(\"ARIZE_SPACE_ID\") or getpass(\"ðŸ”‘ Enter your Arize Space ID: \")\n",
    "\n",
    "os.environ[\"ARIZE_API_KEY\"] = globals().get(\"ARIZE_API_KEY\") or getpass(\"ðŸ”‘ Enter your Arize API Key: \")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = globals().get(\"OPENAI_API_KEY\") or getpass(\"ðŸ”‘ Enter your OpenAI API Key: \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47e0638",
   "metadata": {},
   "source": [
    "# Configure Tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884b706e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arize.otel import register\n",
    "\n",
    "model_id = \"movie-recommendation-agent\"\n",
    "tracer_provider = register(\n",
    "    space_id=os.getenv(\"ARIZE_SPACE_ID\"),\n",
    "    api_key=os.getenv(\"ARIZE_API_KEY\"),\n",
    "    project_name=model_id,\n",
    "    set_global_tracer_provider=True \n",
    ")\n",
    "\n",
    "from openinference.instrumentation.openai_agents import OpenAIAgentsInstrumentor\n",
    "from openinference.instrumentation.openai import OpenAIInstrumentor\n",
    "\n",
    "OpenAIAgentsInstrumentor().instrument(tracer_provider=tracer_provider)\n",
    "OpenAIInstrumentor().instrument(tracer_provider=tracer_provider)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3919bd",
   "metadata": {},
   "source": [
    "# Build Movie Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c81c9bb",
   "metadata": {},
   "source": [
    "First, we need to define the tools that our recommendation system will use. For this example, we will define 3 tools:\n",
    "1. Movie Selector: Based on the desired genre indicated by the user, choose up to 5 recent movies availabtle for streaming\n",
    "2. Reviewer: Find reviews for a movie. If given a list of movies, sort movies in order of highest to lowest ratings. \n",
    "3. Preview Summarizer: For each movie, return a 1-2 sentence description \n",
    "\n",
    "Our most ideal flow involves a user simply giving the system a type of movie they are looking for, and in return, the user gets a list of options returned with descriptions and reviews. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64844a7",
   "metadata": {},
   "source": [
    "Let's test our agent & view traces in Arize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1731db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, Runner, function_tool\n",
    "from typing import List, Union\n",
    "from openai import OpenAI\n",
    "import ast\n",
    "from opentelemetry import trace\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "@function_tool\n",
    "def movie_selector_llm(genre: str) -> List[str]:\n",
    "    prompt = (\n",
    "        f\"List up to 5 recent popular streaming movies in the {genre} genre. \"\n",
    "        \"Provide only movie titles as a Python list of strings.\"\n",
    "    )\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.7,\n",
    "        max_tokens=150,\n",
    "    )\n",
    "    content = response.choices[0].message.content\n",
    "    try:\n",
    "        movie_list = ast.literal_eval(content)\n",
    "        if isinstance(movie_list, list):\n",
    "            return movie_list[:5]\n",
    "    except Exception:\n",
    "        return content.split('\\n')\n",
    "\n",
    "@function_tool\n",
    "def reviewer_llm(movies: Union[str, List[str]]) -> str:\n",
    "    if isinstance(movies, list):\n",
    "        movies_str = \", \".join(movies)\n",
    "        prompt = f\"Sort the following movies by rating from highest to lowest and provide a short review for each:\\n{movies_str}\"\n",
    "    else:\n",
    "        prompt = f\"Provide a short review and rating for the movie: {movies}\"\n",
    "    response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.7,\n",
    "            max_tokens=300,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "@function_tool\n",
    "def preview_summarizer_llm(movie: str) -> str:\n",
    "    prompt = f\"Write a 1-2 sentence summary describing the movie '{movie}'.\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.7,\n",
    "        max_tokens=100,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee91369",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    name=\"MovieRecommendationAgentLLM\",\n",
    "    tools=[movie_selector_llm, reviewer_llm, preview_summarizer_llm],\n",
    "    instructions=(\n",
    "        \"You are a helpful movie recommendation assistant with access to three tools:\\n\"\n",
    "        \"1. MovieSelector: Given a genre, returns up to 5 recent streaming movies.\\n\"\n",
    "        \"2. Reviewer: Given one or more movie titles, returns reviews and sorts them by rating.\\n\"\n",
    "        \"3. PreviewSummarizer: Given a movie title, returns a 1-2 sentence summary.\\n\\n\"\n",
    "        \"Your goal is to provide a helpful, user-friendly response combining relevant information.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "import asyncio\n",
    "\n",
    "async def main():\n",
    "    user_input = \"Which comedy movie should I watch?\"\n",
    "    result = await Runner.run(agent, user_input)\n",
    "    print(result.final_output)\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a0ffa2",
   "metadata": {},
   "source": [
    "![Results](https://storage.googleapis.com/arize-phoenix-assets/assets/images/trace-level-evals-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bef5b97",
   "metadata": {},
   "source": [
    "Next, weâ€™ll run the agent a few more times to generate additional traces. Feel free to adapt or customize the questions as you see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf62bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"Which Batman movie should I watch?\",\n",
    "    \"I want to watch a good romcom\",\n",
    "    \"What is a very scary horror movie?\",\n",
    "    \"Name a feel-good holiday movie\",\n",
    "    \"Recommend a musical with great songs\",\n",
    "    \"Give me a classic drama from the 90s\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    result = await Runner.run(agent, question)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f819c8ee",
   "metadata": {},
   "source": [
    "# Get Span Data from Arize "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de0363e",
   "metadata": {},
   "source": [
    "Before running our evaluations, we first retrieve the span data from Arize. We then group the spans by trace and separate the input and output values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99165d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arize.exporter import ArizeExportClient\n",
    "from arize.utils.types import Environments\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "client = ArizeExportClient(api_key=os.environ[\"ARIZE_API_KEY\"])\n",
    "\n",
    "primary_df = client.export_model_to_df(\n",
    "    space_id=os.environ[\"ARIZE_SPACE_ID\"],\n",
    "    model_id=model_id,\n",
    "    environment=Environments.TRACING,\n",
    "    start_time=datetime.now(timezone.utc) - timedelta(days=7),\n",
    "    end_time=datetime.now(timezone.utc),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b78eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "trace_df = (\n",
    "    primary_df.groupby(\"context.trace_id\")\n",
    "      .agg({\n",
    "          \"attributes.input.value\": \"first\",\n",
    "          \"attributes.output.value\": lambda x: \" \".join(x.dropna()),\n",
    "      })\n",
    ")\n",
    "\n",
    "trace_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d014913f",
   "metadata": {},
   "source": [
    "# Define and Run Evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a426e4ca",
   "metadata": {},
   "source": [
    "In this tutorial, we will evaluate two aspects: tool usage and relevance. You can add any additional evaluation templates you like. We will then run the evaluations using an LLM as the judge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f1768d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOOL_CALLING_ORDER = \"\"\"\n",
    "You are evaluating the correctness of the tool calling order in an LLM application's trace.\n",
    "\n",
    "You will be given:\n",
    "1. The user input that initiated the trace\n",
    "2. The full trace output, including the sequence of tool calls made by the agent \n",
    "\n",
    "##\n",
    "User Input:\n",
    "{attributes.input.value}\n",
    "\n",
    "Trace Output:\n",
    "{attributes.output.value}\n",
    "##\n",
    "\n",
    "Respond with exactly one word: `correct` or `incorrect`.\n",
    "1. `correct` â†’ \n",
    "- The tool calls occur in the appropriate order to fulfill the user's request logically and effectively. \n",
    "- A proper answer involves calls to reviews, summaries, and recommendations where relevant.\n",
    "2. `incorrect` â†’ The tool calls are out of order, missing, or do not follow a coherent sequence for the given input.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131a5e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RECOMMENDATION_RELEVANCE = \"\"\"\n",
    "You are evaluating the relevance of movie recommendations provided by an LLM application.\n",
    "\n",
    "You will be given:\n",
    "1. The user input that initiated the trace\n",
    "2. The list of movie recommendations output by the system\n",
    "\n",
    "##\n",
    "User Input:\n",
    "{attributes.input.value}\n",
    "\n",
    "Recommendations:\n",
    "{attributes.output.value}\n",
    "##\n",
    "\n",
    "Respond with exactly one word: `correct` or `incorrect`.\n",
    "1. `correct` â†’ \n",
    "- All recommended movies match the requested genre or criteria in the user input. \n",
    "- The recommendations should be relevant to the user's request and shouldn't be repetitive.\n",
    "- `incorrect` â†’ one or more recommendations do not match the requested genre or criteria.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8d7f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.evals import llm_classify, OpenAIModel\n",
    "import nest_asyncio, os\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "model = OpenAIModel(\n",
    "    api_key = os.environ[\"OPENAI_API_KEY\"],\n",
    "    model   = \"gpt-4o-mini\",\n",
    "    temperature = 0.0,\n",
    ")\n",
    "\n",
    "rails = [\"correct\", \"incorrect\"]\n",
    "\n",
    "tool_eval_results = llm_classify(\n",
    "    dataframe           = trace_df,\n",
    "    template            = TOOL_CALLING_ORDER,\n",
    "    model               = model,\n",
    "    rails               = rails,\n",
    "    provide_explanation = True,   \n",
    "    verbose             = False,\n",
    ")\n",
    "\n",
    "tool_eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd9620e",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_eval_results = llm_classify(\n",
    "    dataframe           = trace_df,\n",
    "    template            = RECOMMENDATION_RELEVANCE,\n",
    "    model               = model,\n",
    "    rails               = rails,\n",
    "    provide_explanation = True,   \n",
    "    verbose             = False,\n",
    ")\n",
    "\n",
    "relevance_eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4613e975",
   "metadata": {},
   "source": [
    "# Log Results Back to Arize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd4401d",
   "metadata": {},
   "source": [
    "The final step is to log our results back to Arize. After running the cell below, youâ€™ll be able to view your trace-level evaluations on the platform, complete with relevant labels, scores, and explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec87930",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arize.pandas.logger import Client\n",
    "\n",
    "tool_eval_results = tool_eval_results.rename(columns={\n",
    "    \"label\": \"ToolEvaluation.label\",\n",
    "    \"explanation\": \"ToolEvaluation.explanation\",\n",
    "})[[\"ToolEvaluation.label\", \"ToolEvaluation.explanation\"]]\n",
    "\n",
    "relevance_eval_results = relevance_eval_results.rename(columns={\n",
    "    \"label\": \"RecommendationRelevance.label\",\n",
    "    \"explanation\": \"RecommendationRelevance.explanation\",\n",
    "})[[\"RecommendationRelevance.label\", \"RecommendationRelevance.explanation\"]]\n",
    "\n",
    "combined_eval_results = tool_eval_results \\\n",
    "    .join(relevance_eval_results, how=\"outer\")\n",
    "\n",
    "merged_df = pd.merge(trace_df, combined_eval_results, left_index=True, right_index=True)\n",
    "merged_df.rename(\n",
    "    columns={\n",
    "        \"ToolEvaluation.label\": \"trace_eval.ToolEvaluation.label\",\n",
    "        \"ToolEvaluation.explanation\": \"trace_eval.ToolEvaluation.explanation\",\n",
    "        \"RecommendationRelevance.label\": \"trace_eval.RecommendationRelevance.label\",\n",
    "        \"RecommendationRelevance.explanation\": \"trace_eval.RecommendationRelevance.explanation\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "root_spans = primary_df[primary_df[\"parent_id\"].isna()][[\"context.trace_id\", \"context.span_id\"]]\n",
    "log_df = trace_df.merge(merged_df, left_on=\"context.trace_id\", right_index=True)\n",
    "log_df = merged_df.merge(root_spans, on=\"context.trace_id\", how=\"left\")\n",
    "\n",
    "\n",
    "arize_client = Client(\n",
    "    space_id = os.environ[\"ARIZE_SPACE_ID\"],\n",
    "    api_key  = os.environ[\"ARIZE_API_KEY\"],\n",
    ")\n",
    "resp = arize_client.log_evaluations_sync(\n",
    "    dataframe = log_df,\n",
    "    model_id  = model_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e36661",
   "metadata": {},
   "source": [
    "![Trace Evals in Arize](https://storage.googleapis.com/arize-phoenix-assets/assets/images/trace-level-evals-2.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
