{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "    <img alt=\"arize logo\" src=\"https://storage.googleapis.com/arize-assets/arize-logo-white.jpg\" width=\"300\"/>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/arize/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/client_python\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://arize-ai.slack.com/join/shared_invite/zt-11t1vbu4x-xkBIHmOREQnYnYDH1GDfCg\">Slack Community</a>\n",
    "    </p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session Level Evals for an AI Tutor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial demonstrates how to run session-level evaluations on conversations with an AI tutor. You'll log the results back to Arize AX for further monitoring and analysis. Session-level evaluations are valuable because they provide a holistic view of the entire interaction, enabling you to assess broader patterns and answer high-level questions about user experience and system performance.\n",
    "\n",
    "In this tutorial, you will:\n",
    "- Trace and aggregate multi-turn interactions into structured sessions\n",
    "- Evaluate sessions across multiple dimensions such as Correctness, Goal Completion, and Frustration\n",
    "- Format the evaluation outputs to match Arize's schema and log them to the platform\n",
    "\n",
    "By the end, youâ€™ll have a robust evaluation pipeline for analyzing and comparing session-level performance.\n",
    "\n",
    "âœ… Youâ€™ll need a free Arize AX account and an Anthropic API key to run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up Dependencies & Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install arize openinference-instrumentation-anthropic openinference-instrumentation arize-otel arize-phoenix nest_asyncio anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"SPACE_ID\"] = globals().get(\"SPACE_ID\") or getpass(\"ðŸ”‘ Enter your Arize Space ID: \")\n",
    "\n",
    "os.environ[\"API_KEY\"] = globals().get(\"API_KEY\") or getpass(\"ðŸ”‘ Enter your Arize API Key: \")\n",
    "\n",
    "ANTHROPIC_API_KEY = globals().get(\"ANTHROPIC_API_KEY\") or getpass(\"ðŸ”‘ Enter your Anthropic API Key: \")\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = ANTHROPIC_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arize.otel import register\n",
    "from openinference.instrumentation.anthropic import AnthropicInstrumentor\n",
    "\n",
    "model_id = \"my-ai-tutor\"\n",
    "tracer_provider = register(\n",
    "    space_id=os.environ[\"SPACE_ID\"],\n",
    "    api_key=os.environ[\"API_KEY\"],\n",
    "    project_name=model_id,\n",
    ")\n",
    "\n",
    "AnthropicInstrumentor().instrument(tracer_provider=tracer_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and Run AI Tutor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we demonstrate how to evaluate AI tutor sessions. The tutor begins by receiving a user ID, topic, and question. It then explains the topic to the student and engages them with follow-up questions in a multi-turn conversation, continuing until the student ends the session. Our goal is to assess the overall quality of this interaction from start to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import anthropic\n",
    "from openinference.instrumentation import using_attributes\n",
    "\n",
    "client = anthropic.Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "def run_session(user_id: str, topic: str, question: str):\n",
    "    session_id = f\"tutor-{uuid.uuid4()}\"\n",
    "    chat = [\n",
    "        {\"role\": \"system\", \"content\": (\n",
    "            f\"You are a thoughtful AI tutor teaching {topic}. \"\n",
    "            \"Ask questions, give hints, and only suggest full answers \"\n",
    "            \"when student shows correct reasoning.\"\n",
    "        )},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    "\n",
    "    while True:\n",
    "      with using_attributes(session_id=session_id, user_id=user_id):\n",
    "          messages = []\n",
    "          for msg in chat:\n",
    "              if msg[\"role\"] == \"system\":\n",
    "                  if not messages:\n",
    "                      messages.append({\"role\": \"user\", \"content\": msg[\"content\"]})\n",
    "              else:\n",
    "                  messages.append(msg)\n",
    "          \n",
    "          resp = client.messages.create(\n",
    "              model=\"claude-3-5-sonnet-20241022\",\n",
    "              messages=messages,\n",
    "              max_tokens=1000,\n",
    "              temperature=0.5,\n",
    "          )\n",
    "      assistant_msg = resp.content[0].text.strip()\n",
    "      assistant_msg += \"\\n\\n(You can type 'DONE' if you're finished.)\"\n",
    "\n",
    "      chat.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
    "      print(f\"Tutor: {assistant_msg}\")\n",
    "\n",
    "      student_input = input(\"> your answer: \")\n",
    "      if student_input.strip().upper() == \"DONE\":\n",
    "          print(\"âœ… Student is DONE â€” ending session.\")\n",
    "          break\n",
    "\n",
    "      chat.append({\"role\": \"user\", \"content\": student_input})\n",
    "    return session_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask any question to the AI tutor!\n",
    "run_session(\"Sanjana\", \"Math\", \"What is a parabola?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Spans for Session-Level Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These following cells prepare the data for session-level evaluation. We start by loading all spans into a DataFrame, then sort them chronologically and group them by session ID. You can also group the spans by user ID. \n",
    "\n",
    "Next, we separate user inputs from AI responses, and finally, store the structured results in a dataframe. We will use this dataframe to run our evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arize.exporter import ArizeExportClient\n",
    "from arize.utils.types import Environments\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "client = ArizeExportClient(api_key=os.environ[\"API_KEY\"])\n",
    "\n",
    "primary_df = client.export_model_to_df(\n",
    "    space_id=os.environ[\"SPACE_ID\"],\n",
    "    model_id=model_id,\n",
    "    environment=Environments.TRACING,\n",
    "    start_time=datetime.now(timezone.utc) - timedelta(days=7),\n",
    "    end_time=datetime.now(timezone.utc),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we group our spans together to make a session dataframe. We also include logic to truncate part of the sesssion messages if token limits are exceeded. This prevents context window issues for longer sessions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def truncate_text(text, max_chars, strategy=\"end\"):\n",
    "    \"\"\"Truncate text to max_chars using the specified strategy.\"\"\"\n",
    "    if not text or len(text) <= max_chars:\n",
    "        return text\n",
    "\n",
    "    if strategy == \"start\":\n",
    "        return \"...\" + text[-(max_chars - 3):]\n",
    "    elif strategy == \"middle\":\n",
    "        half = (max_chars - 3) // 2\n",
    "        return text[:half] + \"...\" + text[-half:]\n",
    "    else:  # \"end\"\n",
    "        return text[:max_chars - 3] + \"...\"\n",
    "\n",
    "def estimate_session_size(user_inputs, output_messages):\n",
    "    \"\"\"Estimate total character count of session content.\"\"\"\n",
    "    total_chars = sum(len(msg) for msg in user_inputs + output_messages if isinstance(msg, str))\n",
    "    return total_chars\n",
    "\n",
    "def prepare_sessions(\n",
    "    df: pd.DataFrame,\n",
    "    max_chars_per_value=5000,\n",
    "    max_chars_per_session=100000,\n",
    "    truncation_strategy=\"end\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Collapse spans into a single row per session with truncation support.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame of spans.\n",
    "        max_chars_per_value: Max chars per individual input/output value.\n",
    "        max_chars_per_session: Max chars allowed in an entire session.\n",
    "        truncation_strategy: 'start', 'end', or 'middle'.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with one row per session.\n",
    "    \"\"\"\n",
    "    sessions = []\n",
    "\n",
    "    # Sort and group\n",
    "    grouped = df.sort_values(\"start_time\").groupby(\"attributes.session.id\", as_index=False)\n",
    "\n",
    "    for session_id, group in grouped:\n",
    "        # Drop NA values and apply per-value truncation\n",
    "        user_inputs = [\n",
    "            truncate_text(msg, max_chars_per_value, truncation_strategy)\n",
    "            for msg in group[\"attributes.input.value\"].dropna().tolist()\n",
    "        ]\n",
    "        output_messages = [\n",
    "            truncate_text(msg, max_chars_per_value, truncation_strategy)\n",
    "            for msg in group[\"attributes.output.value\"].dropna().tolist()\n",
    "        ]\n",
    "\n",
    "        # Estimate total session size\n",
    "        total_chars = estimate_session_size(user_inputs, output_messages)\n",
    "\n",
    "        # Truncate session-level size if needed\n",
    "        if total_chars > max_chars_per_session:\n",
    "            print(f\"Session {session_id} exceeds {max_chars_per_session} chars. Truncating...\")\n",
    "\n",
    "            # Keep messages evenly from start and end (half-half)\n",
    "            def smart_truncate(msgs):\n",
    "                keep_half = len(msgs) // 2\n",
    "                return msgs[:keep_half // 2] + msgs[-(keep_half - keep_half // 2):]\n",
    "\n",
    "            user_inputs = smart_truncate(user_inputs)\n",
    "            output_messages = smart_truncate(output_messages)\n",
    "\n",
    "            # Optional: truncate remaining messages again more aggressively\n",
    "            total_chars = estimate_session_size(user_inputs, output_messages)\n",
    "            if total_chars > max_chars_per_session:\n",
    "                aggressive_limit = max_chars_per_value // 2\n",
    "                user_inputs = [truncate_text(m, aggressive_limit, truncation_strategy) for m in user_inputs]\n",
    "                output_messages = [truncate_text(m, aggressive_limit, truncation_strategy) for m in output_messages]\n",
    "\n",
    "        sessions.append({\n",
    "            \"session_id\": session_id,\n",
    "            \"user_inputs\": user_inputs,\n",
    "            \"output_messages\": output_messages,\n",
    "            \"trace_count\": group[\"context.trace_id\"].nunique(),\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(sessions)\n",
    "\n",
    "\n",
    "sessions_df = prepare_sessions(primary_df, truncation_strategy=\"middle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session Correctness Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to begin running our evals. Let's start with an eval that ensures the AI tutor is giving the student factual information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SESSION_CORRECTNESS_PROMPT = \"\"\"\n",
    "You are an expert tutor assistant evaluating the **correctness and educational quality** of an AI tutor's session with a student.\n",
    "\n",
    "A session consists of multiple traces (interactions) between a student and an AI tutor. I will provide you with:\n",
    "1. The student's messages (user inputs) in order.\n",
    "2. The AI tutor's responses (output messages) in order.\n",
    "\n",
    "An effective and correct tutoring session should:\n",
    "- Provide factually and conceptually accurate explanations\n",
    "- Correctly answer student questions\n",
    "- Clarify misunderstandings if they occur\n",
    "- Build upon previous context in a coherent way\n",
    "- Avoid hallucinations, vague responses, or incorrect reasoning\n",
    "\n",
    "##\n",
    "Student Inputs:\n",
    "{user_inputs}\n",
    "\n",
    "Tutor Outputs:\n",
    "{output_messages}\n",
    "##\n",
    "\n",
    "Based on the above, evaluate the session **only for correctness and educational soundness**.\n",
    "\n",
    "Respond with a single word: `correct` or `incorrect`.\n",
    "\n",
    "- Respond with `correct` if the AI tutor consistently provides accurate, clear, and educationally sound answers.\n",
    "- Respond with `incorrect` if the AI tutor gives factually wrong, misleading, or incoherent explanations at any point.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.evals import llm_classify, AnthropicModel\n",
    "import anthropic\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Configure your evaluation model using Claude 3.5 Sonnet\n",
    "model = AnthropicModel(\n",
    "    # api_key=os.environ[\"ANTHROPIC_API_KEY\"],\n",
    "    model=\"claude-3-7-sonnet-latest\",\n",
    ")\n",
    "\n",
    "# Run the evaluation\n",
    "rails = [\"correct\", \"incorrect\"]\n",
    "eval_results_correctness = llm_classify(\n",
    "    data=sessions_df,\n",
    "    template=SESSION_CORRECTNESS_PROMPT,\n",
    "    model=model,\n",
    "    rails=rails,\n",
    "    provide_explanation=True,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "eval_results_correctness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session Frustration Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This evaluation is used to make sure the student isn't getting frustrated with the tutor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SESSION_FRUSTRATION_PROMPT = \"\"\"\n",
    "You are an AI assistant evaluating whether a student became frustrated during a tutoring session with an AI tutor.\n",
    "\n",
    "A session consists of multiple traces (interactions) between a student and an AI tutor. You will be given:\n",
    "1. The student's messages (user inputs), in order.\n",
    "2. The AI tutor's messages (output messages), in order.\n",
    "\n",
    "Signs of student frustration may include:\n",
    "- Repeating or rephrasing the same question multiple times\n",
    "- Expressing confusion (\"I don't get it\", \"This doesn't make sense\", etc.)\n",
    "- Disagreeing with the tutor's responses\n",
    "- Asking for clarification frequently without resolution\n",
    "- Expressing annoyance, impatience, or disengagement\n",
    "- Abruptly ending the session\n",
    "\n",
    "##\n",
    "Student Inputs:\n",
    "{user_inputs}\n",
    "\n",
    "Tutor Outputs:\n",
    "{output_messages}\n",
    "##\n",
    "\n",
    "Based on the above, evaluate whether the student showed signs of frustration at any point in the session.\n",
    "\n",
    "Respond with a single word: `frustrated` or `not_frustrated`.\n",
    "\n",
    "- Respond with `frustrated` if there is evidence of confusion, dissatisfaction, or emotional frustration.\n",
    "- Respond with `not_frustrated` if the student appears to stay engaged and satisfied throughout.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the evaluation\n",
    "rails = [\"frustrated\", \"not_frustrated\"]\n",
    "eval_results_frustration = llm_classify(\n",
    "    data=sessions_df,\n",
    "    template=SESSION_FRUSTRATION_PROMPT,\n",
    "    model=model,\n",
    "    rails=rails,\n",
    "    provide_explanation=True,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "eval_results_frustration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session Goal Achievement Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we evaluate to ensure the tutor helped the student reach their learning goals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SESSION_GOAL_ACHIEVEMENT_PROMPT = \"\"\"\n",
    "You are an AI assistant evaluating whether the AI tutor successfully helped the student achieve their learning goals during a tutoring session.\n",
    "\n",
    "A session consists of multiple interactions between a student and an AI tutor. You will be given:\n",
    "1. The studentâ€™s messages (user inputs), in chronological order.\n",
    "2. The AI tutorâ€™s responses (output messages), in chronological order.\n",
    "\n",
    "To determine if the studentâ€™s goals were achieved, consider:\n",
    "- Whether the AI tutor addressed the studentâ€™s questions and requests directly\n",
    "- Whether the explanations provided resolved the studentâ€™s doubts or problems\n",
    "- Whether the studentâ€™s inputs indicate understanding or closure by the end\n",
    "- Whether the conversation logically progressed toward completing the studentâ€™s objectives\n",
    "\n",
    "##\n",
    "Student Inputs:\n",
    "{user_inputs}\n",
    "\n",
    "Tutor Outputs:\n",
    "{output_messages}\n",
    "##\n",
    "\n",
    "Evaluate the session and respond with a single word: `achieved` or `not_achieved`.\n",
    "\n",
    "- Respond with `achieved` if the tutoring session successfully met the studentâ€™s learning goals and resolved their questions.\n",
    "- Respond with `not_achieved` if the session left the studentâ€™s questions unanswered or goals unmet.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the evaluation\n",
    "rails = [\"achieved\", \"not_achieved\"]\n",
    "eval_results_goal_achievement = llm_classify(\n",
    "    data=sessions_df,\n",
    "    template=SESSION_GOAL_ACHIEVEMENT_PROMPT,\n",
    "    model=model,\n",
    "    rails=rails,\n",
    "    provide_explanation=True,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "eval_results_goal_achievement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log Evaluations Back to Arize AX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can log the evaluation results back to Arize AX. In the sessions, tab of your project, you will see the evaluation results populate for each session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from arize.pandas.logger import Client\n",
    "\n",
    "eval_results_correctness = eval_results_correctness.rename(columns={\n",
    "    \"label\": \"SessionCorrectness.label\",\n",
    "    \"explanation\": \"SessionCorrectness.explanation\",\n",
    "})[[\"SessionCorrectness.label\", \"SessionCorrectness.explanation\"]]\n",
    "\n",
    "eval_results_goal_achievement = eval_results_goal_achievement.rename(columns={\n",
    "    \"label\": \"GoalCompletion.label\",\n",
    "    \"explanation\": \"GoalCompletion.explanation\",\n",
    "})[[\"GoalCompletion.label\", \"GoalCompletion.explanation\"]]\n",
    "\n",
    "eval_results_frustration = eval_results_frustration.rename(columns={\n",
    "    \"label\": \"Frustration.label\",\n",
    "    \"explanation\": \"Frustration.explanation\",\n",
    "})[[\"Frustration.label\", \"Frustration.explanation\"]]\n",
    "#Combine all the evaluation results\n",
    "combined_eval_results = eval_results_correctness \\\n",
    "    .join(eval_results_goal_achievement, how=\"outer\") \\\n",
    "    .join(eval_results_frustration, how=\"outer\")\n",
    "\n",
    "# Merge evaluation results with session data\n",
    "merged_df = pd.merge(sessions_df, combined_eval_results, left_index=True, right_index=True)\n",
    "merged_df.rename(\n",
    "    columns={\n",
    "        \"SessionCorrectness.label\": \"session_eval.SessionCorrectness.label\",\n",
    "        \"SessionCorrectness.explanation\": \"session_eval.SessionCorrectness.explanation\",\n",
    "        \"GoalCompletion.label\": \"session_eval.GoalCompletion.label\",\n",
    "        \"GoalCompletion.explanation\": \"session_eval.GoalCompletion.explanation\",\n",
    "        \"Frustration.label\": \"session_eval.Frustration.label\",\n",
    "        \"Frustration.explanation\": \"session_eval.Frustration.explanation\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "# Get the root span for each session to log the evaluation against\n",
    "root_spans = (\n",
    "    primary_df.sort_values(\"start_time\")\n",
    "    .drop_duplicates(subset=[\"attributes.session.id\"], keep=\"first\")[\n",
    "        [\"attributes.session.id\", \"context.span_id\"]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Merge to get the root span_id for each session\n",
    "final_df = pd.merge(\n",
    "    merged_df,              # left\n",
    "    root_spans,             # right\n",
    "    left_on=\"session_id\",   # column in merged_df\n",
    "    right_on=\"attributes.session.id\",  # column in root_spans\n",
    "    how=\"left\",\n",
    ")\n",
    "final_df = final_df.set_index(\"context.span_id\", drop=False)\n",
    "\n",
    "# Log evaluations back to Arize\n",
    "arize_client = Client(space_id=os.environ[\"SPACE_ID\"], api_key=os.environ[\"API_KEY\"])\n",
    "response = arize_client.log_evaluations_sync(\n",
    "    dataframe=final_df,\n",
    "    model_id=model_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Session Eval Results](https://storage.googleapis.com/arize-phoenix-assets/assets/images/arize-session-level-evals.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
