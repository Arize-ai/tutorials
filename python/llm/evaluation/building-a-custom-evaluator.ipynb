{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FdgxBgEf9fC3"
   },
   "source": [
    "<center>\n",
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "    <img alt=\"arize logo\" src=\"https://storage.googleapis.com/arize-assets/arize-logo-white.jpg\" width=\"300\"/>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/arize/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/client_python\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://arize-ai.slack.com/join/shared_invite/zt-11t1vbu4x-xkBIHmOREQnYnYDH1GDfCg\">Slack Community</a>\n",
    "    </p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsP3EGfq1sZi"
   },
   "source": [
    "# Using a Benchmark Dataset to Build a Custom LLM as a Judge Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJmqp2Po9mQM"
   },
   "source": [
    "In this tutorial, you’ll learn how to build a custom LLM-as-a-Judge Evaluator tailored to your specific use case. While Arize provides several [pre-built evaluators](https://arize.com/docs/ax/evaluate/llm-as-a-judge/arize-evaluators-llm-as-a-judge) that have been tested against benchmark datasets, these may not always cover the nuances of your application.\n",
    "\n",
    "So how can you achieve the same level of rigor when your use case falls outside the scope of standard evaluators?\n",
    "\n",
    "We’ll walk through how to create your own benchmark dataset using a small set of annotated examples. This dataset will allow you to build and refine a custom evaluator by revealing failure cases and guiding iteration. The use case we will be exploring is data extraction from an image of a receipt. \n",
    "\n",
    "To follow along, you’ll need:\n",
    "\n",
    "*   A free [Arize AX](https://app.arize.com/auth/join) account\n",
    "*   An OpenAI API Key\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Keys and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qqqq arize arize-otel openinference-instrumentation-openai arize-phoenix-evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qq openai nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "from getpass import getpass\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "# if not \"SPACE_ID\" in os.environ:\n",
    "#     os.environ[\"SPACE_ID\"] = getpass(\"🔑 Enter your Arize Space ID: \")\n",
    "# if not \"API_KEY\" in os.environ:\n",
    "#     os.environ[\"API_KEY\"] = getpass(\"🔑 Enter your Arize API Key: \")\n",
    "\n",
    "os.environ[\"SPACE_ID\"] = \"U3BhY2U6MTg3MjU6S2RTRQ==\"\n",
    "os.environ[\"API_KEY\"] = \"ak-7641807d-614f-4c03-b21c-87ad7ad7f7b7-jPieRjiTYRsm-17_D3mxNbRKs1_ai3I2\"\n",
    "if not \"OPENAI_API_KEY\" in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"🔑 Enter your OpenAI API Key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arize.otel import register\n",
    "from openinference.instrumentation.openai import OpenAIInstrumentor\n",
    "\n",
    "# configure the Phoenix tracer\n",
    "tracer_provider = register(\n",
    "    space_id=os.environ[\"SPACE_ID\"],\n",
    "    api_key=os.environ[\"API_KEY\"],\n",
    "    project_name=\"receipt-classifications\",\n",
    ")\n",
    "OpenAIInstrumentor().instrument(tracer_provider=tracer_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gWSWwjCkmdS"
   },
   "source": [
    "# Generate Image Classification Traces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UdYjpCRIlO1n"
   },
   "source": [
    "In this tutorial, we’ll ask an LLM to generate expense reports from receipt images provided as public URLs. Running the cells below will generate traces, which you can explore directly in Phoenix for annotation. We’ll use GPT-4, which supports image inputs.\n",
    "\n",
    "\n",
    "Dataset Information:\n",
    "Jakob (2024). Receipt or Invoice Dataset. Roboflow Universe. CC BY 4.0. Available at: https://universe.roboflow.com/jakob-awn1e/receipt-or-invoice (accessed on 2025‑07‑29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import pandas as pd\n",
    "urls = [\n",
    "\"https://source.roboflow.com/Zf1kEIcRTrhHBZ7wgJleS4E92P23/8M5px2yLoNtZ6gOQ2r1D/original.jpg\",\n",
    "\"https://source.roboflow.com/Zf1kEIcRTrhHBZ7wgJleS4E92P23/8EVgYMNObyV6kLqBNeFG/original.jpg\",\n",
    "\"https://source.roboflow.com/Zf1kEIcRTrhHBZ7wgJleS4E92P23/86aohWmcEfO0XkflO8AB/original.jpg\",\n",
    "\"https://source.roboflow.com/HahhKcbQqdf8YAudM4kU3PuVCS72/1eGPBChz7wvovQROk2l8/original.jpg\",\n",
    "\"https://source.roboflow.com/HahhKcbQqdf8YAudM4kU3PuVCS72/0WqR2GSfGmxWB7ozo3Pj/original.jpg\",\n",
    "\"https://source.roboflow.com/HahhKcbQqdf8YAudM4kU3PuVCS72/FAEJRtviIboCYSKFZcEZ/original.jpg\",\n",
    "\"https://source.roboflow.com/HahhKcbQqdf8YAudM4kU3PuVCS72/0AoEaFy8FAw6DVieWCa8/original.jpg\",\n",
    "\"https://source.roboflow.com/HahhKcbQqdf8YAudM4kU3PuVCS72/0Q3hAyNwXNpHTeoWU7fz/original.jpg\",\n",
    "\"https://source.roboflow.com/HahhKcbQqdf8YAudM4kU3PuVCS72/2r876u4WpaCYFdMPwieK/original.jpg\",\n",
    "\"https://source.roboflow.com/HahhKcbQqdf8YAudM4kU3PuVCS72/2ZWeE0yO0oJUDtpgEAPY/original.jpg\",\n",
    "\"https://source.roboflow.com/HahhKcbQqdf8YAudM4kU3PuVCS72/37PF6xfHyuqzIBdO7Kgw/original.jpg\",\n",
    "\"https://source.roboflow.com/Zf1kEIcRTrhHBZ7wgJleS4E92P23/6mo4M0nJeKZEsdKrRfsR/original.jpg\",\n",
    "\"https://source.roboflow.com/Zf1kEIcRTrhHBZ7wgJleS4E92P23/5ezJ8tUBGbNnt0jZi2JU/original.jpg\",\n",
    "\"https://source.roboflow.com/Zf1kEIcRTrhHBZ7wgJleS4E92P23/4BCIWGazhCj03oTMWboO/original.jpg\",\n",
    "\"https://source.roboflow.com/Zf1kEIcRTrhHBZ7wgJleS4E92P23/4B8vXJNwJ7ZuHEWyjgAv/original.jpg\",\n",
    "\"https://source.roboflow.com/Zf1kEIcRTrhHBZ7wgJleS4E92P23/2EpeKbAqsSwciH2IHGyV/original.jpg\",\n",
    "\"https://source.roboflow.com/Zf1kEIcRTrhHBZ7wgJleS4E92P23/2LP3g9rKZrYDkNB3I78c/original.jpg\",\n",
    "\"https://source.roboflow.com/Zf1kEIcRTrhHBZ7wgJleS4E92P23/1hT6iLEIAFBw8W70u2FY/original.jpg\",\n",
    "\"https://source.roboflow.com/Zf1kEIcRTrhHBZ7wgJleS4E92P23/1zaKpaDhRPxkiIDTvMuc/original.jpg\",\n",
    "\"https://source.roboflow.com/Zf1kEIcRTrhHBZ7wgJleS4E92P23/1hF1R2Pt41hnlqhlXLDD/original.jpg\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "def extract_receipt_data(input):\n",
    "  response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"Analyze this receipt and return a brief summary for an expense report. Only include category of expense, total cost, and summary of items\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": input,\n",
    "                    },\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=500,\n",
    "  )\n",
    "  return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in urls:\n",
    "  extract_receipt_data(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5oUoJBZgleEe"
   },
   "source": [
    "# Create Benchmarked Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AemZrhRh_SpU"
   },
   "source": [
    "After generating traces, open Arize to begin annotating your dataset. In this example, we’ll annotate based on \"accuracy\", but you can choose any evaluation criterion that fits your use case. Just be sure to update the query below to match the annotation key you’re using—this ensures the annotated examples are included in your benchmark dataset.\n",
    "\n",
    "Run the cell below to see annotations in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "video_url = \"https://storage.googleapis.com/arize-phoenix-assets/assets/videos/arize-annotation.mp4\"\n",
    "\n",
    "HTML(f\"\"\"\n",
    "<iframe width=\"1200\" height=\"700\" src=\"{video_url}\"\n",
    "frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n",
    "allowfullscreen></iframe>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "from arize.exporter import ArizeExportClient\n",
    "from arize.utils.types import Environments\n",
    "\n",
    "client = ArizeExportClient(api_key = os.environ[\"API_KEY\"])\n",
    "\n",
    "print('#### Exporting your primary dataset into a dataframe.')\n",
    "\n",
    "primary_df = client.export_model_to_df(\n",
    "    space_id=os.environ[\"SPACE_ID\"],\n",
    "    model_id='receipt-classifications',\n",
    "    environment=Environments.TRACING,\n",
    "    start_time=datetime.now() - timedelta(days=50),\n",
    "    end_time=datetime.now(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = primary_df[\n",
    "    (primary_df['annotation.accuracy.label'].notna())\n",
    "][[\n",
    "    'attributes.input.value',\n",
    "    'attributes.output.value',\n",
    "    'annotation.accuracy.label',\n",
    "]].rename(columns={\n",
    "    'attributes.input.value': 'image',\n",
    "    'attributes.output.value': 'response',\n",
    "    'annotation.accuracy.label': 'accuracy'\n",
    "})\n",
    "\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def extract_url(input_value):\n",
    "    data = json.loads(input_value)\n",
    "    return data[\"messages\"][0][\"content\"][1][\"image_url\"][\"url\"]\n",
    "\n",
    "def extract_content(input_value):\n",
    "    data = json.loads(input_value) \n",
    "    return data[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "filtered_df['image'] = filtered_df['image'].apply(extract_url)\n",
    "filtered_df['response'] = filtered_df['response'].apply(extract_content)\n",
    "\n",
    "\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arize.experimental.datasets import ArizeDatasetsClient\n",
    "from arize.experimental.datasets.utils.constants import GENERATIVE\n",
    "\n",
    "client = ArizeDatasetsClient(api_key=os.environ[\"API_KEY\"])\n",
    "dataset_id = client.create_dataset(\n",
    "    space_id=os.environ[\"SPACE_ID\"],\n",
    "    dataset_name=\"annotated-receipts\",\n",
    "    dataset_type=GENERATIVE,\n",
    "    data=filtered_df\n",
    ")\n",
    "\n",
    "dataset_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Dataset](https://storage.googleapis.com/arize-phoenix-assets/assets/images/eval-tutorial-annotated-dataset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7FzMJND2lnHY"
   },
   "source": [
    "# Create evaluation template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38Sm4UBeAJrd"
   },
   "source": [
    "Next, we’ll create a baseline evaluation template and define both the task and the evaluation function. Once these are set up, we’ll run an experiment to compare the evaluator’s performance against our ground truth annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.evals.templates import (\n",
    "    ClassificationTemplate,\n",
    "    PromptPartTemplate,\n",
    "    PromptPartContentType,\n",
    ")\n",
    "\n",
    "rails = [\"accurate\", \"almost accurate\", \"inaccurate\"]\n",
    "classification_template = ClassificationTemplate(\n",
    "    rails=rails,  # Specify the valid output labels\n",
    "    template=[\n",
    "        # Prompt part 1: Task description\n",
    "        PromptPartTemplate(\n",
    "            content_type=PromptPartContentType.TEXT,\n",
    "            template=\"\"\" You are an evaluator tasked with assessing the quality of a model-generated expense report based on a receipt.\n",
    "Below is the model’s generated expense report and the input image:\n",
    "---\n",
    "MODEL OUTPUT (Expense Report): {output}\n",
    "\n",
    "---\n",
    "INPUT RECEIPT: \"\"\",\n",
    "        ),\n",
    "        # Prompt part 2: Insert the image data\n",
    "        PromptPartTemplate(\n",
    "            content_type=PromptPartContentType.IMAGE,\n",
    "            template=\"{image}\",  # Placeholder for the image URL\n",
    "        ),\n",
    "        # Prompt part 3: Define the response format\n",
    "        PromptPartTemplate(\n",
    "            content_type=PromptPartContentType.TEXT,\n",
    "            template=\"\"\" Evaluate the following three aspects and assign one of the following labels for each. Only include the label:\n",
    "- **\"accurate\"** – Fully correct\n",
    "- **\"almost accurate\"** – Mostly correct\n",
    "- **\"inaccurate\"** – Substantially wrong\n",
    "            \"\"\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(classification_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.evals import llm_classify\n",
    "from phoenix.evals.models import OpenAIModel\n",
    "from arize.experimental.datasets.experiments.evaluators.base import EvaluationResult, Evaluator\n",
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "\n",
    "def task_function(dataset_row):\n",
    "    image_url = dataset_row[\"image\"]\n",
    "    output = dataset_row[\"response\"]\n",
    "    response_classification = llm_classify(\n",
    "        data=pd.DataFrame([{\"image\": image_url, \"output\": output}]),\n",
    "        template=classification_template,\n",
    "        model=OpenAIModel(model=\"gpt-4.1\"),\n",
    "        rails=rails,\n",
    "        provide_explanation=True,\n",
    "    )\n",
    "    label = response_classification.iloc[0][\"label\"]\n",
    "    return label\n",
    "\n",
    "class MyEval(Evaluator):\n",
    "    def evaluate(\n",
    "        self, *, output: str, dataset_row: Dict[str, Any], **kwargs: Any\n",
    "    ) -> EvaluationResult:\n",
    "        expected_label = dataset_row[\"accuracy\"]\n",
    "        \n",
    "        # Your evaluation logic here\n",
    "        if output == expected_label:\n",
    "            return EvaluationResult(\n",
    "                explanation=\"Output matches expected accuracy\",\n",
    "                score=1.0,\n",
    "                label=\"correct\"\n",
    "            )\n",
    "        else:\n",
    "            return EvaluationResult(\n",
    "                explanation=\"Output does not match expected accuracy\",\n",
    "                score=0.0,\n",
    "                label=\"incorrect\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.run_experiment(\n",
    "    space_id=os.environ[\"SPACE_ID\"],\n",
    "    dataset_id=dataset_id,\n",
    "    task=task_function,\n",
    "    evaluators=[MyEval()],\n",
    "    experiment_name=\"Initial Experiment\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see your experiment result in the experiments tab of your dataset: \n",
    "\n",
    "![Initial Experiment](https://storage.googleapis.com/arize-phoenix-assets/assets/images/eval-tutorial-first-experiment-arize.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZASPVzDLQiQE"
   },
   "source": [
    "# Iteration 1 to improve evaluator prompt template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXTk3celAZHW"
   },
   "source": [
    "Next, we’ll refine our evaluation prompt template by adding more specific instructions to classification rules. We can add these rules based on gaps we saw in the previous iteration. This additional guidance helps improve accuracy and ensures the evaluator's judgments better align with human expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_template = ClassificationTemplate(\n",
    "    rails=rails,  # Specify the valid output labels\n",
    "    template=[\n",
    "        # Prompt part 1: Task description\n",
    "        PromptPartTemplate(\n",
    "            content_type=PromptPartContentType.TEXT,\n",
    "            template=\"\"\" You are an evaluator tasked with assessing the quality of a model-generated expense report based on a receipt.\n",
    "Below is the model’s generated expense report and the input image:\n",
    "---\n",
    "MODEL OUTPUT (Expense Report): {output}\n",
    "\n",
    "---\n",
    "INPUT RECEIPT: \"\"\",\n",
    "        ),\n",
    "        # Prompt part 2: Insert the image data\n",
    "        PromptPartTemplate(\n",
    "            content_type=PromptPartContentType.IMAGE,\n",
    "            template=\"{image}\",  # Placeholder for the image URL\n",
    "        ),\n",
    "        # Prompt part 3: Define the response format\n",
    "        PromptPartTemplate(\n",
    "            content_type=PromptPartContentType.TEXT,\n",
    "            template=\"\"\" Evaluate the following and assign one of the following labels for each. Only include the label:\n",
    "- **\"accurate\"** – Total price, itemized list, and expense category are all accurate. All three must be correct to get this label.\n",
    "- **\"almost accurate\"** – Mostly correct but with small issues. For example, expense category is too vague.\n",
    "- **\"inaccurate\"** – Substantially wrong or missing information. For example, incorrect total price.\n",
    "            \"\"\",\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.run_experiment(\n",
    "    space_id=os.environ[\"SPACE_ID\"],\n",
    "    dataset_id=dataset_id,\n",
    "    task=task_function,\n",
    "    evaluators=[MyEval()],\n",
    "    experiment_name=\"Stronger Prompt Experiment\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OUlH0oT3SnLq"
   },
   "source": [
    "# Iteration 2 to improve evaluator prompt template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EtCVayyRA1j8"
   },
   "source": [
    "To further improve our evaluator, we’ll introduce few-shot examples into the evaluation prompt. These examples help highlight common failure cases and guide the evaluator toward more consistent and generalized judgments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_template = ClassificationTemplate(\n",
    "    rails=rails,  # Specify the valid output labels\n",
    "    template=[\n",
    "        # Prompt part 1: Task description\n",
    "        PromptPartTemplate(\n",
    "            content_type=PromptPartContentType.TEXT,\n",
    "            template=\"\"\" You are an evaluator tasked with assessing the quality of a model-generated expense report based on a receipt.\n",
    "Below is the model’s generated expense report and the input image:\n",
    "---\n",
    "MODEL OUTPUT (Expense Report): {output}\n",
    "\n",
    "---\n",
    "INPUT RECEIPT: \"\"\",\n",
    "        ),\n",
    "        # Prompt part 2: Insert the image datae\n",
    "        PromptPartTemplate(\n",
    "            content_type=PromptPartContentType.IMAGE,\n",
    "            template=\"{image}\",  # Placeholder for the image URL\n",
    "        ),\n",
    "        # Prompt part 3: Define the response format\n",
    "        PromptPartTemplate(\n",
    "            content_type=PromptPartContentType.TEXT,\n",
    "            template=\"\"\" Evaluate the following three aspects and assign one of the following labels for each. Only include the label:\n",
    "- **\"accurate\"** – Total price, itemized list, and expense category are accurate. All three must be correct to get this label.\n",
    "  An incorrect category is one that is overly vague (e.g., “Miscellaneous”, \"Supplies\") or does not accurately reflect the itemized list.\n",
    "  For example, \"Dining and Entertainment\" should not be grouped together if the itemized list only includes food.\n",
    "  Reasonable general categories like “Office Supplies” or “Groceries” are acceptable if they align with the listed items.\n",
    "\n",
    "- **\"almost accurate\"** – Mostly correct but with small issues. For example, expense category is too vague.\n",
    "  If a category includes extra fields (ex: \"Dining and Entertainment\", but the receipt only includes food) mark this as almost correct.\n",
    "- **\"inaccurate\"** – Substantially wrong or missing. For example, incorrect total price or one of more of the items is missing makes the total result inaccurate.\n",
    "            \"\"\",\n",
    "        ),\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.run_experiment(\n",
    "    space_id=os.environ[\"SPACE_ID\"],\n",
    "    dataset_id=dataset_id,\n",
    "    task=task_function,\n",
    "    evaluators=[MyEval()],\n",
    "    experiment_name=\"Few Shot Experiment\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WD5A8ILUBGSN"
   },
   "source": [
    "# Final Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3dxEKV2YBH7V"
   },
   "source": [
    "Once your evaluator reaches a performance level you're satisfied with, it's ready for use. The target score will depend on your benchmark dataset and specific use case. That said, you can continue applying the techniques from this tutorial to refine and iterate until the evaluator meets your desired level of quality.\n",
    "\n",
    "You can also compare your experiment outcomes to baseline results or previous versions to evaluate progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQrp-RXBDUW7"
   },
   "source": [
    "![Final Results](https://storage.googleapis.com/arize-phoenix-assets/assets/images/eval-tutorial-compare-experiment.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
