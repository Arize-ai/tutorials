{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLdlB9yTCDxG"
   },
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "    <img alt=\"arize logo\" src=\"https://storage.googleapis.com/arize-assets/arize-logo-white.jpg\" width=\"300\"/>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/arize/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/client_python\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://join.slack.com/t/arize-ai/shared_invite/zt-1px8dcmlf-fmThhDFD_V_48oU7ALan4Q\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "\n",
    "<center><h1>Evaluations Quickstart </h1></center>\n",
    "\n",
    "## Overview\n",
    "Evaluations are essential to understanding how well your model is performing in real-world scenarios, allowing you to identify strengths, weaknesses, and areas of improvement.\n",
    "\n",
    "Offline evaluations are run as code and then sent back to Arize using `log_evaluations_sync`.\n",
    "\n",
    "This guide assumes you have traces in Arize and are looking to run an evaluation to measure your application performance.\n",
    "\n",
    "To add evaluations you can set up online evaluations as a task to run automatically, or you can follow the steps below to generate evaluations and log them to Arize:\n",
    "\n",
    "1. Install the Arize SDK\n",
    "2. Import your spans in code\n",
    "3. Run a custom evaluator using Phoenix\n",
    "4. Log evaluations back to Arize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_1sryUCQCDxK"
   },
   "source": [
    "## Install dependencies and setup keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q arize arize-phoenix openai pandas nest_asyncio 'httpx<0.28'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTY35G60CDxL"
   },
   "source": [
    "Copy the ARIZE_API_KEY, SPACE_ID, and ARIZE_DEVELOPER_KEY from your Space Settings page (shown below) to the variables in the cell below.\n",
    "\n",
    "<center><img src=\"https://storage.googleapis.com/arize-assets/barcelos/Screenshot%202024-11-11%20at%209.28.27%E2%80%AFPM.png\" width=\"700\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if not os.environ.get(\"SPACE_ID\"):\n",
    "    os.environ[\"SPACE_ID\"] = getpass(\"ðŸ”‘ Enter your space id: \")\n",
    "\n",
    "if not os.environ.get(\"ARIZE_API_KEY\"):\n",
    "    os.environ[\"ARIZE_API_KEY\"] = getpass(\"ðŸ”‘ Enter your ARIZE_API_KEY: \")\n",
    "\n",
    "if not os.environ.get(\"ARIZE_DEVELOPER_KEY\"):\n",
    "    os.environ[\"ARIZE_DEVELOPER_KEY\"] = getpass(\"ðŸ”‘ Enter your developer key: \")\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yabIw0AkCDxM"
   },
   "source": [
    "## Import your spans in code\n",
    "\n",
    "Once you have traces in Arize, you can visit the LLM Tracing tab to see your traces and export them in code. By clicking the export button, you can get the boilerplate code to copy paste to your evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements required for getting your spans\n",
    "from datetime import datetime, timedelta\n",
    "from arize.exporter import ArizeExportClient\n",
    "from arize.utils.types import Environments\n",
    "\n",
    "start_time = datetime.now() - timedelta(days=14)  # 14 days ago\n",
    "end_time = datetime.now()  # Today\n",
    "\n",
    "# Exporting your dataset into a dataframe\n",
    "client = ArizeExportClient(api_key=os.environ[\"ARIZE_DEVELOPER_KEY\"])\n",
    "primary_df = client.export_model_to_df(\n",
    "    space_id=os.environ[\"SPACE_ID\"],\n",
    "    model_id=\"tracing-haiku-tutorial\",  # change this to the name of your project\n",
    "    environment=Environments.TRACING,\n",
    "    start_time=start_time,\n",
    "    end_time=end_time,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jd7GFiuoCDxM"
   },
   "source": [
    "## Run a custom evaluator using Phoenix\n",
    "\n",
    "Create a prompt template for the LLM to judge the quality of your responses. You can utilize any of the Arize Evaluator Templates or you can create your own. Below is an example which judges the positivity or negativity of the LLM output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from phoenix.evals import OpenAIModel, llm_classify\n",
    "\n",
    "eval_model = OpenAIModel(\n",
    "    model=\"gpt-4o\", temperature=0, api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    ")\n",
    "\n",
    "MY_CUSTOM_TEMPLATE = \"\"\"\n",
    "    You are evaluating the positivity or negativity of the responses to questions.\n",
    "    [BEGIN DATA]\n",
    "    ************\n",
    "    [Question]: {input}\n",
    "    ************\n",
    "    [Response]: {output}\n",
    "    [END DATA]\n",
    "\n",
    "\n",
    "    Please focus on the tone of the response.\n",
    "    Your answer must be single word, either \"positive\" or \"negative\"\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ALmn0LXCDxM"
   },
   "source": [
    "Notice the variables in brackets for {input} and {output} above. You will need to set those variables appropriately for the dataframe so you can run your custom template. We use OpenInference as a set of conventions (complementary to OpenTelemetry) to trace AI applications. This means depending on the provider you are using, the attributes of the trace will be different.\n",
    "\n",
    "You can use the code below to check which attributes are in the traces in your dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KD7ZvjgQCDxM"
   },
   "source": [
    "Use the code below to set the input and output variables needed for the prompt above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_df[\"input\"] = primary_df[\"attributes.input.value\"]\n",
    "primary_df[\"output\"] = primary_df[\"attributes.output.value\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-GyksRt_CDxN"
   },
   "source": [
    "Use the `llm_classify` function to run the evaluation using your custom template. You will be using the dataframe from the traces you generated above. We also add `nest_asyncio` to run the evaluations concurrently (if you are running multiple evaluations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "evals_df = llm_classify(\n",
    "    dataframe=primary_df,\n",
    "    template=MY_CUSTOM_TEMPLATE,\n",
    "    model=eval_model,\n",
    "    rails=[\"positive\", \"negative\"],\n",
    "    provide_explanation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znv29AdHCDxN"
   },
   "source": [
    "If you'd like more information, see our detailed guide on <a href=\"https://docs.arize.com/arize/llm-evaluation-and-annotations/catching-hallucinations/custom-evaluators\"> custom evaluators.</a> You can also use our <a href=\"https://docs.arize.com/arize/llm-evaluation-and-annotations/catching-hallucinations/arize-evaluators-llm-as-a-judge\">pre-tested evaluators</a> for evaluating hallucination, toxicity, retrieval, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bfeISZDNCDxN"
   },
   "source": [
    "# Log evaluations back to Arize\n",
    "Use the `log_evaluations_sync` function as part of our Python SDK to attach evaluations you've run to traces. The code below assumes that you have already completed an evaluation run, and you have the `evals_dataframe` object. It also assumes you have a `traces_dataframe` object to get the `span_id` that you need to attach the evals.\n",
    "\n",
    "The `evals_dataframe` requires four columns, which should be auto-generated for you based on the evaluation you ran using Phoenix. The `<eval_name>` must be alphanumeric and cannot have hyphens or spaces.\n",
    "- `eval.<eval_name>.label`\n",
    "- `eval.<eval_name>.score`\n",
    "- `eval.<eval_name>.explanation`\n",
    "- `context.span_id`\n",
    "\n",
    "An example evaluation data dictionary would look like:\n",
    "```python\n",
    "evaluation_data = {\n",
    "   'context.span_id': ['74bdfb83-a40e-4351-9f41-19349e272ae9'],  # Use your span_id\n",
    "   'eval.myeval.label': ['accuracy'],  # Example label name\n",
    "   'eval.myeval.score': [0.95],        # Example label value\n",
    "   'eval.myeval.explanation': [\"some explanation\"]\n",
    "}\n",
    "evaluation_df = pd.DataFrame(evaluation_data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUWLwKDOCDxN"
   },
   "source": [
    "Here is sample code to log the evaluations back to Arize. The API reference can be found <a href=\"https://arize-client-python.readthedocs.io/en/latest/llm-api/logger.html#arize.pandas.logger.Client.log_evaluations_sync\">here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals_df[\"eval.tone_eval.label\"] = evals_df[\"label\"]\n",
    "evals_df[\"eval.tone_eval.explanation\"] = evals_df[\"explanation\"]\n",
    "evals_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from arize.pandas.logger import Client\n",
    "\n",
    "ARIZE_API_KEY = os.environ.get(\"ARIZE_API_KEY\")\n",
    "SPACE_ID = os.environ.get(\"SPACE_ID\")\n",
    "ARIZE_DEVELOPER_KEY = os.environ.get(\"ARIZE_DEVELOPER_KEY\")\n",
    "\n",
    "# Initialize Arize client to log evaluations\n",
    "arize_client = Client(\n",
    "    space_id=SPACE_ID, api_key=ARIZE_API_KEY, developer_key=ARIZE_DEVELOPER_KEY\n",
    ")\n",
    "\n",
    "# Set the evals_df to have the correct span ID to log it to Arize\n",
    "evals_df[\"context.span_id\"] = primary_df[\"context.span_id\"]\n",
    "\n",
    "# send the eval to Arize\n",
    "arize_client.log_evaluations_sync(evals_df, \"tracing-haiku-tutorial\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
