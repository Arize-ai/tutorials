{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "    <img alt=\"arize logo\" src=\"https://storage.googleapis.com/arize-assets/arize-logo-white.jpg\" width=\"300\"/>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/arize/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/client_python\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://arize-ai.slack.com/join/shared_invite/zt-11t1vbu4x-xkBIHmOREQnYnYDH1GDfCg\">Slack Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "\n",
    "# <center>Tracing and Evaluating Gemini Audio</center>\n",
    "\n",
    "This notebook is adapted from Google's \"[Gemini API: Audio Quickstart Notebook](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Audio.ipynb)\" and provides an example of how to prompt Gemini Flash using an audio file. \n",
    "\n",
    "In this case, you'll use a [sound recording](https://www.jfklibrary.org/asset-viewer/archives/jfkwha-006) of President John F. Kennedyâ€™s 1961 State of the Union address.  \n",
    "\n",
    "This notebook performs the following tasks:\n",
    "\n",
    "1. Prompt Gemini to generate a transcript of the audio recording.\n",
    "2. Trace Gemini API calls and send the traces to the Arize platform with links to audio file for playback.\n",
    "3. Evaluate the transcription output from Gemini for sentiment analysis using Phoenix Evals and Gemini LLM (LLM as a Judge)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "857d8bf104ed"
   },
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -U google-genai \"arize-phoenix-evals==0.19.0\"  \"arize==7.31.1\" opentelemetry-api opentelemetry-sdk openinference-semantic-conventions  arize-otel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXiv-NeZR5WA"
   },
   "source": [
    "## Configure your Gemini API key\n",
    "\n",
    "To run the following cell, your API key must be stored it in a Colab Secret named `GEMINI_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](../quickstarts/Authentication.ipynb) for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "#from google.colab import userdata\n",
    "\n",
    "GEMINI_API_KEY = getpass.getpass(prompt=\"Enter your Gemini API Key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iRmQf9YknJ_8"
   },
   "source": [
    "# Load an audio file sample and set the URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Audio file url --> allows you to play audio in UI\n",
    "URL = \"https://storage.googleapis.com/generativeai-downloads/data/State_of_the_Union_Address_30_January_1961.mp3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q $URL -O sample.mp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "\n",
    "your_file = gemini_client.files.upload(file='sample.mp3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRReWzlWQtYv"
   },
   "source": [
    "# Tracing setup\n",
    "\n",
    "You'll need to set Arize AX variables (Space id, API key and Developer Key) below to send traces to the Arize AX Platform.  Sign up for free [here](https://app.arize.com/auth/join)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opentelemetry import trace\n",
    "from arize.otel import register\n",
    "from opentelemetry.trace import Status, StatusCode\n",
    "from opentelemetry.semconv.trace import SpanAttributes\n",
    "\n",
    "\n",
    "ARIZE_SPACE_ID = getpass.getpass(prompt=\"Enter your ARIZE SPACE ID Key: \")\n",
    "ARIZE_API_KEY = getpass.getpass(prompt=\"Enter your ARIZE API Key: \")\n",
    "ARIZE_DEVELOPER_KEY = getpass.getpass(prompt=\"Enter your ARIZE DEVELOPER Key: \")\n",
    "PROJECT_NAME = \"gemini-audio\"  # Set this to any name you'd like for your app\n",
    "\n",
    "# Setup OTel via our convenience function\n",
    "tracer_provider = register(\n",
    "    space_id = ARIZE_SPACE_ID, # in app space settings page\n",
    "    api_key = ARIZE_API_KEY, # in app space settings page\n",
    "    project_name = PROJECT_NAME,\n",
    ")\n",
    "\n",
    "trace.set_tracer_provider(tracer_provider)\n",
    "tracer = trace.get_tracer(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m01XDoo4UQvN"
   },
   "source": [
    "## Configure prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Provide a transcript of the speech from 01:00 to 01:30.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnKJ_H2VnuTD"
   },
   "source": [
    "## Call Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call Gemini\n",
    "\n",
    "with tracer.start_as_current_span(\n",
    "    \"process_audio\",\n",
    "    openinference_span_kind=\"llm\",\n",
    ") as span:\n",
    "  span.set_attribute(\"input.audio.url\", URL)\n",
    "  span.set_attribute(\"llm.prompts\", prompt)\n",
    "  span.set_attribute(\"input.value\", prompt)\n",
    "  response = gemini_client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    contents=[\n",
    "      prompt,\n",
    "      your_file,\n",
    "    ]\n",
    "  )\n",
    "  span.set_attribute(\"input.audio.transcript\", response.text)\n",
    "  span.set_attribute(\"output.value\", response.text)\n",
    "  span.set_status(Status(StatusCode.OK))\n",
    "\n",
    "response.text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ln36O5eNLltg"
   },
   "source": [
    "# Evaluate Gemini's output transcript for sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vSTbTsya_bbf"
   },
   "source": [
    "First, export spans from Arize that contain transcript output from Arize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVELOPER_KEY = getpass.getpass(prompt=\"Enter your ARIZE DEVELOPER Key: \")\n",
    "\n",
    "print('#### Installing arize SDK')\n",
    "\n",
    "! pip install \"arize[Tracing]>=7.1.0\"\n",
    "\n",
    "print('#### arize SDK installed!')\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['ARIZE_API_KEY'] = DEVELOPER_KEY\n",
    "\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "from arize.exporter import ArizeExportClient\n",
    "from arize.utils.types import Environments\n",
    "\n",
    "client = ArizeExportClient()\n",
    "\n",
    "print('#### Exporting your primary dataset into a dataframe.')\n",
    "\n",
    "primary_df = client.export_model_to_df(\n",
    "    space_id=getpass.getpass(prompt=\"Enter your ARIZE SPACE ID Key: \"),\n",
    "    model_id=PROJECT_NAME,\n",
    "    where=\"name = 'process_audio'\", #Just pull the spans with name = \"process_audio\"\n",
    "    environment=Environments.TRACING,\n",
    "    start_time = datetime.now(timezone.utc) - timedelta(days=1),\n",
    "    end_time = datetime.now(timezone.utc) #pull traces for the last 24 hours\n",
    ")\n",
    "\n",
    "#set the column in the dataframe to match the variable name used in our eval template\n",
    "primary_df[\"output\"] = primary_df[\"attributes.output.value\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVaX93lvLqQB"
   },
   "source": [
    "### Evaluation Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTIMENT_EVAL_TEMPLATE = \"\"\"\n",
    "\n",
    "You are a helpful AI bot that checks for the sentiment in the output text. Your task is to evaluate the sentiment of the given output and categorize it as positive, neutral, or negative.\n",
    "\n",
    "Here is the data:\n",
    "[BEGIN DATA]\n",
    "============\n",
    "[Output]: {attributes.output.value}\n",
    "============\n",
    "[END DATA]\n",
    "\n",
    "Determine the sentiment of the output based on the content and context provided. Your response should be ONLY a single word, either \"positive\", \"neutral\", or \"negative\", and should not contain any text or characters aside from that word.\n",
    "\n",
    "Then write out in a step by step manner an EXPLANATION to show how you determined the sentiment of the output.  Do not include any text or characters aside from the EXPLANATION.\n",
    "\n",
    "Your response should follow the format of the example response below. Provide a single LABEL and a single EXPLANATION. Do not include any special characters in the response. Do not include special characters such as \"#\" in your response.\n",
    "\n",
    "Example response:\n",
    "\n",
    "EXPLANATION: An explanation of your reasoning for why the label is \"positive\", \"neutral\", or \"negative\"\n",
    "LABEL: \"positive\" or \"neutral\" or \"negative\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "du6u5hDIKVDx"
   },
   "source": [
    "### Evaluate transcriptions using Gemini as a LLM as a Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gemini as LLM as a Judge - LLM Classify\n",
    "\n",
    "#google auth to access the Gemini model\n",
    "!gcloud auth application-default login # authenticate with google\n",
    "!gcloud config set project audioevals # you must have a valid project id in your google cloud account first\n",
    "\n",
    "import pandas as pd\n",
    "from phoenix.evals import (GeminiModel, llm_classify)\n",
    "\n",
    "#We will use Gemini 1.5 pro to evaluate the text transcription\n",
    "project_id = \"audioevals\" # Set this to your google project id\n",
    "gemini_model = GeminiModel(model=\"gemini-1.5-pro\", project=project_id)\n",
    "\n",
    "rails = [\"positive\", \"neutral\", \"negative\"]\n",
    "\n",
    "evals_df = llm_classify(\n",
    "    data=primary_df,\n",
    "    template=SENTIMENT_EVAL_TEMPLATE,\n",
    "    model=gemini_model,\n",
    "    rails=rails,\n",
    "    provide_explanation=True\n",
    ")\n",
    "\n",
    "#set eval labels\n",
    "evals_df[\"eval.sentiment.label\"] = evals_df[\"label\"]\n",
    "evals_df[\"eval.sentiment.explanation\"] = evals_df[\"explanation\"]\n",
    "evals_df[\"context.span_id\"] = primary_df[\"context.span_id\"]\n",
    "\n",
    "evals_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FsVqOtLb7bq8"
   },
   "source": [
    "## Send evaluations to Arize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arize.pandas.logger import Client\n",
    "\n",
    "\n",
    "# Initialize Arize client using the model_id and version you used previously\n",
    "arize_client = Client(\n",
    "    space_id=ARIZE_SPACE_ID,\n",
    "    api_key=ARIZE_API_KEY,\n",
    "    developer_key=ARIZE_DEVELOPER_KEY\n",
    ")\n",
    "\n",
    "# send the evaluation results to Arize\n",
    "arize_client.log_evaluations_sync(evals_df, \"gemini-audio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zudj6gxEWR2Q"
   },
   "source": [
    "## Next Steps\n",
    "### Useful API references:\n",
    "\n",
    "More details about Gemini API's [vision capabilities](https://ai.google.dev/gemini-api/docs/vision) in the documentation.\n",
    "\n",
    "If you want to know about the File API, check its [API reference](https://ai.google.dev/api/files) or the [File API](https://github.com/google-gemini/cookbook/blob/main/quickstarts/File_API.ipynb) quickstart.\n",
    "\n",
    "### Related examples\n",
    "\n",
    "Check this example using the audio files to give you more ideas on what the gemini API can do with them:\n",
    "* Share [Voice memos](https://github.com/google-gemini/cookbook/blob/main/examples/Voice_memos.ipynb) with Gemini API and brainstorm ideas\n",
    "\n",
    "### Continue your discovery of the Gemini API\n",
    "\n",
    "Have a look at the [Audio](../quickstarts/Audio.ipynb) quickstart to learn about another type of media file, then learn more about [prompting with media files](https://ai.google.dev/tutorials/prompting_with_media) in the docs, including the supported formats and maximum length for audio files. .\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
