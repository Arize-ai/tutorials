{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "    <img alt=\"arize logo\" src=\"https://storage.googleapis.com/arize-assets/arize-logo-white.jpg\" width=\"300\"/>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/arize/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/client_python\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://arize-ai.slack.com/join/shared_invite/zt-11t1vbu4x-xkBIHmOREQnYnYDH1GDfCg\">Slack Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "\n",
    "# <center>Evaluation using Pydantic Evals</center>\n",
    "\n",
    "1. Use Pydantic Evals to evaluate your LLM app for a simple question-answering task.\n",
    "2. Log your results to Arize to track your experiments and traces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "857d8bf104ed"
   },
   "source": [
    "## Step 1: Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pydantic-evals \"arize[Tracing]\" arize-otel openai openinference-instrumentation-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Setup API keys and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from pydantic_evals import Case, Dataset\n",
    "from getpass import getpass\n",
    "import os\n",
    "\n",
    "SPACE_ID = globals().get(\"SPACE_ID\") or getpass(\n",
    "    \"ðŸ”‘ Enter your Arize Space ID: \"\n",
    ")\n",
    "API_KEY = globals().get(\"API_KEY\") or getpass(\"ðŸ”‘ Enter your Arize API Key: \")\n",
    "OPENAI_API_KEY = globals().get(\"OPENAI_API_KEY\") or getpass(\n",
    "    \"ðŸ”‘ Enter your OpenAI API key: \"\n",
    ")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Setup Arize\n",
    "Add our auto-instrumentation for OpenAI using arize-otel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arize.otel import register\n",
    "tracer_provider = register(\n",
    "    space_id=SPACE_ID,  \n",
    "    api_key=API_KEY,\n",
    "    project_name=\"pydantic-evals-tutorial\",  \n",
    ")\n",
    "\n",
    "from openinference.instrumentation.openai import OpenAIInstrumentor\n",
    "OpenAIInstrumentor().instrument(tracer_provider=tracer_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define the Evaluation Dataset\n",
    "Create a dataset of test cases using Pydantic Evals for a question-answering task.\n",
    "1. Each Case represents a single test with an input (question) and an expected output (answer).\n",
    "2. The Dataset aggregates these cases for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = [\n",
    "    Case(name=\"capital of France\", inputs=\"What is the capital of France?\", expected_output=\"Paris\"),\n",
    "    Case(name=\"author of Romeo and Juliet\", inputs=\"Who wrote Romeo and Juliet?\", expected_output=\"William Shakespeare\"),\n",
    "    Case(name=\"largest planet\", inputs=\"What is the largest planet in our solar system?\", expected_output=\"Jupiter\")\n",
    "]\n",
    "dataset = Dataset(cases=cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Setup LLM task to evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "def evaluate_case(case):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": case.inputs}]\n",
    "    )\n",
    "    output = response.choices[0].message.content\n",
    "    print(output)\n",
    "    is_correct = case.expected_output.lower() in output.strip().lower()\n",
    "    return is_correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Run your experiment and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [evaluate_case(case) for case in dataset.cases]\n",
    "\n",
    "for case, result in zip(dataset.cases, results):\n",
    "    print(f\"Case: {case.name}, Correct: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7. See your results in Arize\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/arize-assets/fixtures/pydantic-evals.png\" width=\"800\"/>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
