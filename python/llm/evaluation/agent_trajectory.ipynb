{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "    <img alt=\"arize logo\" src=\"https://storage.googleapis.com/arize-assets/arize-logo-white.jpg\" width=\"300\"/>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/arize/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/client_python\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://arize-ai.slack.com/join/shared_invite/zt-11t1vbu4x-xkBIHmOREQnYnYDH1GDfCg\">Slack Community</a>\n",
    "    </p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bF22jNBxpJEm"
   },
   "source": [
    "# Agent Trajectory Evaluation\n",
    "\n",
    "This notebook demonstrates how to evaluate whether an agent's tool calling trajectory matches expected patterns. Agent trajectories represent the sequence of actions (tool calls) an agent takes to accomplish a task.\n",
    "\n",
    "**Why this matters**: Evaluating agent trajectories helps you:\n",
    "- Understand if your agent follows expected problem-solving paths\n",
    "- Identify inefficient or incorrect tool usage patterns\n",
    "- Debug agent behavior\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nue9sj5gpJEt"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Configure your environment variables and import dependencies. You'll need to set up your Arize API key and import necessary libraries for data processing and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"arize>=7.44.0\" arize-phoenix arize-phoenix-evals getpass pandas datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.evals import (\n",
    "    llm_classify,\n",
    "    OpenAIModel # see https://docs.arize.com/phoenix/evaluation/evaluation-models\n",
    "    # for a full list of supported models\n",
    ")\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from arize.exporter import ArizeExportClient\n",
    "from arize.utils.types import Environments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "api_key = getpass('Enter your Arize API key: ')\n",
    "os.environ['ARIZE_API_KEY'] = api_key\n",
    "\n",
    "space_id = getpass('Enter your Arize Space ID: ')\n",
    "os.environ['ARIZE_SPACE_ID'] = space_id\n",
    "\n",
    "model_id = getpass('Enter your Arize Space ID(Project Name): ')\n",
    "os.environ['ARIZE_MODEL_ID'] = model_id\n",
    "\n",
    "openai_key = getpass('Enter your OpenAI API key: ')\n",
    "os.environ['OPENAI_API_KEY'] = openai_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DWmdVV8kpJEz"
   },
   "source": [
    "   ## Data Extraction\n",
    "   \n",
    "   Pull trace data from Arize and prepare it for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ArizeExportClient(api_key=os.environ['ARIZE_API_KEY'])\n",
    "\n",
    "print('#### Exporting your primary dataset into a dataframe.')\n",
    "\n",
    "\n",
    "primary_df = client.export_model_to_df(\n",
    "    space_id=os.environ['ARIZE_SPACE_ID'],\n",
    "    model_id=os.environ['ARIZE_MODEL_ID'],\n",
    "    environment=Environments.TRACING,\n",
    "    start_time=datetime.now() - timedelta(days=7),\n",
    "    end_time=datetime.now(),\n",
    "    # Optionally specify columns to improve query performance\n",
    "    # columns=['context.span_id', 'attributes.llm.input']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "# data_url = \"https://storage.cloud.google.com/arize-assets/tutorials/example/agent_trajectory_sample_data.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZtkqLzVkpJE3"
   },
   "source": [
    "## Prompt Template Definition\n",
    "\n",
    "The evaluation uses a carefully designed prompt template that instructs the LLM how to compare actual agent trajectories against golden trajectories. You can customize this template to fit your specific evaluation criteria.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4mMxHVzfpJE4"
   },
   "source": [
    "### Prompt Variables\n",
    "\n",
    "| Variable | Description | Source |\n",
    "|----------|-------------|--------|\n",
    "| `{reference_outputs}` | The golden/expected trajectory | From your reference data |\n",
    "| `{tool_calls}` | The actual trajectory executed by the agent | Extracted from trace data |\n",
    "\n",
    "### Customizing the Prompt\n",
    "\n",
    "You may want to adjust the evaluation criteria or output format based on your specific use case:\n",
    "\n",
    "- Add specific criteria relevant to your agent's domain\n",
    "- Include additional metadata\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAJECTORY_ACCURACY_PROMPT_WITH_REFERENCE = \"\"\"\n",
    "You are a helpful AI bot that checks whether an AI agent’s internal trajectory is accurate and effective.\n",
    "\n",
    "You will be given:\n",
    "1. The agent’s actual trajectory of tool calls\n",
    "2. You will be given input data from a user that the agent used to make a decision\n",
    "3. You will be given a tool call definition, what the agent used to make the tool call\n",
    "4. You will be given a golden trajectory that represents the ideal flows in normal use\n",
    "\n",
    "An accurate trajectory:\n",
    "- Progresses logically from step to step\n",
    "- Follows the golden trajectory where reasonable\n",
    "- Shows a clear path toward completing a goal\n",
    "- Is reasonably efficient (doesn’t take unnecessary detours)\n",
    "\n",
    "##\n",
    "\n",
    "Correct Trajectory:\n",
    "{reference_outputs}\n",
    "\n",
    "##\n",
    "\n",
    "Actual Trajectory:\n",
    "{tool_calls}\n",
    "\n",
    "Use Inputs:\n",
    "{attributes.input.value}\n",
    "\n",
    "Tool Definition:\n",
    "{attributes.llm.tools}\n",
    "\n",
    "##\n",
    "\n",
    "Compare the actual trajectory to the golden one:\n",
    "- Highlight any major deviations\n",
    "- Determine whether the deviations are acceptable or harmful\n",
    "- Assess if the overall goal is still achieved effectively\n",
    "\n",
    "Your response must be a single string, either `correct` or `incorrect`, and must not include any additional text.\n",
    "\n",
    "- Respond with `correct` if the agent’s trajectory adheres to the rubric and accomplishes the task effectively.\n",
    "- Respond with `incorrect` if the trajectory is confusing, misaligned with the goal, inefficient, or does not accomplish the task.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAJECTORY_ACCURACY_PROMPT_WITHOUT_REFERENCE = \"\"\"\n",
    "You are a helpful AI bot that checks whether an AI agent’s internal trajectory is accurate and effective.\n",
    "\n",
    "You will be given:\n",
    "1. The agent’s actual trajectory of tool calls\n",
    "2. You will be given input data from a user that the agent used to make a decision\n",
    "3. You will be given a tool call definition, what the agent used to make the tool call\n",
    "\n",
    "An accurate trajectory:\n",
    "- Progresses logically from step to step\n",
    "- Follows the golden trajectory where reasonable\n",
    "- Shows a clear path toward completing a goal\n",
    "- Is reasonably efficient (doesn’t take unnecessary detours)\n",
    "\n",
    "##\n",
    "\n",
    "Actual Trajectory:\n",
    "{tool_calls}\n",
    "\n",
    "Use Inputs:\n",
    "{attributes.input.value}\n",
    "\n",
    "Tool Definitions:\n",
    "{attributes.llm.tools}\n",
    "\n",
    "##\n",
    "\n",
    "\n",
    "Your response must be a single string, either `correct` or `incorrect`, and must not include any additional text.\n",
    "\n",
    "- Respond with `correct` if the agent’s trajectory adheres to the rubric and accomplishes the task effectively.\n",
    "- Respond with `incorrect` if the trajectory is confusing, misaligned with the goal, inefficient, or does not accomplish the task.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "di-Npfe5pJE7"
   },
   "source": [
    " ## Data Preparation\n",
    "\n",
    "These functions filter and transform trace data into the format needed for evaluation.\n",
    "\n",
    "**Core concepts:**\n",
    "- **Trace filtering**: Selecting which agent executions to evaluate\n",
    "- **Span filtering**: Selecting which parts of each execution to analyze\n",
    "- **Tool call extraction**: Identifying the sequence of actions taken\n",
    "\n",
    "The `filter_spans_by_trace_criteria` function is particularly important as it allows you to:\n",
    "1. Select relevant traces using trace-level filters (e.g., by user query type, duration)\n",
    "2. Focus on specific spans within those traces (e.g., only LLM-generated tool calls)\n",
    "\n",
    "This two-level filtering gives you fine-grained control over your evaluation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "\n",
    "def filter_spans_by_trace_criteria(\n",
    "    df: pd.DataFrame,\n",
    "    trace_filters: Dict[str, Dict[str, Any]],\n",
    "    span_filters: Dict[str, Dict[str, Any]]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Filter spans based on trace-level and span-level criteria.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with trace data\n",
    "        trace_filters: Dictionary of column names and filtering criteria for traces\n",
    "                      Format: {\"column_name\": {\"operator\": value}}\n",
    "                      Supported operators: \">=\", \"<=\", \"==\", \"!=\", \"contains\", \"notna\", \"isna\"\n",
    "        span_filters: Dictionary of column names and filtering criteria for spans\n",
    "                     Format: {\"column_name\": {\"operator\": value}}\n",
    "                     Same supported operators as trace_filters\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with filtered spans from traces that match trace_filters\n",
    "    \"\"\"\n",
    "    # Get all unique trace_ids\n",
    "    all_trace_ids = set(df['context.trace_id'].unique())\n",
    "    print(f\"Total traces: {len(all_trace_ids)}\")\n",
    "\n",
    "    # Create a copy of the dataframe for filtering\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Find traces matching the trace criteria\n",
    "    traces_df = df_copy.copy()\n",
    "    for column, criteria in trace_filters.items():\n",
    "        if column not in traces_df.columns:\n",
    "            print(f\"Warning: Column '{column}' not found in dataframe\")\n",
    "            continue\n",
    "\n",
    "        for operator, value in criteria.items():\n",
    "            if operator == \">=\":\n",
    "                matching_spans = traces_df[traces_df[column] >= value]\n",
    "            elif operator == \"<=\":\n",
    "                matching_spans = traces_df[traces_df[column] <= value]\n",
    "            elif operator == \"==\":\n",
    "                matching_spans = traces_df[traces_df[column] == value]\n",
    "            elif operator == \"!=\":\n",
    "                matching_spans = traces_df[traces_df[column] != value]\n",
    "            elif operator == \"contains\":\n",
    "                matching_spans = traces_df[traces_df[column].str.contains(value, case=False, na=False)]\n",
    "            elif operator == \"isna\":\n",
    "                matching_spans = traces_df[traces_df[column].isna()]\n",
    "            elif operator == \"notna\":\n",
    "                matching_spans = traces_df[traces_df[column].notna()]\n",
    "            else:\n",
    "                print(f\"Warning: Unsupported operator '{operator}' - skipping\")\n",
    "                continue\n",
    "\n",
    "            traces_df = matching_spans\n",
    "\n",
    "    matching_trace_ids = set(traces_df['context.trace_id'].unique())\n",
    "    print(f\"Found {len(matching_trace_ids)} traces matching trace criteria\")\n",
    "\n",
    "    if not matching_trace_ids:\n",
    "        print(\"No matching traces found\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Filter to keep only rows from matching traces\n",
    "    result_df = df[df['context.trace_id'].isin(matching_trace_ids)].copy()\n",
    "\n",
    "    # Apply span filters\n",
    "    for column, criteria in span_filters.items():\n",
    "        if column not in result_df.columns:\n",
    "            print(f\"Warning: Column '{column}' not found in dataframe\")\n",
    "            continue\n",
    "\n",
    "        for operator, value in criteria.items():\n",
    "            if operator == \">=\":\n",
    "                result_df = result_df[result_df[column] >= value]\n",
    "            elif operator == \"<=\":\n",
    "                result_df = result_df[result_df[column] <= value]\n",
    "            elif operator == \"==\":\n",
    "                result_df = result_df[result_df[column] == value]\n",
    "            elif operator == \"!=\":\n",
    "                result_df = result_df[result_df[column] != value]\n",
    "            elif operator == \"contains\":\n",
    "                result_df = result_df[result_df[column].str.contains(value, case=False, na=False)]\n",
    "            elif operator == \"isna\":\n",
    "                result_df = result_df[result_df[column].isna()]\n",
    "            elif operator == \"notna\":\n",
    "                result_df = result_df[result_df[column].notna()]\n",
    "            else:\n",
    "                print(f\"Warning: Unsupported operator '{operator}' - skipping\")\n",
    "                continue\n",
    "\n",
    "    print(f\"Final result: {len(result_df)} spans from {len(matching_trace_ids)} traces\")\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_trace_data_for_evaluation(\n",
    "    df,\n",
    "    group_by_col=\"context.trace_id\",\n",
    "    extract_cols={\"tool_calls\": \"tool_calls\"},\n",
    "    additional_data=None,\n",
    "    filter_empty=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Prepare trace data for evaluation by grouping, sorting by start_time, and extracting specified columns.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame containing trace data\n",
    "        group_by_col: Column to group traces by (default: \"context.trace_id\")\n",
    "        extract_cols: Dict mapping {output_key: source_column} to extract from each row\n",
    "                     Can contain multiple columns to extract\n",
    "        additional_data: Dict of additional data to include with each trace (default: None)\n",
    "        filter_empty: Whether to filter out empty values (default: True)\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with processed trace data ready for evaluation\n",
    "    \"\"\"\n",
    "    # Group by specified column\n",
    "    grouped = df.groupby(group_by_col)\n",
    "\n",
    "    # Prepare results list\n",
    "    results = []\n",
    "\n",
    "    for group_id, group in grouped:\n",
    "        # Always sort by start_time to ensure correct order\n",
    "        group = group.sort_values(\"start_time\")\n",
    "\n",
    "        # Initialize a dict to store extracted data\n",
    "        trace_data = {group_by_col: group[group_by_col].iloc[0]}\n",
    "\n",
    "        # Extract and process each requested column\n",
    "        for output_key, source_col in extract_cols.items():\n",
    "            ordered_extracts = []\n",
    "            # Iterate through rows as dictionaries to handle column names with dots\n",
    "            for i, (_, row_data) in enumerate(group.reset_index(drop=True).iterrows()):\n",
    "                # Convert row to dictionary for easier access\n",
    "                row_dict = row_data.to_dict()\n",
    "                value = row_dict.get(source_col)\n",
    "                if not filter_empty or (value is not None and value):\n",
    "                    ordered_extracts.append({str(i + 1): value})\n",
    "            trace_data[output_key] = ordered_extracts\n",
    "\n",
    "        # Add any additional data\n",
    "        if additional_data:\n",
    "            trace_data.update(additional_data)\n",
    "\n",
    "        # Add to results\n",
    "        results.append(trace_data)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tool_calls(output_messages):\n",
    "    if not output_messages:\n",
    "        return []\n",
    "\n",
    "    tool_calls = []\n",
    "    for message in output_messages:\n",
    "        if \"message.tool_calls\" in message:\n",
    "            for tool_call in message[\"message.tool_calls\"]:\n",
    "                tool_calls.append({\n",
    "                    \"name\": tool_call[\"tool_call.function.name\"],\n",
    "                    \"arguments\": tool_call[\"tool_call.function.arguments\"]\n",
    "                })\n",
    "    return tool_calls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eh0p7yzapJE9"
   },
   "source": [
    "## Evaluation Configuration\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6rsMwPtpJE9"
   },
   "source": [
    "**Reference outputs** define your golden path - what tools *should* be called and in what order. These represent your expectation of the ideal agent behavior for a given task.\n",
    "\n",
    "Note: This only makes sense with deterministic paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference_outputs = {\"1\":\"get_llm_table_search\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjVHhyJDpJE9"
   },
   "source": [
    "#### **Filter Data**\n",
    "\n",
    "Customize these parameters to match your specific evaluation needs:\n",
    "\n",
    "| Parameter | Description | Example |\n",
    "|-----------|-------------|---------|\n",
    "| reference_outputs | Expected tool calls | `{\"1\": \"get_llm_table_search\"}` |\n",
    "| trace_filters | Criteria for selecting traces | `{\"name\": {\"contains\": \"searchrouter\"}}` |\n",
    "| span_filters | Criteria for selecting spans within traces | `{\"attributes.openinference.span.kind\": {\"==\": \"LLM\"}}` |\n",
    "\n",
    "Span filters are crucial as they determine which specific spans within the matched traces will be used for the evaluation. For example, filtering for `\"openinference.span.kind\": \"LLM\"` ensures we only analyze LLM-related spans within the selected traces.\n",
    "   > **Note**: Update the `trace_filters` and `span_filters` to match your specific evaluation criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eval_traces = filter_spans_by_trace_criteria(\n",
    "    df=primary_df,\n",
    "    trace_filters={\"name\": {\"contains\": \"searchrouter\"}},\n",
    "    span_filters={\"attributes.openinference.span.kind\": {\"==\": \"LLM\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WqHppiaxpJE-"
   },
   "source": [
    "We need to extract the tool calls from the output messages to use in the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_traces['tool_calls'] = eval_traces['attributes.llm.output_messages'].apply(extract_tool_calls)\n",
    "\n",
    "eval_traces[['tool_calls']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6Tu46BtpJE-"
   },
   "source": [
    "### Prepare the data for the evaluation\n",
    "This will group the prompt variables by trace_id and extract the required columns and append any additional data to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_calls_df = prepare_trace_data_for_evaluation(\n",
    "    df=eval_traces,\n",
    "    extract_cols={\"tool_calls\": \"tool_calls\", \"attributes.llm.tools\": \"attributes.llm.tools\", \"attributes.input.value\":\"attributes.input.value\"}, #can also add any additional columns to the dataframe\n",
    "    # additional_data={\"reference_outputs\": reference_outputs},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_calls_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = tool_calls_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-HaNCE1QpJFI"
   },
   "source": [
    "## Running the Evaluation\n",
    "\n",
    "After preparing your traces and configuring the evaluation parameters, you can execute the LLM-based evaluation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = OpenAIModel(\n",
    "    api_key=os.environ['OPENAI_API_KEY'],\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rails =[\"correct\",\"incorrect\"]\n",
    "eval_results = llm_classify(\n",
    "    dataframe=sample_data,\n",
    "    template=TRAJECTORY_ACCURACY_PROMPT_WITHOUT_REFERENCE,\n",
    "    model=model,\n",
    "    rails=rails,\n",
    "    provide_explanation=True,\n",
    "    verbose=False,\n",
    "    concurrency=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHbyZv7VpJFK"
   },
   "source": [
    "   ## Analyzing Results\n",
    "   \n",
    "   The evaluation results contain:\n",
    "   - **label**: Overall trajectory assessment (correct/incorrect)\n",
    "   - **explanation**: Detailed reasoning for the assessment\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFjuG6xopJFK"
   },
   "source": [
    "The evaluation results can then be merged with your original data for analysis or to log back to Arize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# merge with original df to get span_id\n",
    "merged_df = pd.merge(\n",
    "    sample_data, eval_results, left_index=True, right_index=True\n",
    ")\n",
    "\n",
    "\n",
    "merged_df.rename(columns={\n",
    "    'label': 'trace_eval.AgentTrajectoryAccuracy.label',\n",
    "    'explanation': 'trace_eval.AgentTrajectoryAccuracy.explanation'\n",
    "}, inplace=True)\n",
    "\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the span_id where parent_id is null for each trace_id\n",
    "root_spans = primary_df[primary_df['parent_id'].isnull()][['context.trace_id', 'context.span_id']]\n",
    "\n",
    "# Merge with merged_df to get the root span_id\n",
    "final_df = pd.merge(\n",
    "    merged_df,\n",
    "    root_spans,\n",
    "    on='context.trace_id',\n",
    "    how='left'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_eval_df = final_df[['context.trace_id', 'context.span_id', 'trace_eval.AgentTrajectoryAccuracy.label', 'trace_eval.AgentTrajectoryAccuracy.explanation']]\n",
    "\n",
    "final_eval_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from arize.pandas.logger import Client\n",
    "\n",
    "\n",
    "\n",
    "# Initialize Arize client using the model_id of your traces\n",
    "arize_client = Client(space_id=os.environ['ARIZE_SPACE_ID'], api_key=os.environ['ARIZE_API_KEY'])\n",
    "\n",
    "\n",
    "# Set the evals_df to have the correct span ID to log it to Arize\n",
    "final_eval_df = final_eval_df.set_index(final_df[\"context.span_id\"])\n",
    "\n",
    "# Use Arize client to log evaluations\n",
    "response = arize_client.log_evaluations_sync(\n",
    "    dataframe=final_eval_df,\n",
    "    model_id=os.environ['ARIZE_MODEL_ID'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See your results in Arize\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/arize-phoenix-assets/assets/docs/notebooks/agent-trajectory/Screenshot%202025-06-22%20at%208.43.47%E2%80%AFPM.png\" width=\"800\"/>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
