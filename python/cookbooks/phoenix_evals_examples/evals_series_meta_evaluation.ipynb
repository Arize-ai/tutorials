{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c91db37",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-phoenix-assets/assets/phoenix-logo-light.svg\" width=\"200\"/>\n",
    "        <br>\n",
    "        <a href=\"https://arize.com/docs/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://arize-ai.slack.com/join/shared_invite/zt-2w57bhem8-hq24MB6u7yE_ZF_ilOYSBw#/shared-invite/email\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\">LLM as a Judge 102: Meta-Evaluation </h1>\n",
    "\n",
    "\n",
    "The purpose of this notebook is apart of the Evals Best Practices Series, Episode 6: LLM as a Judge 102: Meta-Evaluation. \n",
    "This notebook will go through the process of Meta Evaluation, the process of evaluating your evaluator.\n",
    "\n",
    ">\n",
    "> ##### Note: This notebook was last updated on Dec 10, 2025. \n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae36e36e",
   "metadata": {},
   "source": [
    "##### Install Dependencies and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d94f30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q arize-phoenix openai getpass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd52adf",
   "metadata": {},
   "source": [
    "##### Initiate the Tracer Provider to Auto Instrument our Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5243ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.otel import register \n",
    "import os \n",
    "from openai import AsyncOpenAI\n",
    "from getpass import getpass \n",
    "\n",
    "tracer_provider = register(auto_instrument=True)\n",
    "\n",
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "\n",
    "openai_client = AsyncOpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eae360e",
   "metadata": {},
   "source": [
    "## Step 1: Prepare your dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f88abb",
   "metadata": {},
   "source": [
    "#### Import CSV data in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d399932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "full_df = pd.read_csv('TruthfulQA.csv')\n",
    "print(f\"Dataset shape: {full_df.shape}\")\n",
    "print(f\"\\nColumns: {full_df.columns.tolist()}\")\n",
    "full_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec65e487",
   "metadata": {},
   "source": [
    "#### Take in 250 total random samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b8ab82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = full_df.sample(n=250)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46397fca",
   "metadata": {},
   "source": [
    "#### Create a 80/20 split for our Dev & Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17043d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(.8 * len(df))\n",
    "dev_df = df.iloc[:split]\n",
    "test_df = df.iloc[split:]\n",
    "print(f\"Dataset shape: {dev_df.shape}\")\n",
    "print(f\"Dataset shape: {test_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b545f5",
   "metadata": {},
   "source": [
    "#### Set your Data to follow a 75/25 Correct/Incorrect Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75efd5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def correct_incorrect_split(df):\n",
    "    df_copy = df.copy()\n",
    "    n_total = len(df_copy)\n",
    "    n_correct = int(0.75 * n_total)\n",
    "    labels = ['Correct'] * n_correct + ['Incorrect'] * (n_total - n_correct)\n",
    "    random.shuffle(labels)\n",
    "    df_copy['Ground Truth'] = labels\n",
    "    return df_copy\n",
    "\n",
    "def add_answer(df):\n",
    "    df_copy = correct_incorrect_split(df)\n",
    "    df_copy['Answer'] = df_copy.apply(\n",
    "        lambda row: row['Correct Answers'] if row['Ground Truth'] == 'Correct' \n",
    "        else row['Incorrect Answers'] if row['Ground Truth'] == 'Incorrect'\n",
    "        else None,\n",
    "        axis=1\n",
    "    )\n",
    "    df_copy = df_copy[['Type', 'Category', 'Question', 'Answer', 'Ground Truth', 'Source', 'Correct Answers', 'Incorrect Answers', 'Best Answer']]\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "test = add_answer(test_df)\n",
    "dev = add_answer(dev_df)\n",
    "test.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c68175b",
   "metadata": {},
   "source": [
    "#### Send your Datasets to Phoenix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0819adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.client import AsyncClient\n",
    "\n",
    "client = AsyncClient()\n",
    "\n",
    "test_dataset = await client.datasets.create_dataset(\n",
    "    dataframe=test,\n",
    "    name=\"test-dataset\",\n",
    "    input_keys=[\"Question\", \"Answer\"],\n",
    "    output_keys=[\"Ground Truth\"]\n",
    ")\n",
    "\n",
    "dev_dataset = await client.datasets.create_dataset(\n",
    "    dataframe=dev,\n",
    "    name=\"dev-dataset\",\n",
    "    input_keys=[\"Question\", \"Answer\"],\n",
    "    output_keys=[\"Ground Truth\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3fd3a7",
   "metadata": {},
   "source": [
    "#### Create our experiment to run\n",
    "##### Define your Task & evaluators "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebd2fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.evals import create_classifier\n",
    "from phoenix.evals.llm import LLM\n",
    "\n",
    "async def base_task(example) -> str: \n",
    "    base_qa_prompt = \"\"\"\n",
    "        You are given a question and an answer.\n",
    "        {input}\n",
    "        Return only the label \"Correct\" or \"Incorrect\"\n",
    "    \"\"\"\n",
    "\n",
    "    base_qa_eval = create_classifier(\n",
    "        name=\"base_qa_eval\",\n",
    "        prompt_template=base_qa_prompt,\n",
    "        llm=LLM(provider=\"openai\", model=\"gpt-4\"),\n",
    "        choices={\"Correct\": 1, \"Incorrect\": 0},\n",
    "    )\n",
    "\n",
    "    eval_result = await base_qa_eval.async_evaluate(example)\n",
    "    score = eval_result[0]\n",
    "\n",
    "    return {\"label\": score.label, \"score\": score.score, \"explanation\": score.explanation} \n",
    "\n",
    "async def exact_match(example, output) -> float:\n",
    "    return 1.0 if output[\"label\"] in example.output[\"Ground Truth\"] else 0.0\n",
    "\n",
    "evaluators = [exact_match]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec3b97b",
   "metadata": {},
   "source": [
    "#### Run your Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5166144",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.client.experiments import async_run_experiment\n",
    "dev_base_experiment = await async_run_experiment(\n",
    "    dataset=dev_dataset,\n",
    "    task=base_task,\n",
    "    evaluators=evaluators,\n",
    "    experiment_name=\"base task\",\n",
    "    client=client,\n",
    "    repetitions=1,\n",
    ")\n",
    "\n",
    "test_base_experiment = await async_run_experiment(\n",
    "    dataset=test_dataset,\n",
    "    task=base_task,\n",
    "    evaluators=evaluators,\n",
    "    experiment_name=\"new base task\",\n",
    "    client=client,\n",
    "    repetitions=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c8ead1",
   "metadata": {},
   "source": [
    "## Step 2: Calculate Metrics\n",
    "\n",
    "#### Compare your human and LLM judgements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df5bf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment_results(experiment, dataset):\n",
    "    task_runs = pd.DataFrame([run for run in experiment['task_runs']])[['dataset_example_id', 'output']].rename(columns={'output': 'output_dict'})\n",
    "    task_runs[['label', 'score', 'explanation']] = (task_runs['output_dict'].apply(lambda x: pd.Series(x) if isinstance(x, dict) else pd.Series([None, None, None])))\n",
    "    return task_runs.merge(pd.DataFrame(dataset), left_on='dataset_example_id', right_on='id', how='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95c0675",
   "metadata": {},
   "source": [
    "#### Calculate classification metrics & Plot a confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0511eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "base_dev_df = get_experiment_results(dev_base_experiment, dev_dataset)\n",
    "\n",
    "# Map Ground Truth to binary labels (1 for 'Correct Answers', 0 for 'Incorrect Answers')\n",
    "y_true = base_dev_df['output'].apply(lambda x: 1 if x['Ground Truth'] == 'Correct' else 0)\n",
    "y_pred = base_dev_df['score'] \n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=['Incorrect', 'Correct']))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Incorrect', 'Correct'], \n",
    "            yticklabels=['Incorrect', 'Correct'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6305d4",
   "metadata": {},
   "source": [
    "## Step 3: Inspect Results\n",
    "##### See the examples where the eval did not match the ground truth. Looking at the explanations can help provide insight into changes to make to the evaluator prompt. You can do this either in code or in the Phoenix UI. \n",
    "\n",
    "![ui](https://storage.googleapis.com/arize-phoenix-assets/assets/images/phoenix-docs-images/experiment_explanatiosn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff8d63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = base_dev_df['output'].apply(lambda x: x['Ground Truth'])\n",
    "mismatched_df = base_dev_df[base_dev_df['label'] != ground_truth]\n",
    "\n",
    "info = mismatched_df[['input', 'output', 'label', 'explanation']]\n",
    "info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306d68f1",
   "metadata": {},
   "source": [
    "## Step 4: Iterate and Improve\n",
    "##### Time for Improvements - Tweak your prompt, model, or criteria based on the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70b967d",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def human_task(example) -> str: \n",
    "    human_qa_prompt = \"\"\"\n",
    "        You are given a question and an answer. Determine whether the answer correctly and fully answers \n",
    "        the question based on the information contained in the answer itself. If the question can have multiple answers, \n",
    "        and the answer provided is one of them, classify it as correct. Take the question at face value, \n",
    "        and then look at the answer to determine if the answer is correct or incorrect.\n",
    "        {input}\n",
    "        Return only the label \"Correct\" or \"Incorrect\"\n",
    "    \"\"\"\n",
    "\n",
    "    human_qa_eval = create_classifier(\n",
    "        name=\"human_qa_eval\",\n",
    "        prompt_template=human_qa_prompt,\n",
    "        llm=LLM(provider=\"openai\", model=\"gpt-4\"),\n",
    "        choices={\"Correct\": 1, \"Incorrect\": 0},\n",
    "    )\n",
    "\n",
    "    eval_result = await human_qa_eval.async_evaluate(example)\n",
    "    score = eval_result[0]\n",
    "\n",
    "    return {\"label\": score.label, \"score\": score.score, \"explanation\": score.explanation} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b533f1d9",
   "metadata": {},
   "source": [
    "#### Run new + old against the test set for a final comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75f432b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_human_experiment = await async_run_experiment(\n",
    "    dataset=dev_dataset,\n",
    "    task=human_task,\n",
    "    evaluators=evaluators,\n",
    "    experiment_name=\"human task\",\n",
    "    client=client,\n",
    "    repetitions=1,\n",
    ")\n",
    "\n",
    "test_human_experiment = await async_run_experiment(\n",
    "    dataset=test_dataset,\n",
    "    task=human_task,\n",
    "    evaluators=evaluators,\n",
    "    experiment_name=\"human task\",\n",
    "    client=client,\n",
    "    repetitions=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982c7869",
   "metadata": {},
   "source": [
    "#### Calculate classification metrics & Plot a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3313fd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "human_dev_df = get_experiment_results(dev_human_experiment, dev_dataset)\n",
    "\n",
    "# Map Ground Truth to binary labels (1 for 'Correct Answers', 0 for 'Incorrect Answers')\n",
    "y_true = human_dev_df['output'].apply(lambda x: 1 if x['Ground Truth'] == 'Correct' else 0)\n",
    "y_pred = human_dev_df['score'] \n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=['Incorrect', 'Correct']))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Incorrect', 'Correct'], \n",
    "            yticklabels=['Incorrect', 'Correct'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b619b8b7",
   "metadata": {},
   "source": [
    "### Bonus!\n",
    "\n",
    "##### Let's test out a Meta Eval - pass your eval through an LLM and ask it to output an improved version. \n",
    "##### First, Define your prompt for this improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25295f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_qa_prompt = \"\"\"\n",
    "    You are given a question and an answer.\n",
    "    {input}\n",
    "    Return only the label \"Correct\" or \"Incorrect\"\n",
    "\"\"\"\n",
    "\n",
    "meta_eval_prompt = f\"\"\"\n",
    "You are an expert in prompt optimization. You will be given the original baseline prompt and the following associated metadata \n",
    "(such as model inputs, outputs, evaluation labels and explanations). Your task is to generate a revised version of the original \n",
    "prompt that would likely improve results with respect to the evaluation labels.\n",
    "Your goal is to align the prompt with the feedback and evaluation criteria. Look at every example in the data. \n",
    "You may want to evaluate the explanation for clarity, correctness, completeness, and alignment with the intent of the original \n",
    "prompt. Identify weaknesses such as ambiguity, missing steps, unnecessary text, or faulty reasoning. \n",
    "Produce a much more detailed & improved version of the baseline prompt. \n",
    "\n",
    "The baseline prompt is: {base_qa_prompt}\n",
    "\n",
    "The data to take into account is: {info}\n",
    "\n",
    "Your Goal: Iterate on the original prompt (above) with a new, better prompt that will improve the results, based on the examples and feedback above.\n",
    "Your Output: Return only the new prompt.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4cc39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = await openai_client.chat.completions.create(model=\"gpt-4o-mini\", messages=[{\"role\": \"user\", \"content\": meta_eval_prompt}])\n",
    "content = resp.choices[0].message.content.strip()\n",
    "new_meta_prompt = content\n",
    "new_meta_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfbc248",
   "metadata": {},
   "source": [
    "##### Copy your updated prompt in the function below as `meta_qa_prompt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833839c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def meta_task(example) -> str: \n",
    "    meta_qa_prompt = \"\"\"\n",
    "        You are tasked with evaluating whether the provided answer accurately addresses the question posed. \n",
    "        For each input, which includes a \\'Question\\' and an \\'Answer\\', please consider the following guidelines \n",
    "        to determine if the answer is correct:\n",
    "\n",
    "        1. Assess the question and the answer in relation to known facts or common understanding on the subject matter. \n",
    "        2. Identify if the answer explicitly addresses the question or misses critical aspects.\n",
    "        3. Consider whether multiple interpretations of the question or answer exist and evaluate their correctness based on context.\n",
    "\n",
    "        Your response should strictly consist of the label \"Correct\" if the answer is accurate and \n",
    "        properly addresses the question, or \"Incorrect\" if it is not.\n",
    "\n",
    "        Please analyze the following input: \\n{input}'\n",
    "    \"\"\"\n",
    "\n",
    "    meta_qa_eval = create_classifier(\n",
    "        name=\"meta_qa_eval\",\n",
    "        prompt_template=meta_qa_prompt,\n",
    "        llm=LLM(provider=\"openai\", model=\"gpt-4\"),\n",
    "        choices={\"Correct\": 1, \"Incorrect\": 0},\n",
    "    )\n",
    "\n",
    "    eval_result = await meta_qa_eval.async_evaluate(example)\n",
    "    score = eval_result[0]\n",
    "\n",
    "    return {\"label\": score.label, \"score\": score.score, \"explanation\": score.explanation} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc0353a",
   "metadata": {},
   "source": [
    "##### Run your Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14282e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_meta_experiment = await async_run_experiment(\n",
    "    dataset=dev_dataset,\n",
    "    task=meta_task,\n",
    "    evaluators=evaluators,\n",
    "    experiment_name=\"meta task\",\n",
    "    client=client,\n",
    "    repetitions=1,\n",
    ")\n",
    "\n",
    "test_meta_experiment = await async_run_experiment(\n",
    "    dataset=test_dataset,\n",
    "    task=meta_task,\n",
    "    evaluators=evaluators,\n",
    "    experiment_name=\"meta task\",\n",
    "    client=client,\n",
    "    repetitions=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8443900",
   "metadata": {},
   "source": [
    "#### Calculate classification metrics & Plot a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53088f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "meta_dev_df = get_experiment_results(dev_meta_experiment, dev_dataset)\n",
    "\n",
    "# Map Ground Truth to binary labels (1 for 'Correct Answers', 0 for 'Incorrect Answers')\n",
    "y_true = meta_dev_df['output'].apply(lambda x: 1 if x['Ground Truth'] == 'Correct' else 0)\n",
    "y_pred = meta_dev_df['score'] \n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=['Incorrect', 'Correct']))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Incorrect', 'Correct'], \n",
    "            yticklabels=['Incorrect', 'Correct'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb7a0e9",
   "metadata": {},
   "source": [
    "### Look at your Improvements\n",
    "![](https://storage.googleapis.com/arize-phoenix-assets/assets/images/phoenix-docs-images/meta_eval_102.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
