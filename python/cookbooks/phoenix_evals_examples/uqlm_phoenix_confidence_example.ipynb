{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4548b727",
   "metadata": {},
   "source": [
    "\n",
    "# UQLM √ó Arize Phoenix ‚Äî Response-Level Confidence & Hallucination Risk\n",
    "\n",
    "This notebook shows how to compute **model-agnostic, ground-truth-free** confidence / risk scores using **[UQLM](https://github.com/cvs-health/uqlm)** after creating a dataset and starting an experiment in **Arize Phoenix**. \n",
    "\n",
    "**What you'll do:**\n",
    "1. Install and configure Phoenix & UQLM\n",
    "2. Create a small demo dataset with prompts & responses\n",
    "3. Upload your Dataset into Phoenix & Configure a task to create your pre-sampled responses\n",
    "4. Compute UQLM **per-scorer confidences** and an **ensemble** confidence\n",
    "5. Derive **risk = 1 - confidence** and an optional **high_risk** flag\n",
    "\n",
    "> üß© **Why**: UQLM provides a production-friendly, model-agnostic uncertainty signal that complements judge-style evals and helps you **flag risky answers without labeled ground truth**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efdc7dd",
   "metadata": {},
   "source": [
    "In your terminal, after run `pip install arize-phoenix`, please run `phoenix serve` to locally host Phoenix. Then proceed with this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640d068f",
   "metadata": {},
   "source": [
    "## 0) Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f51027",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q arize-phoenix uqlm pandas numpy getpass "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa33d95c",
   "metadata": {},
   "source": [
    "## 1) Imports & version check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89a1140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from getpass import getpass \n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "import pandas as pd\n",
    "from uqlm import BlackBoxUQ, WhiteBoxUQ\n",
    "HAVE_UQLM = True\n",
    "\n",
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"üîë Enter your OpenAI API key: \")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adc92c7",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Configure Phoenix connection\n",
    "\n",
    "Set these if you're sending results to **Phoenix Cloud** or your own **self-hosted** Phoenix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df56981",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.otel import register\n",
    "project_name = \"cvs evals\"\n",
    "tracer_provider = register(project_name=project_name, auto_instrument=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae16bff",
   "metadata": {},
   "source": [
    "## 3) Demo dataset\n",
    "\n",
    "We'll make a small dataset with:\n",
    "- `input` ‚Äì the user prompt\n",
    "- `output` ‚Äì the model response \n",
    "\n",
    "Once we upload this dataset into Phoenix, we can run a task to create our:\n",
    "- `sampled_responses` ‚Äì a list of stochastic responses for the same prompt (for **black-box UQLM**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89cccac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.client import Client\n",
    "\n",
    "simple_dataset = [{\n",
    "    \"input\": \"What is the capital of France?\",\n",
    "    \"output\": \"Paris is the capital of France.\",\n",
    "}, {\n",
    "    \"input\": \"Explain quantum entanglement in one sentence.\",\n",
    "    \"output\": \"Quantum entanglement is when particles share a state no matter the distance, showing instant correlations.\",\n",
    "}, {\n",
    "    \"input\": \"Who won the 2023 Wimbledon men's singles?\",\n",
    "    \"output\": \"Carlos Alcaraz won the 2023 Wimbledon men's singles title.\",\n",
    "}, {\n",
    "    \"input\": \"Give me three uses of sodium chloride in medicine.\",\n",
    "        \"output\": \"Sodium chloride is used for IV fluids, nasal irrigation, and as a wound-cleaning solution.\",\n",
    "}]\n",
    "simple_df = pd.DataFrame(simple_dataset)\n",
    "\n",
    "client = Client()\n",
    "dataset = client.datasets.create_dataset(\n",
    "    dataframe=simple_df,\n",
    "    name=\"cvs_evals\",\n",
    "    input_keys=[\"input\"],\n",
    "    output_keys=[\"output\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab3dc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "def my_task(example):\n",
    "    client = OpenAI()\n",
    "    prompt = f\"\"\"\n",
    "    You will be given a question. I want 5 sampled responses to the question.\n",
    "    You will return a list of 5 responses. \n",
    "    Here is your question: {example.input}\n",
    "    This is the expected output: \n",
    "    [\n",
    "        \"response 1\",\n",
    "        \"response 2\",\n",
    "        \"response 3\",\n",
    "        \"response 4\",\n",
    "        \"response 5\"\n",
    "    ]\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcaa436",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.client.experiments import run_experiment\n",
    "\n",
    "experiment = run_experiment(\n",
    "    dataset=dataset,\n",
    "    task=my_task,\n",
    "    experiment_name=\"my-experiment\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e940d174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "rows = []\n",
    "for run in experiment['task_runs']:\n",
    "    row = dict(run)\n",
    "    output = run.get('output', {})\n",
    "    if isinstance(output, dict):\n",
    "        row.update(output)\n",
    "    else:\n",
    "        row['output'] = output\n",
    "    rows.append(row)\n",
    "df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ec460a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'output': 'sampled_responses'})\n",
    "responses_df = df['sampled_responses']\n",
    "responses_df = responses_df.iloc[::-1].reset_index(drop=True)\n",
    "df = pd.merge(simple_df, responses_df, left_index=True,right_index=True, how='left')\n",
    "df[\"sampled_responses\"] = df[\"sampled_responses\"].apply(json.loads)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0922c2d3",
   "metadata": {},
   "source": [
    "\n",
    "## 4) UQLM adapter: per-scorer + ensemble confidence\n",
    "\n",
    "Below is a compact adapter that:\n",
    "\n",
    "- Runs **BlackBoxUQ** `score(...)` if you already have `responses` and `sampled_responses`. # CVS: Added 'responses'\n",
    "- Optionally runs **BlackBoxUQ** `generate_and_score(...)` if you pass an `llm` and set `num_responses > 0`.\n",
    "- Optionally runs **WhiteBoxUQ**  if you pass an `llm` that returns token-level logprobs.  # CVS: Removed 'generate_and_score(...)'\n",
    "- Computes an **ensemble** confidence (mean/median/weighted) and adds `uqlm_confidence`, `uqlm_risk`, and `uqlm_high_risk`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b58258b",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def compute_uqlm_confidence(\n",
    "    dataframe: pd.DataFrame,\n",
    "    prompt_col: str = \"input\",\n",
    "    response_col: Optional[str] = None,\n",
    "    sampled_responses_col: Optional[str] = None,\n",
    "    blackbox_scorers: List[str] = [\"noncontradiction\"], \n",
    "    ensemble: str = \"mean\",\n",
    "    ensemble_weights: Optional[Dict[str, float]] = None,\n",
    "    risk_threshold: Optional[float] = None,  \n",
    "    mode: str = \"black_box\",\n",
    "    llm: Optional[Any] = None,\n",
    "    num_responses: int = 5,\n",
    "    whitebox_scorers: List[str] = [\"min_probability\"],\n",
    "    verbose: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compute per-scorer and ensemble confidence with UQLM and return merged dataframe.\n",
    "    Adds columns:\n",
    "      - uqlm_confidence [0,1]\n",
    "      - uqlm_risk [0,1] = 1 - confidence\n",
    "      - uqlm_high_risk (optional bool) if risk_threshold provided\n",
    "      - uqlm_<scorer>_conf (per-scorer, if available)\n",
    "    \"\"\"\n",
    "    if not HAVE_UQLM:\n",
    "        raise ImportError(\"UQLM is not installed. `pip install uqlm`.\")\n",
    "\n",
    "    df = dataframe.copy()\n",
    "    per_scorer_cols = []\n",
    "\n",
    "    def _ensemble(row: Dict[str, Any]) -> float:\n",
    "        vals = [row[c] for c in per_scorer_cols if pd.notnull(row.get(c))]\n",
    "        if not vals:\n",
    "            return float(\"nan\")\n",
    "        if ensemble == \"mean\":\n",
    "            return float(sum(vals) / len(vals))\n",
    "        if ensemble == \"median\":\n",
    "            s = sorted(vals)\n",
    "            n = len(s)\n",
    "            return float((s[n//2] if n % 2 else (s[n//2 - 1] + s[n//2]) / 2))\n",
    "        if ensemble == \"weighted_mean\" and ensemble_weights:\n",
    "            num = 0.0\n",
    "            den = 0.0\n",
    "            for c in per_scorer_cols:\n",
    "                sc = c.replace(\"uqlm_\", \"\").replace(\"_conf\", \"\")\n",
    "                w = float(ensemble_weights.get(sc, 0.0))\n",
    "                if sc in ensemble_weights and pd.notnull(row.get(c)):\n",
    "                    num += w * float(row[c])\n",
    "                    den += w\n",
    "            return float(num / den) if den > 0 else float(\"nan\")\n",
    "        return float(sum(vals) / len(vals))\n",
    "\n",
    "    prompts = df[prompt_col].tolist()\n",
    "    responses = df[response_col].tolist() if response_col is not None and response_col in df.columns else None\n",
    "    sampled = df[sampled_responses_col].tolist() if sampled_responses_col is not None and sampled_responses_col in df.columns else None\n",
    "\n",
    "    if mode == \"auto\":\n",
    "        mode_to_run = \"black_box\" \n",
    "        if llm:\n",
    "            if hasattr(llm, \"logprobs\"):\n",
    "                mode_to_run = \"white_box\"\n",
    "       \n",
    "    else:\n",
    "        mode_to_run = mode\n",
    "\n",
    "    if mode_to_run == \"black_box\":\n",
    "        bbuq = BlackBoxUQ(llm=llm, scorers=blackbox_scorers)\n",
    "        if responses is not None and sampled is not None:\n",
    "            results = bbuq.score(responses=responses, sampled_responses=sampled, show_progress_bars=False)\n",
    "        else:\n",
    "            results = await bbuq.generate_and_score(prompts=prompts, num_responses=num_responses, show_progress_bars=False)\n",
    "    \n",
    "        per_scorer_cols = []\n",
    "        for sc_name in results.data:\n",
    "            if sc_name in blackbox_scorers:\n",
    "                per_scorer_cols.append(f\"uqlm_{sc_name}_conf\")\n",
    "                df[f\"uqlm_{sc_name}_conf\"] = results.data[sc_name]\n",
    "\n",
    "    elif mode_to_run == \"white_box\":\n",
    "       \n",
    "        wbuq = WhiteBoxUQ(llm=llm, scorers=whitebox_scorers)\n",
    "        if verbose: print(\"WhiteBoxUQ.generate_and_score ...\")\n",
    "        results = await wbuq.generate_and_score(prompts=prompts, show_progress_bars=False)\n",
    "\n",
    "        for sc_name in results.data:\n",
    "            if sc_name in whitebox_scorers:\n",
    "                per_scorer_cols.append(f\"uqlm_{sc_name}_conf\")\n",
    "                df[f\"uqlm_{sc_name}_conf\"] = results.data[sc_name]\n",
    "    else:\n",
    "        raise ValueError(\"mode must be one of {'black_box', 'white_box', 'auto'}.\")\n",
    "\n",
    "    df[\"uqlm_confidence\"] = df.apply(_ensemble, axis=1)\n",
    "    df[\"uqlm_risk\"] = 1.0 - df[\"uqlm_confidence\"]\n",
    "    if risk_threshold is not None:\n",
    "        df[\"uqlm_high_risk\"] = df[\"uqlm_risk\"] >= float(risk_threshold)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6785ef",
   "metadata": {},
   "source": [
    "\n",
    "### 4.a) Run **BlackBoxUQ** scoring on pre-sampled responses\n",
    "\n",
    "This path requires **no LLM calls**‚Äîit's the fastest way to test-drive UQLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1ab57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAVE_UQLM:\n",
    "    uqlm_df = await compute_uqlm_confidence(\n",
    "        dataframe=df,\n",
    "        prompt_col=\"input\",\n",
    "        response_col=\"output\",\n",
    "        sampled_responses_col=\"sampled_responses\",\n",
    "        blackbox_scorers=[\"noncontradiction\", \"exact_match\"], \n",
    "        ensemble=\"mean\",\n",
    "        risk_threshold=0.3,   \n",
    "        mode=\"black_box\",\n",
    "        llm=None,           \n",
    "        num_responses=5,\n",
    "        verbose=True,\n",
    "    )\n",
    "else:\n",
    "    print(\"Install UQLM to run this section: `pip install uqlm`\")\n",
    "uqlm_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fc8678",
   "metadata": {},
   "source": [
    "\n",
    "### 4.b) (Optional) Generate-and-score with your LLM client\n",
    "\n",
    "If you provide an `llm` client to UQLM (e.g., OpenAI/Anthropic), set `num_responses > 0` to have UQLM **sample K responses** per prompt and score on the fly.\n",
    "\n",
    "> ‚ö†Ô∏è **Note:** Replace the placeholder `MyLLMClient` with your real client that UQLM supports.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0405536c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4\", \n",
    "    temperature= 1\n",
    ")\n",
    "\n",
    "if HAVE_UQLM:\n",
    "    USE_GENERATE_AND_SCORE = True \n",
    "    if USE_GENERATE_AND_SCORE:\n",
    "        llm = llm\n",
    "        uqlm_gen_df = await compute_uqlm_confidence(\n",
    "            dataframe=df,\n",
    "            mode=\"black_box\",\n",
    "            llm=llm,\n",
    "            num_responses=5,\n",
    "            blackbox_scorers=[\"noncontradiction\", \"cosine_sim\"],\n",
    "            ensemble=\"mean\",\n",
    "            risk_threshold=0.5,\n",
    "            verbose=True,\n",
    "        )\n",
    "    else:\n",
    "        print(\"Set USE_GENERATE_AND_SCORE=True after wiring your LLM client.\")\n",
    "else:\n",
    "    print(\"Install UQLM to run this section: `pip install uqlm`\")\n",
    "uqlm_gen_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4eda79a",
   "metadata": {},
   "source": [
    "\n",
    "### 4.c) (Optional) WhiteBox scoring (token-level logprobs)\n",
    "\n",
    "If your client returns **token logprobs**, UQLM can compute white-box signals like **minimum token probability**. Replace the placeholder client below with a real logprob-capable client and set `USE_WHITEBOX=True`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1312ec96",
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAVE_UQLM:\n",
    "    USE_WHITEBOX = True \n",
    "    if USE_WHITEBOX:\n",
    "        llm_logprobs = llm\n",
    "        uqlm_whitebox_df = await compute_uqlm_confidence(\n",
    "            dataframe=df,\n",
    "            mode=\"white_box\",\n",
    "            llm=llm_logprobs,\n",
    "            whitebox_scorers=[\"min_probability\", \"normalized_probability\"],\n",
    "            risk_threshold=0.5,\n",
    "            verbose=True,\n",
    "        )\n",
    "    else:\n",
    "        print(\"Set USE_WHITEBOX=True after wiring a logprob-capable client.\")\n",
    "else:\n",
    "    print(\"Install UQLM to run this section: `pip install uqlm`\")\n",
    "uqlm_whitebox_df"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
