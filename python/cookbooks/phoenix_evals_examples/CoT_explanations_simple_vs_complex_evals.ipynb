{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23a846d8",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-assets/arize-logo-white.jpg\" width=\"200\"/>\n",
    "        <br>\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/phoenix-logo-light.svg\" width=\"200\"/>\n",
    "        <br>\n",
    "        <a href=\"https://arize.com/docs/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://arize-ai.slack.com/join/shared_invite/zt-2w57bhem8-hq24MB6u7yE_ZF_ilOYSBw#/shared-invite/email\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\"> Judge Prompt Comparison: Simple/Complex Ã— Reasoning/Non-Reasoning Models </h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4e7025",
   "metadata": {},
   "source": [
    "This notebook uses **Arize Phoenix `llm_classify`** to evaluate tool-calling predictions on the **Berkeley Function Calling Leaderboard (BFCL)** dataset with:\n",
    "- a **simple** binary prompt (`Yes`/`No`), and\n",
    "- a **complex** multi-class prompt (`correct` / `partially_correct` / `incorrect`).\n",
    "\n",
    "We keep it minimal and focused on classification-style LLM-as-a-judge using a non-reasoning model and a reasoning model for the judge. \n",
    ">\n",
    "> ##### Note: This notebook was last updated on August 20, 2025. \n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052dc541",
   "metadata": {},
   "source": [
    "## Install & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b00e731",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip -q install --upgrade pandas datasets arize-phoenix openai tiktoken nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eab650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "\n",
    "from phoenix.evals import llm_classify\n",
    "from phoenix.evals.models import OpenAIModel\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7185b",
   "metadata": {},
   "source": [
    "## Configure Judge Models \n",
    " We will be using gpt-4o-mini as our nonreasoning & o3 for the Reasoning Model. Make sure to set your OPENAI_API_KEY. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f9c97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass.getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "\n",
    "non_reasoning_model = OpenAIModel(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    ")\n",
    "reasoning_model = OpenAIModel(\n",
    "    model=\"o3\",\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b57f78",
   "metadata": {},
   "source": [
    "## Load BFCL (V3 Exec Splits)\n",
    "\n",
    ">      \n",
    "> The Berkeley function calling leaderboard is a live leaderboard to evaluate the ability of different LLMs to call functions (also ?referred to as tools). We built this dataset from our learnings to be representative of most users' function calling use-cases, for example, in agents, as a part of enterprise workflows, etc. To this end, our evaluation dataset spans diverse categories, and across multiple languages.\n",
    "> \n",
    "\n",
    "The `exec_simple` dataset is where the 'single function evaluation contains the simplest but most commonly seen format, where the user supplies a single JSON function document, with one and only one function call being invoked.'\n",
    "\n",
    "The `exec_multiple` dataset is where the 'multiple function category contains a user question that only invokes one function call out of 2 to 4 JSON function documentations. The model needs to be capable of selecting the best function to invoke according to user-provided context.'\n",
    "\n",
    "More information about these datasets can be found here: https://huggingface.co/datasets/gorilla-llm/Berkeley-Function-Calling-Leaderboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994eb09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BFCL_FILES = {\n",
    "    \"exec_simple\": \"https://huggingface.co/datasets/gorilla-llm/Berkeley-Function-Calling-Leaderboard/resolve/main/BFCL_v3_exec_simple.json\",\n",
    "    \"exec_multiple\": \"https://huggingface.co/datasets/gorilla-llm/Berkeley-Function-Calling-Leaderboard/resolve/main/BFCL_v3_exec_multiple.json\",\n",
    "}\n",
    "\n",
    "\n",
    "def fetch_json(url, out_path):\n",
    "    out = Path(out_path)\n",
    "    if not out.exists():\n",
    "        print(f\"Downloading {url} -> {out}\")\n",
    "        urllib.request.urlretrieve(url, out)\n",
    "    text = Path(out).read_text()\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except Exception:\n",
    "        rows = []\n",
    "        for line in text.splitlines():\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                rows.append(json.loads(line))\n",
    "            except Exception:\n",
    "                pass\n",
    "        return rows\n",
    "\n",
    "\n",
    "df_simple = pd.DataFrame(fetch_json(BFCL_FILES[\"exec_simple\"], \"BFCL_v3_exec_simple.json\"))\n",
    "df_multi = pd.DataFrame(fetch_json(BFCL_FILES[\"exec_multiple\"], \"BFCL_v3_exec_multiple.json\"))\n",
    "print(\"Simple shape:\", df_simple.shape, \"Multiple shape:\", df_multi.shape)\n",
    "df = pd.concat([df_simple, df_multi], ignore_index=True)\n",
    "print(\"Combined shape:\", df.shape)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a35ff9",
   "metadata": {},
   "source": [
    "## Prepare & Format DataFrames\n",
    "Pull out different parts of the data like, instruction, functions, ground truth, & predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e73a4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_gt_call(row):\n",
    "    gt = row.get(\"ground_truth\")\n",
    "    if isinstance(gt, list) and gt:\n",
    "        return gt[0]\n",
    "    if isinstance(gt, str):\n",
    "        return gt\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def extract_functions(row):\n",
    "    fns = row.get(\"function\", [])\n",
    "    if isinstance(fns, dict):\n",
    "        fns = [fns]\n",
    "    return fns\n",
    "\n",
    "\n",
    "def extract_instruction(row):\n",
    "    q = row.get(\"question\", [])\n",
    "    last_user = \"\"\n",
    "    for msg_list in q:\n",
    "        for m in msg_list:\n",
    "            if m.get(\"role\") == \"user\":\n",
    "                last_user = m.get(\"content\", last_user)\n",
    "    return last_user\n",
    "\n",
    "\n",
    "work = []\n",
    "for _, r in df.iterrows():\n",
    "    rr = r.to_dict()\n",
    "    work.append(\n",
    "        {\n",
    "            \"id\": rr.get(\"id\", \"\"),\n",
    "            \"instruction\": extract_instruction(rr),\n",
    "            \"functions_json\": json.dumps(\n",
    "                [\n",
    "                    {\n",
    "                        \"name\": f.get(\"name\"),\n",
    "                        \"parameters\": f.get(\"parameters\"),\n",
    "                        \"description\": f.get(\"description\", \"\"),\n",
    "                    }\n",
    "                    for f in extract_functions(rr)\n",
    "                ],\n",
    "                ensure_ascii=False,\n",
    "            ),\n",
    "            \"ground_truth\": extract_gt_call(rr),\n",
    "        }\n",
    "    )\n",
    "all_data = pd.DataFrame(work)\n",
    "all_data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36abbbb",
   "metadata": {},
   "source": [
    "## Modify Benchmark Dataset\n",
    "\n",
    "The BFCL Dataset does not have any `negative` examples, i.e. only `question`, `available_tools`, and `ground_truth` are present. In order to accurately benchmark our LLM-as-a-Judge, this code implements a data corruption strategy to generate synthetic evaluation datasets for testing LLM-as-a-Judge systems. It's designed to create realistic \"negative examples\" (incorrect tool calls) from existing ground truth data, enabling comprehensive evaluation of classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee03b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt_call(s: str) -> str:\n",
    "    if not s or \"(\" not in s:\n",
    "        return s\n",
    "    tool, args = s.split(\"(\", 1)\n",
    "    tool = tool.strip()\n",
    "    args = args.rstrip(\")\")\n",
    "    if random.random() < 0.5:\n",
    "        tool = tool + \"_alt\"\n",
    "    else:\n",
    "        args = re.sub(r\"(\\d+(?:\\.\\d+)?)\", lambda m: str(float(m.group()) * 1.1), args, count=1)\n",
    "    return f\"{tool} ({args})\"\n",
    "\n",
    "\n",
    "predict_tool_call = [\n",
    "    gt if random.random() < 0.7 else corrupt_call(gt) for gt in all_data[\"ground_truth\"]\n",
    "]\n",
    "data = all_data.copy()\n",
    "data[\"predicted_tool_call\"] = predict_tool_call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c09162",
   "metadata": {},
   "source": [
    "We will be using a small subset of our data for testing purposes. Here we are generating our testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d366c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_data = data.sample(n=30, random_state=24).reset_index(drop=True)\n",
    "small_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bc0086",
   "metadata": {},
   "source": [
    "## Define your LLM-as-a-Judge Templates & Rails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4ae0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMPLE_TEMPLATE = \"\"\"You are grading a tool-calling attempt.\n",
    "\n",
    "Given:\n",
    "USER INSTRUCTION:\n",
    "{instruction}\n",
    "\n",
    "AVAILABLE FUNCTIONS (JSON Schemas):\n",
    "{functions_json}\n",
    "\n",
    "MODEL TOOL CALL (string):\n",
    "{predicted_tool_call}\n",
    "\n",
    "GROUND TRUTH TOOL CALL (string):\n",
    "{ground_truth}\n",
    "\n",
    "Question: Did the model invoke the correct tool(s) AND use the correct parameter names and values?\n",
    "Answer strictly with one word, Yes or No, & an explanation for your answer.\n",
    "\n",
    "Example response:\n",
    "LABEL: \"Yes\" or \"No\"\n",
    "EXPLANATION: An explanation of your reasoning for why the label is \"Yes\" or \"No\"\n",
    "\"\"\"\n",
    "\n",
    "SIMPLE_RAILS = [\"Yes\", \"No\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c76b4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPLEX_TEMPLATE = \"\"\"You are grading a tool-calling attempt.\n",
    "Return ONLY one of the following labels:\n",
    "- correct\n",
    "- partially_correct\n",
    "- incorrect\n",
    "\n",
    "Use these rules:\n",
    "- Consider types and trivial formatting (e.g., '5' vs 5, whitespace) as equivalent.\n",
    "- Consider equivalent units only if explicitly clear from context.\n",
    "- The attempt is \"correct\" only if the tool and all required parameters match the ground truth.\n",
    "- It's \"partially_correct\" if the tool is correct but parameters have minor issues.\n",
    "- It's \"incorrect\" otherwise.\n",
    "\n",
    "Context:\n",
    "USER INSTRUCTION:\n",
    "{instruction}\n",
    "\n",
    "AVAILABLE FUNCTIONS (JSON Schemas):\n",
    "{functions_json}\n",
    "\n",
    "MODEL TOOL CALL (string):\n",
    "{predicted_tool_call}\n",
    "\n",
    "GROUND TRUTH TOOL CALL (string):\n",
    "{ground_truth}\n",
    "\n",
    "Question: Use the rules above to determine if the model's tool call is correct, partially correct, or incorrect.\n",
    "Answer strictly with one label & an explanation for your answer.\n",
    "\n",
    "Example response:\n",
    "LABEL: \"correct\" or \"partially_correct\" or \"incorrect\"\n",
    "EXPLANATION: An explanation of your reasoning for why the label is \"correct\" or \"partially_correct\" or \"incorrect\"\n",
    "\"\"\"\n",
    "\n",
    "COMPLEX_RAILS = [\"correct\", \"partially_correct\", \"incorrect\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a79bb08",
   "metadata": {},
   "source": [
    "## Run our Simple Evaluation on both Judge Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d722232e",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_df = small_data.copy()\n",
    "\n",
    "non_reasoning_simple_results = llm_classify(\n",
    "    data=simple_df.assign(template=SIMPLE_TEMPLATE),\n",
    "    model=non_reasoning_model,\n",
    "    template=\"{template}\",\n",
    "    rails=SIMPLE_RAILS,\n",
    "    provide_explanation=True,\n",
    "    include_prompt=False,\n",
    "    include_response=True,\n",
    "    run_sync=True,\n",
    ")\n",
    "\n",
    "non_reasoning_simple_results.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae98d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_df = small_data.copy()\n",
    "\n",
    "reasoning_simple_results = llm_classify(\n",
    "    data=simple_df.assign(template=SIMPLE_TEMPLATE),\n",
    "    model=reasoning_model,\n",
    "    template=\"{template}\",\n",
    "    rails=SIMPLE_RAILS,\n",
    "    provide_explanation=True,\n",
    "    include_prompt=False,\n",
    "    include_response=True,\n",
    "    run_sync=True,\n",
    ")\n",
    "\n",
    "reasoning_simple_results.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8b1988",
   "metadata": {},
   "source": [
    "## Run our Complex Evaluation on both Judge Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1b05f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_df = small_data.copy()\n",
    "\n",
    "non_reasoning_complex_results = llm_classify(\n",
    "    data=complex_df.assign(template=COMPLEX_TEMPLATE),\n",
    "    model=non_reasoning_model,\n",
    "    template=\"{template}\",\n",
    "    rails=COMPLEX_RAILS,\n",
    "    provide_explanation=True,\n",
    "    include_prompt=False,\n",
    "    include_response=True,\n",
    "    run_sync=True,\n",
    ")\n",
    "non_reasoning_complex_results.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ebbe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_df = small_data.copy()\n",
    "\n",
    "reasoning_complex_results = llm_classify(\n",
    "    data=complex_df.assign(template=COMPLEX_TEMPLATE),\n",
    "    model=reasoning_model,\n",
    "    template=\"{template}\",\n",
    "    rails=COMPLEX_RAILS,\n",
    "    provide_explanation=True,\n",
    "    include_prompt=False,\n",
    "    include_response=True,\n",
    "    run_sync=True,\n",
    ")\n",
    "\n",
    "reasoning_complex_results.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b69a6c",
   "metadata": {},
   "source": [
    "## View Results\n",
    "\n",
    "We will compare the number of times the models disagree on their evaluation labels as well as how many tokens they used to complete their evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcd3002",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"For Simple Eval: \")\n",
    "print(\"-----------------------------------------------------------\")\n",
    "\n",
    "simple_different_labels = (\n",
    "    non_reasoning_simple_results[\"label\"] != reasoning_simple_results[\"label\"]\n",
    ").sum()\n",
    "if simple_different_labels == 0:\n",
    "    print(\"Reasoning and non-reasoning models agree on all samples\")\n",
    "else:\n",
    "    print(f\"Reasoning and non-reasoning models disagree on {simple_different_labels} samples\")\n",
    "NR_simple_tokens = non_reasoning_simple_results[\"total_tokens\"].sum()\n",
    "R_simple_tokens = reasoning_simple_results[\"total_tokens\"].sum()\n",
    "\n",
    "print(f\"Non-reasoning model used {NR_simple_tokens} tokens\")\n",
    "print(f\"Reasoning model used {R_simple_tokens} tokens\")\n",
    "print(\n",
    "    f\"Reasoning model is {R_simple_tokens / NR_simple_tokens} times more expensive than the non-reasoning model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af64f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"For Complex Eval: \")\n",
    "print(\"-----------------------------------------------------------\")\n",
    "\n",
    "complex_different_labels = (\n",
    "    non_reasoning_complex_results[\"label\"] != reasoning_complex_results[\"label\"]\n",
    ").sum()\n",
    "if complex_different_labels == 0:\n",
    "    print(\"Reasoning and non-reasoning models agree on all samples\")\n",
    "else:\n",
    "    print(f\"Reasoning and non-reasoning models disagree on {complex_different_labels} samples\")\n",
    "NR_complex_tokens = non_reasoning_complex_results[\"total_tokens\"].sum()\n",
    "R_complex_tokens = reasoning_complex_results[\"total_tokens\"].sum()\n",
    "\n",
    "print(f\"Non-reasoning model used {NR_complex_tokens} tokens\")\n",
    "print(f\"Reasoning model used {R_complex_tokens} tokens\")\n",
    "print(\n",
    "    f\"Reasoning model is {R_complex_tokens / NR_complex_tokens} times more expensive than the non-reasoning model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c84e99",
   "metadata": {},
   "source": [
    "### References\n",
    "- Phoenix Evals Overview: https://arize.com/docs/phoenix/evaluation/llm-evals\n",
    "- Using `llm_classify` (Docs): https://arize.com/docs/phoenix/evaluation/how-to-evals/bring-your-own-evaluator\n",
    "- BFCL dataset: https://huggingface.co/datasets/gorilla-llm/Berkeley-Function-Calling-Leaderboard\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
