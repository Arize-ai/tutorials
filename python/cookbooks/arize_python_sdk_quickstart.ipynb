{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5i_q-mjPjTi1"
      },
      "source": [
        "<center>\n",
        "    <p style=\"text-align:center\">\n",
        "        <img alt=\"arize logo\" src=\"https://storage.googleapis.com/arize-assets/arize-logo-white.jpg\" width=\"200\"/>\n",
        "        <br>\n",
        "        <a href=\"https://arize.com/docs/ax\">Docs</a>\n",
        "        |\n",
        "        <a href=\"https://github.com/Arize-ai/client_python\">GitHub</a>\n",
        "        |\n",
        "        <a href=\"https://arize-ai.slack.com/join/shared_invite/zt-2w57bhem8-hq24MB6u7yE_ZF_ilOYSBw#/shared-invite/email\">Community</a>\n",
        "    </p>\n",
        "</center>\n",
        "<h1 align=\"center\">Arize Python SDK Quickstart Guide</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook contains runnable code examples that accompany the three-part Arize AX Python SDK tutorial. It walks through the complete workflow:\n",
        "\n",
        "First, you will instrument a multi-agent application to send traces to Arize AX & evaluate those traces to measure performance. Then, you will create datasets and run experiments to systematically improve your application.\n",
        "\n",
        "Use this notebook alongside the [documentation](https://arize.com/docs/api-clients/python/version-8/tutorial/get-started-tracing-with-arize-sdk) - run each cell as you follow along with the guides to see the SDK in action and build a working financial research agent from scratch.\n",
        "\n",
        "The notebook is designed to be executed sequentially, with each section building on the previous one, so you'll have trace data to evaluate and then use those evaluations to guide your experiments."
      ],
      "metadata": {
        "id": "JRP-xhGE-98V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Send Traces to Arize AX"
      ],
      "metadata": {
        "id": "1GBULqKCIJID"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "dEE2ZFuVjTi3"
      },
      "outputs": [],
      "source": [
        "!pip install --pre arize[otel] crewai crewai-tools openinference-instrumentation-crewai openai openinference-instrumentation-openai arize-phoenix-evals\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HY2NTLrRjTi5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"ARIZE_API_KEY\"] = \"arize-api-key\"\n",
        "os.environ[\"ARIZE_SPACE_ID\"] = \"arize-space-id\"\n",
        "os.environ[\"SERPER_API_KEY\"] = \"serper-api-key\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"openai-api-key\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzDYOU10jTi5"
      },
      "outputs": [],
      "source": [
        "from arize.otel import register\n",
        "\n",
        "tracer_provider = register(\n",
        "    space_id=os.getenv(\"ARIZE_SPACE_ID\"),\n",
        "    api_key=os.getenv(\"ARIZE_API_KEY\"),\n",
        "    project_name=\"arize-sdk-quickstart\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRDChB2qjTi5"
      },
      "outputs": [],
      "source": [
        "from openinference.instrumentation.openai import OpenAIInstrumentor\n",
        "from openinference.instrumentation.crewai import CrewAIInstrumentor\n",
        "\n",
        "# Finish automatic instrumentation\n",
        "OpenAIInstrumentor().instrument(tracer_provider=tracer_provider)\n",
        "CrewAIInstrumentor().instrument(tracer_provider=tracer_provider)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynjC8KunjTi5"
      },
      "source": [
        "# Send Traces to Arize AX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GcxV2gzjTi5"
      },
      "outputs": [],
      "source": [
        "from crewai import Agent, Crew, Process, Task\n",
        "from crewai_tools import SerperDevTool\n",
        "\n",
        "search_tool = SerperDevTool()\n",
        "\n",
        "researcher = Agent(\n",
        "    role=\"Financial Research Analyst\",\n",
        "    goal=\"Gather up-to-date financial data, trends, and news for the target companies or markets\",\n",
        "    backstory=\"\"\"\n",
        "        You are a Senior Financial Research Analyst.\n",
        "    \"\"\",\n",
        "    verbose=True,\n",
        "    allow_delegation=False,\n",
        "    max_iter=2,\n",
        "    tools=[search_tool],\n",
        ")\n",
        "\n",
        "writer = Agent(\n",
        "    role=\"Financial Report Writer\",\n",
        "    goal=\"Compile and summarize financial research into clear, actionable insights\",\n",
        "    backstory=\"\"\"\n",
        "        You are an experienced financial content writer.\n",
        "    \"\"\",\n",
        "    verbose=True,\n",
        "    allow_delegation=True,\n",
        "    max_iter=1,\n",
        ")\n",
        "\n",
        "task1 = Task(\n",
        "    description=\"\"\"\n",
        "        Research: {tickers}\n",
        "        Focus on: {focus}\n",
        "    \"\"\",\n",
        "    expected_output=\"Detailed financial research summary with web search findings\",\n",
        "    agent=researcher,\n",
        ")\n",
        "\n",
        "task2 = Task(\n",
        "    description=\"Write a report based on the research above.\",\n",
        "    expected_output=\"A polished financial analysis report\",\n",
        "    agent=writer,\n",
        ")\n",
        "\n",
        "crew = Crew(\n",
        "    agents=[researcher, writer],\n",
        "    tasks=[task1, task2],\n",
        "    verbose=1,\n",
        "    process=Process.sequential,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u14J2HYMjTi6"
      },
      "outputs": [],
      "source": [
        "user_inputs = {\"tickers\": \"TSLA\", \"focus\": \"financial analysis and market outlook\"}\n",
        "\n",
        "result = crew.kickoff(inputs=user_inputs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_L4KTc1jTi6"
      },
      "outputs": [],
      "source": [
        "test_queries = [\n",
        "    {\"tickers\": \"AAPL\", \"focus\": \"financial analysis and market outlook\"},\n",
        "    {\"tickers\": \"NVDA\", \"focus\": \"valuation metrics and growth prospects\"},\n",
        "    {\"tickers\": \"AMZN\", \"focus\": \"profitability and market share\"},\n",
        "    {\"tickers\": \"AAPL, MSFT\", \"focus\": \"comparative financial analysis\"},\n",
        "    {\"tickers\": \"META, SNAP, PINS\", \"focus\": \"social media sector trends\"},\n",
        "    {\"tickers\": \"RIVN\", \"focus\": \"financial health and viability\"},\n",
        "    {\"tickers\": \"SNOW\", \"focus\": \"revenue growth trajectory\"},\n",
        "    {\"tickers\": \"KO\", \"focus\": \"dividend yield and stability\"},\n",
        "    {\"tickers\": \"META\", \"focus\": \"latest developments and stock performance\"},\n",
        "    {\"tickers\": \"AAPL, MSFT, GOOGL, AMZN, META\", \"focus\": \"big tech comparison and market outlook\"},\n",
        "    {\"tickers\": \"AMC\", \"focus\": \"financial analysis and market sentiment\"},\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CnrDS7EjTi6"
      },
      "outputs": [],
      "source": [
        "for query in test_queries:\n",
        "    crew.kickoff(inputs=query)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwhEINZ6jTi6"
      },
      "source": [
        "# Run and Log Evals to Measure Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LD2VkxqjTi6"
      },
      "outputs": [],
      "source": [
        "financial_completeness_template = \"\"\"\n",
        "You are evaluating whether a financial research report correctly completes ALL parts of the user's task with COMPREHENSIVE coverage.\n",
        "\n",
        "User input: {attributes.input.value}\n",
        "\n",
        "Generated report:\n",
        "{attributes.output.value}\n",
        "\n",
        "To be marked as \"complete\", the report MUST meet ALL of these strict requirements:\n",
        "\n",
        "1. TICKER COVERAGE (MANDATORY):\n",
        "   - Cover ALL companies/tickers mentioned in the input\n",
        "   - If multiple tickers are listed, EACH must have dedicated analysis (not just mentioned in passing)\n",
        "   - For multiple tickers, the report must provide COMPARATIVE analysis when relevant\n",
        "\n",
        "2. FOCUS AREA COVERAGE (MANDATORY):\n",
        "   - Address ALL focus areas mentioned in the input\n",
        "   - If the focus mentions multiple topics (e.g., \"earnings and outlook\"), BOTH must be thoroughly addressed\n",
        "   - Each focus area must have substantial content, not just a brief mention\n",
        "\n",
        "3. FINANCIAL DATA REQUIREMENTS (MANDATORY):\n",
        "   - For EACH ticker, the report must include:\n",
        "     * Current/recent stock price or performance data\n",
        "     * At least 2 key financial ratios (P/E, P/B, debt-to-equity, ROE, etc.)\n",
        "     * Revenue or earnings information\n",
        "     * Recent news or developments (within last 6 months)\n",
        "   - If focus mentions specific metrics (e.g., \"P/E ratio\"), those MUST be explicitly provided\n",
        "\n",
        "4. DEPTH REQUIREMENT (MANDATORY):\n",
        "   - Each ticker must have at least 3-4 sentences of dedicated analysis\n",
        "   - Generic statements without specific data do NOT count\n",
        "   - The report must demonstrate thorough research, not superficial coverage\n",
        "\n",
        "5. COMPARISON REQUIREMENT (if multiple tickers):\n",
        "   - If 2+ tickers are requested, the report MUST include direct comparisons\n",
        "   - Comparisons should cover multiple key metrics side-by-side\n",
        "   - Generic statements like \"both companies are good\" do NOT satisfy this requirement\n",
        "   - Must explicitly state which company performs better/worse on specific metrics\n",
        "\n",
        "The report is \"incomplete\" if it fails ANY of the above requirements, including:\n",
        "- Missing any ticker or only mentioning it briefly\n",
        "- Failing to address any focus area or only addressing it superficially\n",
        "- Missing required financial data for any ticker\n",
        "- Providing generic analysis without specific metrics or data\n",
        "- Failing to provide comparisons when multiple tickers are requested\n",
        "- Not meeting the depth requirement for any ticker\n",
        "\n",
        "Respond with ONLY one word: \"complete\" or \"incomplete\"\n",
        "Then provide a detailed explanation of which specific requirements were met or failed.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkVH2xRhjTi6"
      },
      "outputs": [],
      "source": [
        "from arize import ArizeClient\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "client = ArizeClient(api_key=os.getenv(\"ARIZE_API_KEY\"))\n",
        "\n",
        "# Export spans from the last hour (adjust time range as needed)\n",
        "end_time = datetime.now()\n",
        "start_time = end_time - timedelta(hours=1)\n",
        "\n",
        "df = client.spans.export_to_df(\n",
        "    space_id=os.getenv(\"ARIZE_SPACE_ID\"),\n",
        "    project_name=\"arize-sdk-quickstart\",\n",
        "    start_time=start_time,\n",
        "    end_time=end_time,\n",
        ")\n",
        "\n",
        "parent_spans = df[df[\"attributes.openinference.span.kind\"] == \"CHAIN\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUdj2WMIjTi6"
      },
      "outputs": [],
      "source": [
        "from phoenix.evals import LLM\n",
        "\n",
        "llm = LLM(model=\"gpt-4o\", provider=\"openai\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gARRn_lrjTi6"
      },
      "outputs": [],
      "source": [
        "from phoenix.evals import create_classifier\n",
        "\n",
        "completness_evaluator = create_classifier(\n",
        "    name=\"completeness\",\n",
        "    prompt_template=financial_completeness_template,\n",
        "    llm=llm,\n",
        "    choices={\"complete\": 1.0, \"incomplete\": 0.0},\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_MkE-vQjTi6"
      },
      "outputs": [],
      "source": [
        "from phoenix.evals import evaluate_dataframe\n",
        "from openinference.instrumentation import suppress_tracing\n",
        "\n",
        "with suppress_tracing():\n",
        "    results_df = evaluate_dataframe(dataframe=parent_spans, evaluators=[completness_evaluator])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vC5tIVhjTi7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from phoenix.evals.utils import to_annotation_dataframe\n",
        "\n",
        "# Convert Phoenix eval results to annotation format\n",
        "annotation_df = to_annotation_dataframe(dataframe=results_df)\n",
        "\n",
        "# Get span_ids from parent_spans\n",
        "span_ids = parent_spans[\"context.span_id\"].values\n",
        "\n",
        "# Build evaluation DataFrame\n",
        "eval_results = []\n",
        "eval_name = \"completeness\"\n",
        "\n",
        "for idx, span_id in enumerate(span_ids):\n",
        "    if idx < len(annotation_df):\n",
        "        eval_result = {\n",
        "            \"context.span_id\": span_id,\n",
        "            f\"eval.{eval_name}.label\": annotation_df[\"label\"].iloc[idx],\n",
        "            f\"eval.{eval_name}.score\": float(annotation_df[\"score\"].iloc[idx]),\n",
        "            f\"eval.{eval_name}.explanation\": annotation_df[\"explanation\"].iloc[idx],\n",
        "        }\n",
        "        eval_results.append(eval_result)\n",
        "\n",
        "evals_df = pd.DataFrame(eval_results)\n",
        "print(evals_df)\n",
        "\n",
        "# Upload evaluations to Arize\n",
        "response = client.spans.update_evaluations(\n",
        "    space_id=os.getenv(\"ARIZE_SPACE_ID\"),\n",
        "    project_name=\"arize-sdk-quickstart\",\n",
        "    dataframe=evals_df,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgLTcI3JjTi7"
      },
      "source": [
        "# Iterate with Experiments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sW5EgpAbjTi7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Create a dataset from the test_queries for the experiment\n",
        "examples_df = pd.DataFrame([\n",
        "    {\n",
        "        \"input\": f\"Research: {query['tickers']}\\nFocus on: {query['focus']}\",\n",
        "        \"tickers\": str(query[\"tickers\"]),  # Ensure string type\n",
        "        \"focus\": str(query[\"focus\"]),  # Ensure string type\n",
        "    }\n",
        "    for query in test_queries\n",
        "])\n",
        "\n",
        "# Create the dataset\n",
        "def append_timestamp(text: str) -> str:\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "    return f\"{text}_{timestamp}\"\n",
        "\n",
        "dataset = client.datasets.create(\n",
        "    space_id=os.getenv(\"ARIZE_SPACE_ID\"),\n",
        "    name=append_timestamp(\"arize-sdk-quickstart-dataset\"),\n",
        "    examples=examples_df,\n",
        ")\n",
        "\n",
        "print(f\"Created dataset: {dataset.id} - {dataset.name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pC48MALWjTi7"
      },
      "outputs": [],
      "source": [
        "# Define evaluators for experiments\n",
        "from phoenix.evals import ClassificationEvaluator\n",
        "from arize.experiments.evaluators.base import EvaluationResult\n",
        "\n",
        "completeness_evaluator = ClassificationEvaluator(\n",
        "    name=\"completeness\",\n",
        "    prompt_template=financial_completeness_template,\n",
        "    llm=llm,\n",
        "    choices={\"complete\": 1.0, \"incomplete\": 0.0},\n",
        ")\n",
        "\n",
        "\n",
        "def completeness(output, dataset_row):\n",
        "    # Extract input from dataset_row - construct it from tickers and focus\n",
        "    if isinstance(dataset_row, dict):\n",
        "        tickers = dataset_row.get(\"tickers\", \"\")\n",
        "        focus = dataset_row.get(\"focus\", \"\")\n",
        "    else:\n",
        "        tickers = getattr(dataset_row, \"tickers\", \"\")\n",
        "        focus = getattr(dataset_row, \"focus\", \"\")\n",
        "\n",
        "    # Format input as expected by the evaluator template\n",
        "    input_value = f\"Research: {tickers}\\nFocus on: {focus}\"\n",
        "\n",
        "    results = completeness_evaluator.evaluate(\n",
        "        {\"attributes.input.value\": input_value, \"attributes.output.value\": output}\n",
        "    )\n",
        "    # Convert Phoenix evaluation result to EvaluationResult\n",
        "    result = results[0]\n",
        "    return EvaluationResult(\n",
        "        score=result.score,\n",
        "        label=result.label,\n",
        "        explanation=result.explanation if hasattr(result, \"explanation\") else None,\n",
        "    )\n",
        "\n",
        "\n",
        "evaluators = [completeness]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4sZbBtVOjTi7"
      },
      "outputs": [],
      "source": [
        "# Run baseline experiment with original agent\n",
        "def my_task(dataset_row):\n",
        "    # Extract tickers and focus from dataset_row\n",
        "    if isinstance(dataset_row, dict):\n",
        "        tickers = dataset_row.get(\"tickers\", \"\")\n",
        "        focus = dataset_row.get(\"focus\", \"\")\n",
        "    else:\n",
        "        tickers = getattr(dataset_row, \"tickers\", \"\")\n",
        "        focus = getattr(dataset_row, \"focus\", \"\")\n",
        "\n",
        "    # Run the original crew with the extracted inputs\n",
        "    inputs = {\"tickers\": tickers, \"focus\": focus}\n",
        "    result = crew.kickoff(inputs=inputs)\n",
        "\n",
        "    # Return the output string\n",
        "    if hasattr(result, \"raw\"):\n",
        "        return result.raw\n",
        "    elif isinstance(result, str):\n",
        "        return result\n",
        "    else:\n",
        "        return str(result)\n",
        "\n",
        "# Run baseline experiment\n",
        "baseline_experiment, baseline_df = client.experiments.run(\n",
        "    name=\"baseline-experiment\",\n",
        "    dataset_id=dataset.id,\n",
        "    task=my_task,\n",
        "    evaluators=evaluators,\n",
        ")\n",
        "\n",
        "print(f\"Baseline experiment: {baseline_experiment}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_df"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ah7_IFtj4JmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aUdrPJFjTi7"
      },
      "outputs": [],
      "source": [
        "# Create an improved version of the crew with better prompts\n",
        "# Based on evaluation explanations from baseline, we're strengthening the agent goals\n",
        "researcher = Agent(\n",
        "    role=\"Financial Research Analyst\",\n",
        "    goal=\"\"\"Gather up-to-date financial data, trends, and news for the target companies/markets.\n",
        "        Make sure to include more than 1 financial ratios (such as P/E or P/B), news from the last 6 months, and current stock price or performance data.\"\"\",\n",
        "    backstory=\"\"\"\n",
        "            You are a Senior Financial Research Analyst.\n",
        "        \"\"\",\n",
        "    verbose=True,\n",
        "    allow_delegation=False,\n",
        "    max_iter=2,\n",
        "    tools=[search_tool],\n",
        ")\n",
        "writer = Agent(\n",
        "    role=\"Financial Report Writer\",\n",
        "    goal=\"Compile and summarize financial research into clear, actionable insights. If there are multiple tickers, make sure to include a dedicated comparison section.\",\n",
        "    backstory=\"\"\"\n",
        "        You are an experienced financial content writer.\n",
        "    \"\"\",\n",
        "    verbose=True,\n",
        "    allow_delegation=True,\n",
        "    max_iter=1,\n",
        ")\n",
        "updated_crew = Crew(\n",
        "    agents=[researcher, writer],\n",
        "    tasks=[task1, task2],\n",
        "    verbose=1,\n",
        "    process=Process.sequential,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def improved_task(dataset_row):\n",
        "    # Extract tickers and focus from dataset_row\n",
        "    if isinstance(dataset_row, dict):\n",
        "        tickers = dataset_row.get(\"tickers\", \"\")\n",
        "        focus = dataset_row.get(\"focus\", \"\")\n",
        "    else:\n",
        "        tickers = getattr(dataset_row, \"tickers\", \"\")\n",
        "        focus = getattr(dataset_row, \"focus\", \"\")\n",
        "\n",
        "    # Run the original crew with the extracted inputs\n",
        "    inputs = {\"tickers\": tickers, \"focus\": focus}\n",
        "    result = updated_crew.kickoff(inputs=inputs)\n",
        "\n",
        "    # Return the output string\n",
        "    if hasattr(result, \"raw\"):\n",
        "        return result.raw\n",
        "    elif isinstance(result, str):\n",
        "        return result\n",
        "    else:\n",
        "        return str(result)"
      ],
      "metadata": {
        "id": "KSEAO_U8OlX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "T7FlVpA8jTi7"
      },
      "outputs": [],
      "source": [
        "# Run the experiment with improved agent using the same task function and evaluator\n",
        "\n",
        "experiment, experiment_df = client.experiments.run(\n",
        "    name=\"improved-agent-instructions\",\n",
        "    dataset_id=dataset.id,\n",
        "    task=improved_task,\n",
        "    evaluators=evaluators,\n",
        ")\n",
        "\n",
        "print(f\"Experiment: {experiment_df}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "arize-quickstart",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}