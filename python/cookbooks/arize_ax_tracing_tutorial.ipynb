{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arize AX Tracing Tutorial: Building a Customer Support Agent\n",
    "\n",
    "In this tutorial, you'll learn how to instrument an LLM application with complete observability using Arize AX. We'll build a customer support chatbot called **SupportBot** that handles order status queries and FAQ questions.\n",
    "\n",
    "By the end of this tutorial, you'll be able to:\n",
    "\n",
    "- Capture complete execution traces for LLM calls, tool invocations, and RAG pipelines\n",
    "- Collect and log user feedback\n",
    "- Run automated LLM-as-Judge evaluations\n",
    "- Track multi-turn conversations with session management\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting, make sure you have:\n",
    "\n",
    "- An Arize AX account (sign up for free at [app.arize.com](https://app.arize.com))\n",
    "- Your Space ID and API Key (found in Settings â†’ API Keys)\n",
    "- OpenAI API key (or another supported LLM provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Installation\n",
    "\n",
    "First, let's install the required dependencies for tracing with Arize AX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qqqq openinference-instrumentation-openai openai arize-otel arize openinference-instrumentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Configure Tracing\n",
    "\n",
    "Next, we'll set up the connection to Arize AX and instrument OpenAI to automatically capture traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"ARIZE_API_KEY\"] = \"<YOUR_API_KEY>\"\n",
    "os.environ[\"ARIZE_SPACE_ID\"] = \"<YOUR_SPACE_ID>\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<YOUR_API_KEY>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”­ OpenTelemetry Tracing Details ðŸ”­\n",
      "|  Arize Project: my-support-bot\n",
      "|  Span Processor: BatchSpanProcessor\n",
      "|  Collector Endpoint: otlp.arize.com\n",
      "|  Transport: gRPC\n",
      "|  Transport Headers: {'authorization': '****', 'api_key': '****', 'arize-space-id': '****', 'space_id': '****', 'arize-interface': '****'}\n",
      "|  \n",
      "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
      "|  \n",
      "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
      "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from arize.otel import register\n",
    "from openinference.instrumentation.openai import OpenAIInstrumentor\n",
    "import openai\n",
    "from opentelemetry import trace\n",
    "\n",
    "tracer_provider = register(\n",
    "    space_id=os.getenv(\"ARIZE_SPACE_ID\"), \n",
    "    api_key=os.getenv(\"ARIZE_API_KEY\"),  \n",
    "    project_name=\"my-support-bot\",\n",
    ")\n",
    "\n",
    "OpenAIInstrumentor().instrument(tracer_provider=tracer_provider)\n",
    "\n",
    "openai_client = openai.OpenAI()\n",
    "tracer = trace.get_tracer(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**That's it!** With just these few lines, OpenInference will automatically capture traces for all your OpenAI calls. The instrumentation handles all the OpenTelemetry boilerplate for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Your First Trace - Query Classification\n",
    "\n",
    "Let's start by creating a simple query classifier that determines if a user message is about order status or a general FAQ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_query(user_message: str) -> str:\n",
    "    \"\"\"Classify if the query is about order status or a general FAQ.\"\"\"\n",
    "    \n",
    "    with tracer.start_as_current_span(\"classify-query\") as span:\n",
    "        span.set_attribute(\"openinference.span.kind\", \"CHAIN\")\n",
    "        span.set_attribute(\"input.value\", user_message)\n",
    "        \n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a classifier. Respond with either 'ORDER_STATUS' or 'FAQ'.\"\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": user_message}\n",
    "            ],\n",
    "        )\n",
    "        \n",
    "        classification = response.choices[0].message.content.strip()\n",
    "        span.set_attribute(\"output.value\", classification)\n",
    "        \n",
    "        return classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification: ORDER_STATUS\n"
     ]
    }
   ],
   "source": [
    "result = classify_query(\"Where is my order #12345?\")\n",
    "print(f\"Classification: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Head to your Arize AX dashboard, and you'll see this trace appear in real-time! You can see:\n",
    "- The parent \"classify-query\" span\n",
    "- The automatically captured OpenAI call details\n",
    "- Input and output values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Tool Call Tracing\n",
    "\n",
    "Now let's add the ability to look up order status using OpenAI's function calling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def get_order_status(order_id: str) -> dict:\n",
    "    \"\"\"Simulate looking up an order in a database.\"\"\"\n",
    "    return {\n",
    "        \"order_id\": order_id,\n",
    "        \"status\": \"In Transit\",\n",
    "        \"estimated_delivery\": \"2024-03-15\"\n",
    "    }\n",
    "\n",
    "def handle_order_query(user_message: str, prior_messages: list | None = None) -> str:\n",
    "    \"\"\"Handle order status queries using tool calling. Optional prior_messages for multi-turn context.\"\"\"\n",
    "    \n",
    "    with tracer.start_as_current_span(\"handle-order-query\") as span:\n",
    "        span.set_attribute(\"openinference.span.kind\", \"CHAIN\")\n",
    "        span.set_attribute(\"input.value\", user_message)\n",
    "        \n",
    "        system_msg = {\"role\": \"system\", \"content\": \"You are a customer support agent with access to order lookup. Use the get_order_status tool when the user asks about their order or provides an order ID.\"}\n",
    "        messages = [system_msg] + (prior_messages or []) + [{\"role\": \"user\", \"content\": user_message}]\n",
    "        \n",
    "        tools = [\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"get_order_status\",\n",
    "                    \"description\": \"Look up the status of a customer order\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"order_id\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The order ID (e.g., '12345')\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"order_id\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "        )\n",
    "\n",
    "        if response.choices[0].message.tool_calls:\n",
    "            tool_call = response.choices[0].message.tool_calls[0]\n",
    "            \n",
    "            with tracer.start_as_current_span(\"execute-tool\") as tool_span:\n",
    "                tool_span.set_attribute(\"openinference.span.kind\", \"TOOL\")\n",
    "                tool_span.set_attribute(\"tool.name\", tool_call.function.name)\n",
    "                tool_span.set_attribute(\"tool.parameters\", tool_call.function.arguments)\n",
    "                \n",
    "                args = json.loads(tool_call.function.arguments)\n",
    "                result = get_order_status(args[\"order_id\"])\n",
    "                \n",
    "                tool_span.set_attribute(\"tool.result\", json.dumps(result))\n",
    "            \n",
    "            tool_messages = [\n",
    "                {\"role\": \"user\", \"content\": user_message},\n",
    "                response.choices[0].message,\n",
    "                {\"role\": \"tool\", \"tool_call_id\": tool_call.id, \"content\": json.dumps(result)}\n",
    "            ]\n",
    "            final_response = openai_client.chat.completions.create(\n",
    "                model=\"gpt-4\",\n",
    "                messages=[system_msg] + (prior_messages or []) + tool_messages,\n",
    "            )\n",
    "\n",
    "            span.set_attribute(\"output.value\", final_response.choices[0].message.content)\n",
    "            \n",
    "            return final_response.choices[0].message.content\n",
    "        \n",
    "        return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The status of your order number 12345 is \"In Transit\". The estimated delivery date is 15th March, 2024.\n"
     ]
    }
   ],
   "source": [
    "response = handle_order_query(\"What's the status of order 12345?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Arize AX, you'll now see a complete trace tree showing:\n",
    "- The parent \"handle-order-query\" span\n",
    "- The LLM call that decided to use a tool\n",
    "- The \"execute-tool\" span with parameters and results\n",
    "- The final LLM call that formulated the response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: RAG Tracing\n",
    "\n",
    "For FAQ queries, we'll use a simple RAG (Retrieval-Augmented Generation) pipeline. Let's trace both the retrieval and generation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text(text: str) -> list[float]:\n",
    "    \"\"\"Generate embedding for text.\"\"\"\n",
    "    \n",
    "    with tracer.start_as_current_span(\"embed-text\") as span:\n",
    "        span.set_attribute(\"openinference.span.kind\", \"EMBEDDING\")\n",
    "        span.set_attribute(\"input.value\", text)\n",
    "        span.set_attribute(\"embedding.model_name\", \"text-embedding-3-small\")\n",
    "        span.set_attribute(\n",
    "            \"embedding.invocation_parameters\",\n",
    "            json.dumps({\"model\": \"text-embedding-3-small\"})\n",
    "        )\n",
    "        \n",
    "        response = openai_client.embeddings.create(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            input=text\n",
    "        )\n",
    "        \n",
    "        embedding = response.data[0].embedding\n",
    "        \n",
    "        span.set_attribute(\"embedding.embeddings.0.embedding.text\", text)\n",
    "        span.set_attribute(\"embedding.embeddings.0.embedding.vector\", embedding)\n",
    "        \n",
    "        return embedding\n",
    "\n",
    "def retrieve_documents(query: str, knowledge_base: list[str], top_k: int = 2) -> list[str]:\n",
    "    \"\"\"Retrieve relevant documents using embeddings.\"\"\"\n",
    "    \n",
    "    with tracer.start_as_current_span(\"retrieve-documents\") as span:\n",
    "        span.set_attribute(\"openinference.span.kind\", \"RETRIEVER\")\n",
    "        span.set_attribute(\"input.value\", query)\n",
    "        \n",
    "        query_embedding = embed_text(query)\n",
    "        span.set_attribute(\"retrieval.query_embedding_dims\", len(query_embedding))\n",
    "        \n",
    "        retrieved = knowledge_base[:top_k]\n",
    "\n",
    "        span.set_attribute(\"retrieval.documents\", retrieved)\n",
    "        \n",
    "        return retrieved\n",
    "\n",
    "def handle_faq_query(user_message: str) -> str:\n",
    "    \"\"\"Handle FAQ queries using RAG.\"\"\"\n",
    "    \n",
    "    knowledge_base = [\n",
    "        \"We offer free shipping on orders over $50.\",\n",
    "        \"Returns are accepted within 30 days of purchase.\",\n",
    "        \"You can track your order using the tracking number in your confirmation email.\",\n",
    "    ]\n",
    "    \n",
    "    with tracer.start_as_current_span(\"handle-faq-query\") as span:\n",
    "        span.set_attribute(\"openinference.span.kind\", \"CHAIN\")\n",
    "        span.set_attribute(\"input.value\", user_message)\n",
    "        \n",
    "        relevant_docs = retrieve_documents(user_message, knowledge_base)\n",
    "        \n",
    "        context = \"\\n\".join(relevant_docs)\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": f\"Answer the user's question using this context:\\n{context}\"\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": user_message}\n",
    "            ],\n",
    "        )\n",
    "        \n",
    "        answer = response.choices[0].message.content\n",
    "        span.set_attribute(\"output.value\", answer)\n",
    "        \n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our shipping policy is that we offer free shipping on orders over $50.\n"
     ]
    }
   ],
   "source": [
    "response = handle_faq_query(\"What's your shipping policy?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! Now you can see exactly:\n",
    "- What query was sent to retrieval\n",
    "- Which documents were retrieved\n",
    "- What context was passed to the LLM\n",
    "- What answer was generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Complete SupportBot\n",
    "\n",
    "Let's put it all together into a complete SupportBot with full tracing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supportbot(user_message: str) -> str:\n",
    "    \"\"\"Main SupportBot entry point with complete tracing.\"\"\"\n",
    "    \n",
    "    with tracer.start_as_current_span(\"supportbot\") as span:\n",
    "        span.set_attribute(\"openinference.span.kind\", \"AGENT\")\n",
    "        span.set_attribute(\"input.value\", user_message)\n",
    "        \n",
    "        classification = classify_query(user_message)\n",
    "        \n",
    "        if classification == \"ORDER_STATUS\":\n",
    "            response = handle_order_query(user_message)\n",
    "        else:\n",
    "            response = handle_faq_query(user_message)\n",
    "        \n",
    "        span.set_attribute(\"output.value\", response)\n",
    "        span.set_attribute(\"classification\", classification)\n",
    "        \n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User: Where is my order #12345?\n",
      "Bot: Your order #12345 is currently in transit. The estimated delivery date is March 15, 2024.\n",
      "\n",
      "User: What's your return policy?\n",
      "Bot: Our return policy allows for returns within 30 days of purchase.\n",
      "\n",
      "User: Can I track my order?\n",
      "Bot: Of course, I'd be happy to help you with that. Could you please provide me with your order ID?\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    \"Where is my order #12345?\",\n",
    "    \"What's your return policy?\",\n",
    "    \"Can I track my order?\",\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\nUser: {query}\")\n",
    "    print(f\"Bot: {supportbot(query)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotations & Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Automated Evaluations\n",
    "\n",
    "Manual feedback doesn't scale. Let's build automated evaluators using LLM-as-Judge to evaluate thousands of traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def evaluate_tool_results(trace_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Evaluate if tool calls succeeded or failed. Returns a DataFrame with context.span_id, label, score.\"\"\"\n",
    "    tool_spans = trace_df[trace_df[\"attributes.openinference.span.kind\"] == 'TOOL']\n",
    "    eval_results = []\n",
    "    eval_name = \"tool_success\"\n",
    "\n",
    "    for _, span in tool_spans.iterrows():\n",
    "        span_id = span[\"context.span_id\"]\n",
    "        tool_result = span.get(\"attributes.tool.result\", \"\")\n",
    "        tool_result_str = str(tool_result).lower() if tool_result else \"\"\n",
    "        has_error = any(\n",
    "            kw in tool_result_str for kw in [\"error\", \"failed\", \"invalid\", \"not found\"]\n",
    "        )\n",
    "\n",
    "        label = \"FAILED\" if has_error else \"SUCCESS\"\n",
    "        scores = 0.0 if has_error else 1.0\n",
    "\n",
    "        current_eval = {\n",
    "            \"context.span_id\": span_id,\n",
    "            f\"eval.{eval_name}.label\": label,\n",
    "            f\"eval.{eval_name}.score\" : scores,\n",
    "        }\n",
    "        eval_results.append(current_eval)\n",
    "    return pd.DataFrame(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize._exporter.client | INFO | Fetching data...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  exporting 40 rows: 100%|\u001b[38;2;0;128;0mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 40/40 [00:00, 436.27 row/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated 3 tool calls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from arize import ArizeClient\n",
    "from datetime import datetime, timedelta\n",
    "from openinference.instrumentation import suppress_tracing\n",
    "\n",
    "client = ArizeClient(api_key=os.getenv(\"ARIZE_API_KEY\"))\n",
    "\n",
    "end_time = datetime.now()\n",
    "start_time = end_time - timedelta(hours=1)\n",
    "\n",
    "trace_df = client.spans.export_to_df(\n",
    "    space_id=os.getenv(\"ARIZE_SPACE_ID\"),\n",
    "    project_name=\"my-support-bot\",\n",
    "    start_time=start_time,\n",
    "    end_time=end_time,\n",
    ")\n",
    "\n",
    "with suppress_tracing():\n",
    "    tool_eval_df = evaluate_tool_results(trace_df)\n",
    "\n",
    "print(f\"Evaluated {len(tool_eval_df)} tool calls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.arrow | INFO | âœ… Success! Check out your data at https://app.arize.com/organizations/QWNjb3VudE9yZ2FuaXphdGlvbjoxMzYwMDpZbVlu/spaces/U3BhY2U6MTQyNTM6a3pZWA==/models/modelName/my-support-bot?selectedTab=llmTracing\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = client.spans.update_evaluations(\n",
    "    space_id=os.getenv(\"ARIZE_SPACE_ID\"),\n",
    "    project_name=\"my-support-bot\",\n",
    "    dataframe=tool_eval_df,\n",
    "    force_http=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Session Tracking for Multi-Turn Conversations\n",
    "\n",
    "Real conversations involve multiple turns. Let's add session tracking to group related traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import re\n",
    "from openinference.instrumentation import using_session\n",
    "\n",
    "class ConversationManager:\n",
    "    \"\"\"Manage multi-turn conversations with session tracking.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.sessions = {} \n",
    "    \n",
    "    def start_conversation(self) -> str:\n",
    "        \"\"\"Start a new conversation and return session ID.\"\"\"\n",
    "        session_id = str(uuid.uuid4())\n",
    "        self.sessions[session_id] = {\n",
    "            \"history\": [],\n",
    "            \"context\": {}  \n",
    "        }\n",
    "        return session_id\n",
    "    \n",
    "    def handle_message(self, session_id: str, user_message: str) -> str:\n",
    "        \"\"\"Handle a message within a conversation session.\"\"\"\n",
    "        \n",
    "        with using_session(session_id=session_id):\n",
    "            with tracer.start_as_current_span(\"conversation-turn\") as span:\n",
    "                span.set_attribute(\"openinference.span.kind\", \"CHAIN\")\n",
    "                span.set_attribute(\"input.value\", user_message)\n",
    "                \n",
    "                history = self.sessions[session_id][\"history\"]\n",
    "                context = self.sessions[session_id][\"context\"]\n",
    "                \n",
    "                prior_messages = []\n",
    "                for turn in history:\n",
    "                    prior_messages.append({\"role\": \"user\", \"content\": turn[\"user\"]})\n",
    "                    prior_messages.append({\"role\": \"assistant\", \"content\": turn[\"assistant\"]})\n",
    "                \n",
    "                classification = classify_query(user_message)\n",
    "                if classification == \"ORDER_STATUS\":\n",
    "                    bot_response = handle_order_query(user_message, prior_messages=prior_messages)\n",
    "                else:\n",
    "                    bot_response = handle_faq_query(user_message)\n",
    "                \n",
    "                history.append({\n",
    "                    \"user\": user_message,\n",
    "                    \"assistant\": bot_response\n",
    "                })\n",
    "                \n",
    "                self._update_context(context, user_message, bot_response)\n",
    "                \n",
    "                span.set_attribute(\"output.value\", bot_response)\n",
    "                span.set_attribute(\"turn_number\", len(history))\n",
    "                \n",
    "                return bot_response\n",
    "    \n",
    "    def _update_context(self, context: dict, user_msg: str, bot_msg: str):\n",
    "        \"\"\"Extract and store conversation context.\"\"\"\n",
    "        order_ids = re.findall(r\"\\b\\d{5}\\b\", user_msg + \" \" + bot_msg)\n",
    "        if order_ids:\n",
    "            context[\"order_id\"] = order_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Conversation Started ===\n",
      "User: Where is my order?\n",
      "Bot: To help you with your request, could you kindly provide the order ID of your package?\n",
      "\n",
      "User: It's order 12345\n",
      "Bot: Your order is currently in transit. The estimated delivery is on the 15th of March, 2024. If you have any more questions or need further assistance, feel free to ask.\n",
      "\n",
      "User: When will it arrive?\n",
      "Bot: Your order is still in transit and is estimated to arrive on March 15th, 2024.\n"
     ]
    }
   ],
   "source": [
    "manager = ConversationManager()\n",
    "\n",
    "session_id = manager.start_conversation()\n",
    "\n",
    "print(\"=== Conversation Started ===\")\n",
    "\n",
    "response1 = manager.handle_message(session_id, \"Where is my order?\")\n",
    "print(f\"User: Where is my order?\")\n",
    "print(f\"Bot: {response1}\\n\")\n",
    "\n",
    "response2 = manager.handle_message(session_id, \"It's order 12345\")\n",
    "print(f\"User: It's order 12345\")\n",
    "print(f\"Bot: {response2}\\n\")\n",
    "\n",
    "response3 = manager.handle_message(session_id, \"When will it arrive?\")\n",
    "print(f\"User: When will it arrive?\")\n",
    "print(f\"Bot: {response3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Arize AX, all these traces will be grouped together under the same session ID, allowing you to view the complete conversation thread."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
